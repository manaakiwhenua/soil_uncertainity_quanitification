{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omondiagbep\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\omondiagbep\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\omondiagbep\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\omondiagbep\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.distributions import Normal, beta\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.regression import R2Score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "PREDICT_PROPERTIES=['clay']\n",
    "\n",
    "\n",
    "def get_spectra_data(train_csv, test_csv=None,mineral=False,target_dataframe=None,over_write_csv=False):\n",
    "\n",
    "\n",
    "    SPECTRA_COLUMN_STARTING = \"nir.\"\n",
    "    ID=['labSampleId']\n",
    "\n",
    "    # check if this is a target and make the target csv to be the same with the global file used for training\n",
    "    if isinstance(train_csv, list):\n",
    "        df_source = pd.concat(map(pd.read_csv, train_csv), ignore_index=True)\n",
    "        #remove spectra signal which is more than 1\n",
    "        print(\"removing out of bound spectra from source\")\n",
    "\n",
    "        #subset only the mineral from the lucas data\n",
    "        if mineral:\n",
    "            reflectance_out_of_bound = df_source.loc[df_source[df_source[df_source.columns[pd.Series(df_source.columns).str.startswith(SPECTRA_COLUMN_STARTING)]] > 1].dropna(\n",
    "            how='all', axis=0).index]\n",
    "            df_source = df_source.loc[set(df_source.index) - set(reflectance_out_of_bound.index)]\n",
    "\n",
    "            if 'mineral' in df_source.columns:\n",
    "                df_source=df_source.loc[df_source['mineral'] == 'mineral']\n",
    "\n",
    "\n",
    "    if target_dataframe is not None:\n",
    "        df_target = target_dataframe\n",
    "\n",
    "    if isinstance(test_csv, list) and target_dataframe is None:\n",
    "        df_target = pd.concat(map(pd.read_csv, test_csv), ignore_index=True)\n",
    "\n",
    "        # remove spectra signal which is more than 1\n",
    "        print(\"removing out of bound spectra from target\")\n",
    "\n",
    "        if mineral:\n",
    "            reflectance_out_of_bound = df_target.loc[df_target[df_target[df_target.columns[\n",
    "                pd.Series(df_target.columns).str.startswith(SPECTRA_COLUMN_STARTING)]] > 1].dropna(\n",
    "                how='all', axis=0).index]\n",
    "\n",
    "            df_target = df_target.loc[set(df_target.index) - set(reflectance_out_of_bound.index)]\n",
    "\n",
    "    #common_cols = list(set.intersection(set(df_target), set(df_source)))\n",
    "    #print(common_cols)\n",
    "\n",
    "    common_cols = df_source.columns.intersection(df_target.columns)\n",
    "\n",
    "    # use this list to perform column selection\n",
    "    df_target_ = df_target[common_cols]\n",
    "    df_source_ = df_source[common_cols]\n",
    "\n",
    "\n",
    "    # extract properties for source and target\n",
    "    df_source_ = df_source_.filter(regex='^(nir.|clay|labSampleId)')\n",
    "    df_target_ = df_target_.filter(regex='^(nir.|clay|labSampleId)')\n",
    "\n",
    "\n",
    "\n",
    "    # Define the character to remove\n",
    "    character_to_remove = 'nir.'\n",
    "\n",
    "    # Remove the character from column names\n",
    "    df_source_.columns = df_source_.columns.str.replace(character_to_remove, '')\n",
    "    df_target_.columns = df_target_.columns.str.replace(character_to_remove, '')\n",
    "\n",
    "\n",
    "    # Extract column names with numbers greater than the threshold\n",
    "    source_columns_to_drop = [col for col in df_source_.filter(regex='^(nir.)').columns if int(col) > 4000]\n",
    "    target_columns_to_drop = [col for col in df_target_.filter(regex='^(nir.)').columns if int(col) > 4000]\n",
    "\n",
    "    # Drop the selected columns from the DataFrame\n",
    "    df_source_ = df_source_.drop(columns=source_columns_to_drop)\n",
    "    df_target_ = df_target_.drop(columns=target_columns_to_drop)\n",
    "\n",
    "\n",
    "    #split train datasest into train and amin_val_loss\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df_source_train, df_source_val= train_test_split(df_source_,  test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    df_source_train_= df_source_train.drop(columns=PREDICT_PROPERTIES)\n",
    "    df_source_val_= df_source_val.drop(columns=PREDICT_PROPERTIES)\n",
    "    df_target__= df_target_.drop(columns=PREDICT_PROPERTIES)\n",
    "\n",
    "    df_source_spectral_train = get_spectra(df_source_train_)\n",
    "\n",
    "    df_source_spectral_val = get_spectra(df_source_val_)\n",
    "    print(len(df_source_spectral_train))\n",
    "    print(len(df_source_spectral_val))\n",
    "\n",
    "\n",
    "    df_target_spectral = get_spectra(df_target__)\n",
    "    print(len(df_target_spectral))\n",
    "\n",
    "    if over_write_csv:\n",
    "        pd.DataFrame(df_source_spectral_train).head(2).to_csv(\"../Data/clay_source_file.csv\", index=False)\n",
    "\n",
    "    df_source_spectral_train=df_source_spectral_train.to_numpy()\n",
    "    df_source_spectral_val=df_source_spectral_val.to_numpy()\n",
    "    df_target_spectral=df_target_spectral.to_numpy()\n",
    "\n",
    "    print(df_source_spectral_train.shape)\n",
    "\n",
    "    for i in range(len(PREDICT_PROPERTIES)):\n",
    "        if PREDICT_PROPERTIES[i] not in df_source_train.columns:\n",
    "            df_source_train[PREDICT_PROPERTIES[i]] =df_source_train[PREDICT_PROPERTIES[i]]\n",
    "\n",
    "        if PREDICT_PROPERTIES[i] not in df_source_val.columns:\n",
    "            df_source_val[PREDICT_PROPERTIES[i]] =df_source_val[PREDICT_PROPERTIES[i]]\n",
    "\n",
    "        if PREDICT_PROPERTIES[i] not in df_target_.columns and PREDICT_PROPERTIES[i] in  df_target:\n",
    "            df_target_[PREDICT_PROPERTIES[i]] =df_target[PREDICT_PROPERTIES[i]]\n",
    "\n",
    "    # remove the 2500 column in the USA/NZ data\n",
    "    #df_source = df_source.drop([SPECTRA_COLUMN_STARTING, SPECTRA_COLUMN_STARTING + '2500'], axis=1, errors='ignore')\n",
    "    #df_target = df_target.drop([SPECTRA_COLUMN_STARTING, SPECTRA_COLUMN_STARTING + '2500'], axis=1, errors='ignore')\n",
    "\n",
    "    return df_source_train, df_source_val,df_target_, df_source_spectral_train,df_source_spectral_val, df_target_spectral\n",
    "\n",
    "\n",
    "def get_spectra(df):\n",
    "\n",
    "    filter_col = [col for col in df.columns]\n",
    "\n",
    "    data = df[filter_col]\n",
    "\n",
    "    data=data.iloc[:, 2::10]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing out of bound spectra from source\n",
      "removing out of bound spectra from target\n",
      "2412\n",
      "604\n",
      "1005\n",
      "(2412, 340)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "GLOBAL_CSV=[ \"C:/Projects/SmapProjects/SpectraData2023/mir_texture_cali.csv\"]\n",
    "#df_sources, df_targets, df_sources_spectral, df_targets_spectral=get_spectra_data(train_csv=GLOBAL_CSV,target_dataframe=df)\n",
    "\n",
    "TARGET_CSV=[ \"C:/Projects/SmapProjects/SpectraData2023/mir_texture_val.csv\"]\n",
    "df_source_train, df_source_val,df_target, df_source_spectral_train, df_source_spectral_val,df_target_spectral=get_spectra_data(train_csv=GLOBAL_CSV,test_csv=TARGET_CSV,over_write_csv=True)\n",
    "\n",
    "\n",
    "X_train_raw = df_source_spectral_train#.reshape(df_source_spectral_train.shape[0], df_source_spectral_train.shape[1], 1)\n",
    "#X_train=np.log(1 / X_train)\n",
    "y_train = np.array(df_source_train[PREDICT_PROPERTIES].values)\n",
    "\n",
    "X_val_raw = df_source_spectral_val#.reshape(df_source_spectral_val.shape[0], df_source_spectral_val.shape[1], 1)\n",
    "y_val= np.array(df_source_val[PREDICT_PROPERTIES].values)\n",
    "\n",
    "\n",
    "X_test_raw =df_target_spectral#.reshape(df_target_spectral.shape[0], df_target_spectral.shape[1], 1)\n",
    "y_test =df_target[PREDICT_PROPERTIES].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "     labSampleId   clay      4000      3999      3998      3997      3996  \\\n63      SB10134B  29.00 -0.074913 -0.075331 -0.075767 -0.076227 -0.076636   \n2808    SB09894C   1.00 -0.047917 -0.047805 -0.047687 -0.047565 -0.047452   \n102     SB09842E  16.98 -0.076082 -0.079010 -0.082063 -0.085302 -0.088164   \n2692    SB09745B   7.00 -0.110468 -0.111472 -0.112524 -0.113588 -0.114633   \n416     M18/0291  23.00 -0.083878 -0.084949 -0.086063 -0.087265 -0.088286   \n...          ...    ...       ...       ...       ...       ...       ...   \n283     M17/7911  25.00 -0.090098 -0.090718 -0.091368 -0.092011 -0.092674   \n231     M18/0811  25.00 -0.080462 -0.081107 -0.081777 -0.082507 -0.083111   \n637     SB09963C  16.00 -0.085436 -0.085541 -0.085652 -0.085755 -0.085874   \n1034    SB10004E  33.00 -0.036036 -0.037472 -0.038972 -0.040534 -0.041974   \n96      SB09652A  23.84 -0.094164 -0.093897 -0.093617 -0.093330 -0.093056   \n\n          3995      3994      3993  ...       609       608       607  \\\n63   -0.076476 -0.074446 -0.069726  ... -2.907370 -2.949399 -2.862831   \n2808 -0.047326 -0.047063 -0.046522  ...  0.603775  0.605955  0.611350   \n102  -0.090187 -0.092096 -0.093131  ... -2.526272 -2.665675 -2.753467   \n2692 -0.115279 -0.114402 -0.111095  ... -0.468744 -0.444032 -0.424865   \n416  -0.088509 -0.087096 -0.083157  ... -2.701379 -2.822400 -2.850712   \n...        ...       ...       ...  ...       ...       ...       ...   \n283  -0.093187 -0.092868 -0.091343  ... -2.656169 -2.794697 -2.843566   \n231  -0.083062 -0.081519 -0.077925  ... -2.980524 -3.094550 -3.129518   \n637  -0.085871 -0.085200 -0.083394  ... -2.115041 -2.256682 -2.295815   \n1034 -0.043056 -0.043747 -0.043782  ... -2.354397 -2.438833 -2.449846   \n96   -0.092578 -0.091154 -0.088506  ... -2.753741 -2.927823 -3.028015   \n\n           606       605       604       603       602       601       600  \n63   -2.670668 -2.419237 -2.119185 -1.775398 -1.428811 -1.094719 -0.778498  \n2808  0.621533  0.637356  0.658497  0.683174  0.708055  0.732013  0.754703  \n102  -2.788477 -2.773178 -2.718126 -2.641012 -2.563073 -2.489146 -2.418586  \n2692 -0.412824 -0.407294 -0.409310 -0.418121 -0.427112 -0.435122 -0.443024  \n416  -2.786940 -2.651300 -2.450232 -2.197872 -1.943115 -1.699641 -1.468165  \n...        ...       ...       ...       ...       ...       ...       ...  \n283  -2.806568 -2.702206 -2.539186 -2.332690 -2.124212 -1.925092 -1.735724  \n231  -3.083343 -2.969656 -2.795273 -2.575735 -2.354108 -2.142313 -1.940948  \n637  -2.241302 -2.119427 -1.941005 -1.719844 -1.496625 -1.283015 -1.080069  \n1034 -2.392724 -2.285748 -2.135217 -1.950689 -1.764472 -1.586103 -1.416722  \n96   -3.050720 -3.002509 -2.892837 -2.742251 -2.590065 -2.445654 -2.307850  \n\n[604 rows x 3403 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labSampleId</th>\n      <th>clay</th>\n      <th>4000</th>\n      <th>3999</th>\n      <th>3998</th>\n      <th>3997</th>\n      <th>3996</th>\n      <th>3995</th>\n      <th>3994</th>\n      <th>3993</th>\n      <th>...</th>\n      <th>609</th>\n      <th>608</th>\n      <th>607</th>\n      <th>606</th>\n      <th>605</th>\n      <th>604</th>\n      <th>603</th>\n      <th>602</th>\n      <th>601</th>\n      <th>600</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63</th>\n      <td>SB10134B</td>\n      <td>29.00</td>\n      <td>-0.074913</td>\n      <td>-0.075331</td>\n      <td>-0.075767</td>\n      <td>-0.076227</td>\n      <td>-0.076636</td>\n      <td>-0.076476</td>\n      <td>-0.074446</td>\n      <td>-0.069726</td>\n      <td>...</td>\n      <td>-2.907370</td>\n      <td>-2.949399</td>\n      <td>-2.862831</td>\n      <td>-2.670668</td>\n      <td>-2.419237</td>\n      <td>-2.119185</td>\n      <td>-1.775398</td>\n      <td>-1.428811</td>\n      <td>-1.094719</td>\n      <td>-0.778498</td>\n    </tr>\n    <tr>\n      <th>2808</th>\n      <td>SB09894C</td>\n      <td>1.00</td>\n      <td>-0.047917</td>\n      <td>-0.047805</td>\n      <td>-0.047687</td>\n      <td>-0.047565</td>\n      <td>-0.047452</td>\n      <td>-0.047326</td>\n      <td>-0.047063</td>\n      <td>-0.046522</td>\n      <td>...</td>\n      <td>0.603775</td>\n      <td>0.605955</td>\n      <td>0.611350</td>\n      <td>0.621533</td>\n      <td>0.637356</td>\n      <td>0.658497</td>\n      <td>0.683174</td>\n      <td>0.708055</td>\n      <td>0.732013</td>\n      <td>0.754703</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>SB09842E</td>\n      <td>16.98</td>\n      <td>-0.076082</td>\n      <td>-0.079010</td>\n      <td>-0.082063</td>\n      <td>-0.085302</td>\n      <td>-0.088164</td>\n      <td>-0.090187</td>\n      <td>-0.092096</td>\n      <td>-0.093131</td>\n      <td>...</td>\n      <td>-2.526272</td>\n      <td>-2.665675</td>\n      <td>-2.753467</td>\n      <td>-2.788477</td>\n      <td>-2.773178</td>\n      <td>-2.718126</td>\n      <td>-2.641012</td>\n      <td>-2.563073</td>\n      <td>-2.489146</td>\n      <td>-2.418586</td>\n    </tr>\n    <tr>\n      <th>2692</th>\n      <td>SB09745B</td>\n      <td>7.00</td>\n      <td>-0.110468</td>\n      <td>-0.111472</td>\n      <td>-0.112524</td>\n      <td>-0.113588</td>\n      <td>-0.114633</td>\n      <td>-0.115279</td>\n      <td>-0.114402</td>\n      <td>-0.111095</td>\n      <td>...</td>\n      <td>-0.468744</td>\n      <td>-0.444032</td>\n      <td>-0.424865</td>\n      <td>-0.412824</td>\n      <td>-0.407294</td>\n      <td>-0.409310</td>\n      <td>-0.418121</td>\n      <td>-0.427112</td>\n      <td>-0.435122</td>\n      <td>-0.443024</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>M18/0291</td>\n      <td>23.00</td>\n      <td>-0.083878</td>\n      <td>-0.084949</td>\n      <td>-0.086063</td>\n      <td>-0.087265</td>\n      <td>-0.088286</td>\n      <td>-0.088509</td>\n      <td>-0.087096</td>\n      <td>-0.083157</td>\n      <td>...</td>\n      <td>-2.701379</td>\n      <td>-2.822400</td>\n      <td>-2.850712</td>\n      <td>-2.786940</td>\n      <td>-2.651300</td>\n      <td>-2.450232</td>\n      <td>-2.197872</td>\n      <td>-1.943115</td>\n      <td>-1.699641</td>\n      <td>-1.468165</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>M17/7911</td>\n      <td>25.00</td>\n      <td>-0.090098</td>\n      <td>-0.090718</td>\n      <td>-0.091368</td>\n      <td>-0.092011</td>\n      <td>-0.092674</td>\n      <td>-0.093187</td>\n      <td>-0.092868</td>\n      <td>-0.091343</td>\n      <td>...</td>\n      <td>-2.656169</td>\n      <td>-2.794697</td>\n      <td>-2.843566</td>\n      <td>-2.806568</td>\n      <td>-2.702206</td>\n      <td>-2.539186</td>\n      <td>-2.332690</td>\n      <td>-2.124212</td>\n      <td>-1.925092</td>\n      <td>-1.735724</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>M18/0811</td>\n      <td>25.00</td>\n      <td>-0.080462</td>\n      <td>-0.081107</td>\n      <td>-0.081777</td>\n      <td>-0.082507</td>\n      <td>-0.083111</td>\n      <td>-0.083062</td>\n      <td>-0.081519</td>\n      <td>-0.077925</td>\n      <td>...</td>\n      <td>-2.980524</td>\n      <td>-3.094550</td>\n      <td>-3.129518</td>\n      <td>-3.083343</td>\n      <td>-2.969656</td>\n      <td>-2.795273</td>\n      <td>-2.575735</td>\n      <td>-2.354108</td>\n      <td>-2.142313</td>\n      <td>-1.940948</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>SB09963C</td>\n      <td>16.00</td>\n      <td>-0.085436</td>\n      <td>-0.085541</td>\n      <td>-0.085652</td>\n      <td>-0.085755</td>\n      <td>-0.085874</td>\n      <td>-0.085871</td>\n      <td>-0.085200</td>\n      <td>-0.083394</td>\n      <td>...</td>\n      <td>-2.115041</td>\n      <td>-2.256682</td>\n      <td>-2.295815</td>\n      <td>-2.241302</td>\n      <td>-2.119427</td>\n      <td>-1.941005</td>\n      <td>-1.719844</td>\n      <td>-1.496625</td>\n      <td>-1.283015</td>\n      <td>-1.080069</td>\n    </tr>\n    <tr>\n      <th>1034</th>\n      <td>SB10004E</td>\n      <td>33.00</td>\n      <td>-0.036036</td>\n      <td>-0.037472</td>\n      <td>-0.038972</td>\n      <td>-0.040534</td>\n      <td>-0.041974</td>\n      <td>-0.043056</td>\n      <td>-0.043747</td>\n      <td>-0.043782</td>\n      <td>...</td>\n      <td>-2.354397</td>\n      <td>-2.438833</td>\n      <td>-2.449846</td>\n      <td>-2.392724</td>\n      <td>-2.285748</td>\n      <td>-2.135217</td>\n      <td>-1.950689</td>\n      <td>-1.764472</td>\n      <td>-1.586103</td>\n      <td>-1.416722</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>SB09652A</td>\n      <td>23.84</td>\n      <td>-0.094164</td>\n      <td>-0.093897</td>\n      <td>-0.093617</td>\n      <td>-0.093330</td>\n      <td>-0.093056</td>\n      <td>-0.092578</td>\n      <td>-0.091154</td>\n      <td>-0.088506</td>\n      <td>...</td>\n      <td>-2.753741</td>\n      <td>-2.927823</td>\n      <td>-3.028015</td>\n      <td>-3.050720</td>\n      <td>-3.002509</td>\n      <td>-2.892837</td>\n      <td>-2.742251</td>\n      <td>-2.590065</td>\n      <td>-2.445654</td>\n      <td>-2.307850</td>\n    </tr>\n  </tbody>\n</table>\n<p>604 rows × 3403 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source_val\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "scaler.fit(X_val_raw)\n",
    "X_val = scaler.transform(X_val_raw)\n",
    "\n",
    "X_test = X_test_raw\n",
    "\n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def evaluate(model,test_x,test,type=\"mae\",sample_size = 100):\n",
    "    df =evaluate_soil_property(model,test_x,sample_size = sample_size)\n",
    "\n",
    "    result_total_pret= pd.DataFrame(columns=['upper', 'lower','pred', 'obs'])\n",
    "\n",
    "    result_total_pret['pred'] = df['pred']\n",
    "\n",
    "    result_total_pret['lower'] =df['lower']\n",
    "    result_total_pret['upper'] =df['upper']\n",
    "\n",
    "    result_total_pret['obs']=test[:,0]\n",
    "\n",
    "    if type==\"mae\":\n",
    "        r2_total_pret =mean_absolute_error(test[:,0],result_total_pret['pred'])\n",
    "    elif type==\"r2\":\n",
    "        r2_total_pret =r2_score(test[:,0],result_total_pret['pred'])\n",
    "    elif type==\"mse\":\n",
    "        r2_total_pret =mean_squared_error(test[:,0],result_total_pret['pred'])\n",
    "    elif type==\"rmse\":\n",
    "        r2_total_pret=np.sqrt(mean_squared_error(test[:,0],result_total_pret['pred']))\n",
    "\n",
    "\n",
    "    return r2_total_pret\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "def evaluate_soil_property(model,\n",
    "                         test,\n",
    "                        sample_size = 100):\n",
    "\n",
    "     df = pd.DataFrame(columns=['lower','upper','pred'])\n",
    "\n",
    "     with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(test)\n",
    "        predictions=[]\n",
    "        test = scaler.transform(test)\n",
    "        test = torch.tensor(test, dtype=torch.float32)\n",
    "        for _ in range(sample_size):\n",
    "\n",
    "            output = model(test)  # Replace 'input_data' with your test input\n",
    "            predictions.append(output.numpy())\n",
    "\n",
    "        predictions = torch.tensor(predictions)\n",
    "\n",
    "        prediction_mean = (torch.mean(predictions, dim=0)).detach().numpy()\n",
    "\n",
    "\n",
    "        prediction_std = torch.std(predictions, dim=0).detach().numpy()\n",
    "\n",
    "        # Calculate lower and upper bounds for the prediction interval (e.g., 95% interval)\n",
    "        lower_bound_ = prediction_mean - (1.645 * prediction_std)\n",
    "        upper_bound_ = prediction_mean + (1.645 * prediction_std)\n",
    "\n",
    "\n",
    "        for i in range(0,len(prediction_mean)):\n",
    "\n",
    "            #get the first element since we ar predicting just one at a time\n",
    "            sample_pred = prediction_mean[i][0]\n",
    "\n",
    "            lower_bound = lower_bound_[i][0]\n",
    "            upper_bound = upper_bound_[i][0]\n",
    "\n",
    "            # Check and adjust upper bound if greater than 100\n",
    "            upper_bound = min(upper_bound, 99.9)\n",
    "\n",
    "            # Check and adjust prediction if greater than 100\n",
    "            sample_pred = min(sample_pred, 99.5)\n",
    "\n",
    "            row = {'upper':upper_bound,'lower':abs(lower_bound),'pred':sample_pred}\n",
    "\n",
    "            df.loc[i] = row\n",
    "        return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Define Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal,Uniform\n",
    "\n",
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_feature: int, dims=[512, 28, 64], bias_mean=0.0003,weight_mean=0.0002,weight_std=0.0001, bias_std=0.0005):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "\n",
    "        # Define the fully connected layers based on the specified dimensions\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = num_feature\n",
    "        for out_dim in dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.fc_out = nn.Linear(in_dim, 1)  # Output layer for pret\n",
    "\n",
    "        # Update the dimensions of weight_mu and weight_rho to match the output dimension\n",
    "        self.weight_dim = out_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_std = weight_std\n",
    "        self.bias_std = bias_std\n",
    "        self.weight_mean = weight_mean\n",
    "        self.bias_mean = bias_mean\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weight means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.weight_mu, self.weight_mean, std=self.weight_std)\n",
    "        nn.init.normal_(self.weight_rho, self.weight_mean, std=self.weight_std)\n",
    "\n",
    "        # Initialize bias means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.bias_mu, self.bias_mean, std=self.bias_std)\n",
    "        nn.init.normal_(self.bias_rho, self.bias_mean, std=self.bias_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "\n",
    "        # Pass through fully connected layers with specified dimensions\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.rrelu(fc_layer(x))\n",
    "            #Dropout randomly deactivates a fraction of neurons during training,\n",
    "            # which can help prevent overfitting.\n",
    "            #x = F.dropout(x,p=0.30)\n",
    "\n",
    "\n",
    "        # Re-parameterization trick for sampling weights\n",
    "        #To reduce variability, you can reduce the standard deviation of this sampling.\n",
    "        weight_epsilon = Normal(0.0,1).sample(self.weight_mu.size())\n",
    "        weight_sigma = torch.log(1 + torch.exp(self.weight_rho))\n",
    "        weight = self.weight_mu + (weight_sigma * weight_epsilon)\n",
    "\n",
    "        bias_epsilon = Normal(0.0,1).sample(self.bias_mu.size())\n",
    "        bias_sigma = torch.log(1 + torch.exp(self.bias_rho))\n",
    "        bias = self.bias_mu + (bias_sigma * bias_epsilon)\n",
    "\n",
    "        # Enforce non-negativity on weights and biases\n",
    "        weight = torch.clamp(weight, min=0)\n",
    "        bias = torch.clamp(bias, min=0)\n",
    "\n",
    "\n",
    "        # Final linear layer operation\n",
    "        output = F.linear(x, weight, bias)\n",
    "        # Apply sigmoid activation to squash values between 0 and 1\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        # Scale the values to the desired range (0 to 100)\n",
    "        output = output * 100\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "train_dataloader = DataLoader(train, batch_size=16)\n",
    "\n",
    "val= torch.utils.data.TensorDataset(X_val,y_val)\n",
    "val_dataloader = DataLoader(val, batch_size=16)\n",
    "\n",
    "test= torch.utils.data.TensorDataset(X_test,y_test)\n",
    "test_dataloader = DataLoader(test, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#Manually compute the L1 loss over all model parameters:\n",
    "def l1_penalty(model):\n",
    "    l1_loss = 0.0\n",
    "    for param in model.parameters():\n",
    "        l1_loss += torch.abs(param).sum()\n",
    "    return l1_loss\n",
    "\n",
    "def train_epoch(train,val,model,loss_fn,optimizer,batch_size,n_epochs):\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size)\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] Loss: {loss.item():.5f}\")\n",
    "        #evaluate accuracy at end of each epoch\n",
    "        test_epoch(val,model, loss_fn ,optimizer,batch_size,n_epochs)\n",
    "\n",
    "\n",
    "def test_epoch(val,model,loss_fn,optimizer,batch_size,n_epochs):\n",
    "    val_dataloader = DataLoader(val, batch_size=batch_size)\n",
    "    val_loss=0\n",
    "    model.eval()\n",
    "    for inputs, targets in val_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "    avg_val_loss = val_loss / len(inputs)\n",
    "    print(f\"Epoch [{n_epochs+1}/{n_epochs}] Val Loss: {loss.item():.5f}\")\n",
    "\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def get_metrics(test,y_test,model):\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    rmse= np.sqrt(mse)\n",
    "\n",
    "    test_result= evaluate_soil_property(model,test)\n",
    "    test_result[\"obs\"] = y_test\n",
    "    pcip = calculate_pcip(test_result[\"obs\"], test_result['lower'], test_result['upper'])\n",
    "    r2score = R2Score()\n",
    "    r2= r2score(y_pred, y_test).item()\n",
    "\n",
    "    r2_pent =r2*100\n",
    "    #Balancing Trade-offs between r2 and pcip:\n",
    "    score_avg = (pcip +  r2_pent)/2\n",
    "    print(f'MSE: {mse:.4f},R2: {r2:.4f},RMSE: {rmse:.4f}, PCIP: {pcip:.2f},score_avg:{score_avg:.4f}')\n",
    "    return [mse,r2,rmse,pcip]\n",
    "\n",
    "def calculate_pcip(y_true, lower_bounds, upper_bounds):\n",
    "    num_samples = len(y_true)\n",
    "    num_covering_intervals = np.sum((lower_bounds <= y_true) & (y_true <= upper_bounds))\n",
    "    pcip = (num_covering_intervals / num_samples) * 100\n",
    "    return pcip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Hyperparamter tuning to find best Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 110\u001B[0m\n\u001B[0;32m    108\u001B[0m optimizer_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdams\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSGD\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNadam\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    109\u001B[0m optimizer_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdams\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNadam\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 110\u001B[0m \u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_cpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m_temp_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/ray\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# assign the total # of cpus and gpus, make sure you have ray.init in the beginning and ray.shutdown at the end\u001B[39;00m\n\u001B[0;32m    111\u001B[0m sched \u001B[38;5;241m=\u001B[39m AsyncHyperBandScheduler(  time_attr\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_iteration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    112\u001B[0m     reduction_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    113\u001B[0m     metric\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrmse\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    114\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# set a scheduler\u001B[39;00m\n\u001B[0;32m    116\u001B[0m perturbation_interval \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001B[0m, in \u001B[0;36mclient_mode_hook.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minit\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m is_client_mode_enabled_by_default:\n\u001B[0;32m    102\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(ray, func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\_private\\worker.py:1514\u001B[0m, in \u001B[0;36minit\u001B[1;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001B[0m\n\u001B[0;32m   1480\u001B[0m     ray_params \u001B[38;5;241m=\u001B[39m ray\u001B[38;5;241m.\u001B[39m_private\u001B[38;5;241m.\u001B[39mparameter\u001B[38;5;241m.\u001B[39mRayParams(\n\u001B[0;32m   1481\u001B[0m         node_ip_address\u001B[38;5;241m=\u001B[39mnode_ip_address,\n\u001B[0;32m   1482\u001B[0m         raylet_ip_address\u001B[38;5;241m=\u001B[39mraylet_ip_address,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1508\u001B[0m         node_name\u001B[38;5;241m=\u001B[39m_node_name,\n\u001B[0;32m   1509\u001B[0m     )\n\u001B[0;32m   1510\u001B[0m     \u001B[38;5;66;03m# Start the Ray processes. We set shutdown_at_exit=False because we\u001B[39;00m\n\u001B[0;32m   1511\u001B[0m     \u001B[38;5;66;03m# shutdown the node in the ray.shutdown call that happens in the atexit\u001B[39;00m\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# handler. We still spawn a reaper process in case the atexit handler\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m# isn't called.\u001B[39;00m\n\u001B[1;32m-> 1514\u001B[0m     _global_node \u001B[38;5;241m=\u001B[39m \u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_private\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1515\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1516\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshutdown_at_exit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1517\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspawn_reaper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1518\u001B[0m \u001B[43m        \u001B[49m\u001B[43mray_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mray_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1519\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1520\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1521\u001B[0m     \u001B[38;5;66;03m# In this case, we are connecting to an existing cluster.\u001B[39;00m\n\u001B[0;32m   1522\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_cpus \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m num_gpus \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\_private\\node.py:287\u001B[0m, in \u001B[0;36mNode.__init__\u001B[1;34m(self, ray_params, head, shutdown_at_exit, spawn_reaper, connect_only, default_worker)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;66;03m# Start processes.\u001B[39;00m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m head:\n\u001B[1;32m--> 287\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_head_processes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m connect_only:\n\u001B[0;32m    290\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_ray_processes()\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\_private\\node.py:1181\u001B[0m, in \u001B[0;36mNode.start_head_processes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m     include_dashboard \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1179\u001B[0m     raise_on_api_server_failure \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m-> 1181\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_api_server\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1182\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_dashboard\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_dashboard\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1183\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraise_on_failure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraise_on_api_server_failure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1184\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\_private\\node.py:931\u001B[0m, in \u001B[0;36mNode.start_api_server\u001B[1;34m(self, include_dashboard, raise_on_failure)\u001B[0m\n\u001B[0;32m    926\u001B[0m \u001B[38;5;66;03m# Only redirect logs to .err. .err file is only useful when the\u001B[39;00m\n\u001B[0;32m    927\u001B[0m \u001B[38;5;66;03m# component has an unexpected output to stdout/stderr.\u001B[39;00m\n\u001B[0;32m    928\u001B[0m _, stderr_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_log_file_handles(\n\u001B[0;32m    929\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdashboard\u001B[39m\u001B[38;5;124m\"\u001B[39m, unique\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, create_out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    930\u001B[0m )\n\u001B[1;32m--> 931\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_webui_url, process_info \u001B[38;5;241m=\u001B[39m \u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_private\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mservices\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_api_server\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_dashboard\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    933\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraise_on_failure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ray_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdashboard_host\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    935\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgcs_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    936\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_node_ip_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    937\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_temp_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_logs_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m    \u001B[49m\u001B[43mport\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ray_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdashboard_port\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdashboard_grpc_port\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ray_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdashboard_grpc_port\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfate_share\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_fate_share\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_bytes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbackup_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackup_count\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m    \u001B[49m\u001B[43mredirect_logging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshould_redirect_logs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstdout_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstderr_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstderr_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstderr_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m ray_constants\u001B[38;5;241m.\u001B[39mPROCESS_TYPE_DASHBOARD \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_processes\n\u001B[0;32m    950\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m process_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\_private\\services.py:1198\u001B[0m, in \u001B[0;36mstart_api_server\u001B[1;34m(include_dashboard, raise_on_failure, host, gcs_address, node_ip_address, temp_dir, logdir, session_dir, port, dashboard_grpc_port, fate_share, max_bytes, backup_count, redirect_logging, stdout_file, stderr_file)\u001B[0m\n\u001B[0;32m   1195\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m     \u001B[38;5;66;03m# This is often on the critical path of ray.init() and ray start,\u001B[39;00m\n\u001B[0;32m   1197\u001B[0m     \u001B[38;5;66;03m# so we need to poll often.\u001B[39;00m\n\u001B[1;32m-> 1198\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;66;03m# Dashboard couldn't be started.\u001B[39;00m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dashboard_url \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import ray\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.tune.schedulers import PopulationBasedTraining, HyperBandForBOHB\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from enum import Enum\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "class Optimiser(str, Enum):\n",
    "    Adams = \"Adams\"\n",
    "    SGD =\"SGD\"\n",
    "    Adadelta = \"Adadelta\"\n",
    "    Adagrad = \"Adagrad\"\n",
    "    Adamax = \"Adamax\"\n",
    "    Nadam = \"Nadam\"\n",
    "    Ftrl = \"Ftrl\"\n",
    "    RMSprop=\"RMSprop\"\n",
    "    LBFGS =\"LBFGS\"\n",
    "    #LBFGS =\"LBFGS\"\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    " # set your desired L1 regularization strength\n",
    "def objective(config):  # ①\n",
    "\n",
    "    #dataset\n",
    "    train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "\n",
    "    val= torch.utils.data.TensorDataset(X_val,y_val)\n",
    "\n",
    "\n",
    "    #dim = random.sample(output_dims, config[\"n_layers\"])\n",
    "\n",
    "    criterion=nn.MSELoss()\n",
    "\n",
    "    #  dimensions to select\n",
    "    dims = config[\"dims\"]  # You can change this to any number you want\n",
    "\n",
    "\n",
    "\n",
    "    model = BayesianCNN(num_feature=X_train.shape[1],dims=dims,\n",
    "                           weight_std=config[\"weight_std\"],\n",
    "                          bias_std=config[\"bias_std\"],\n",
    "                        bias_mean=config[\"bias_mean\"],weight_mean=config[\"weight_mean\"]).to(\"cpu\")   # Create a PyTorch conv net\n",
    "\n",
    "    if config[\"optimiser\"] == Optimiser.RMSprop:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), momentum=config[\"opt_momentum\"], lr=config[\"lr\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.LBFGS:\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(),  lr=config[\"lr\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.Adams:\n",
    "        optimizer = torch.optim.Adam(model.parameters(),  lr=config[\"lr\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.SGD:\n",
    "        optimizer = torch.optim.SGD(model.parameters(),  lr=config[\"lr\"],momentum=config[\"opt_momentum\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.Nadam:\n",
    "        optimizer = torch.optim.NAdam(model.parameters(),  lr=config[\"lr\"],momentum_decay=config[\"opt_momentum\"])\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        with checkpoint.as_directory() as dir_path:\n",
    "            print(\"test\")\n",
    "            model_state, optimizer_state = torch.load(os.path.join(dir_path, \"checkpoint.pt\"))\n",
    "\n",
    "            # Load optimizer state (needed since we're using momentum),\n",
    "            # then set the `lr` and `momentum` according to the config.\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "            model.load_state_dict(model_state)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            if \"lr\" in config:\n",
    "                param_group[\"lr\"] = config[\"lr\"]\n",
    "            if \"opt_momentum\" in config:\n",
    "                param_group[\"opt_momentum\"] = config[\"opt_momentum\"]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        train_epoch(train,val,model, criterion ,optimizer,config[\"batch_size\"],config[\"epochs\"])  # Train the model\n",
    "\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        torch.save(\n",
    "            (model.state_dict(), optimizer.state_dict()), \"model/checkpoint.pt\")\n",
    "\n",
    "        mse,r2,rmse,pcip = get_metrics(X_test,y_test,model)  # Compute test accuracy\n",
    "        #optimise with pcip and r2\n",
    "        r2_pent =r2*100\n",
    "        score_avg = (pcip+r2_pent)/2\n",
    "        checkpoint = Checkpoint.from_directory(\"model\")\n",
    "\n",
    "        session.report({\"done\": pcip > 99 and rmse < 0.25,\"mse\": mse, \"r2\": r2,\"rmse\": rmse, \"pcip\": pcip,\"score_avg\":score_avg},checkpoint=checkpoint)  # Report to Tune\n",
    "\n",
    "##### RUN  ##############\n",
    "import ray\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "from ray.tune import tune\n",
    "import random\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "# Your original list of dimensions\n",
    "original_dims = [512, 256,  128, 64, 32,  16,  8, 512,256,128,64,28]\n",
    "# List of optimizer names\n",
    "optimizer_names = [\"Adams\", \"SGD\",\"Nadam\"]\n",
    "optimizer_names = [\"Adams\", \"Nadam\"]\n",
    "ray.init(num_cpus=12, num_gpus=0,_temp_dir=\"/ray\") # assign the total # of cpus and gpus, make sure you have ray.init in the beginning and ray.shutdown at the end\n",
    "sched = AsyncHyperBandScheduler(  time_attr=\"training_iteration\",\n",
    "    reduction_factor=2,\n",
    "    metric='rmse',\n",
    "    mode='min')  # set a scheduler\n",
    "\n",
    "perturbation_interval = 10\n",
    "#use population based training\n",
    "scheduler =PopulationBasedTraining(\n",
    "       time_attr=\"training_iteration\",\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    metric=\"rmse\",\n",
    "    mode=\"min\",\n",
    "    quantile_fraction=0.25,  # copy bottom % with top %\n",
    "    #hyperparam_mutation - for pbt\n",
    "    hyperparam_mutations={\n",
    "        # distribution for resampling\n",
    "        \"lr\": [0.0001, 0.1],\n",
    "        \"momentum\": [0.009,0.01],\n",
    "        \"opt_momentum\": [0.009,0.01],\n",
    "    }\n",
    ")\n",
    "algo =OptunaSearch(metric=[\"rmse\",\"pcip\"], mode=[\"min\",\"max\"])# HyperOptSearch() # if you want to use the Bayesian optimization, import BayesOptSearch instead\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "scheduler = HyperBandForBOHB(\n",
    "    time_attr=\"training_iteration\",\n",
    "    max_t=20,\n",
    "    #metric=\"rmse\",\n",
    "    #mode='min',\n",
    "    reduction_factor=0.25,\n",
    "    stop_last_trials=False,\n",
    ")\n",
    "scheduler = PB2(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    metric=\"score_avg\",\n",
    "    mode=\"max\",\n",
    "    quantile_fraction=0.4,  # copy bottom % with top %\n",
    "    #hyperparam_mutation - for pbt\n",
    "    hyperparam_bounds={\n",
    "        # distribution for resampling\n",
    "        \"lr\": [0.00001, 0.1],\n",
    "        \"momentum\": [0.009,0.01],\n",
    "        \"opt_momentum\": [0.009,0.01],\n",
    "    },\n",
    ")\n",
    "analysis = tune.run(\n",
    "        objective,   # the core training/testing of your model\n",
    "        #storage_path=os.getcwd(), # for saving the log files\n",
    "        name=\"pbt_clay\", # name for the result directory\n",
    "        #resume=\"REMOTE\",\n",
    "        #metric=\"rmse\",\n",
    "        #resume=\"PROMPT\",\n",
    "        #mode='min',\n",
    "        #search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        stop={\n",
    "                \"training_iteration\": 20,\n",
    "                \"done\": True,\n",
    "                \"rmse\": 0.05,\n",
    "        },\n",
    "        resources_per_trial={\n",
    "                \"cpu\": 1,\n",
    "                \"gpu\": 0\n",
    "         },\n",
    "        num_samples=30, # 50 trials\n",
    "        progress_reporter=ray.tune.JupyterNotebookReporter(metric=\"score_avg\",overwrite=True,max_report_frequency=7),\n",
    "        config={\n",
    "\n",
    "               \"lr\": ray.tune.loguniform(0.00001, 0.1),\n",
    "                  #\"pred_interval_constant\":ray.tune.loguniform(0.28, 12.05),\n",
    "                 \"momentum\": ray.tune.loguniform(0.009,0.01),\n",
    "                 \"weight_std\": ray.tune.loguniform(0.0001,10.02),\n",
    "                 \"weight_mean\": ray.tune.loguniform(0.0001,10.02),\n",
    "                 \"bias_mean\": ray.tune.loguniform(0.0001,10.02),\n",
    "                 \"bias_std\": ray.tune.loguniform(0.01, 17.7),\n",
    "                 \"batch_size\": ray.tune.randint(2,128),\n",
    "                 \"epochs\":ray.tune.randint(2,1000),\n",
    "                 \"dims\": ray.tune.sample_from(lambda spec: random.choices(original_dims,k=random.randint(2, 7))),\n",
    "                 \"optimiser\": ray.tune.sample_from(lambda spec: random.choice(optimizer_names)),\n",
    "                 \"checkpoint_interval\": perturbation_interval\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.0004056961680898992, 'momentum': 0.009068718679849, 'weight_std': 0.0015744750485797052, 'bias_std': 0.1327185360434992, 'batch_size': 86, 'num_dims': 3, 'epochs': 243, 'dims': [8, 32, 32, 256, 128, 8], 'optimiser': 'Adams', 'checkpoint_interval': 10, 'opt_momentum': 0.00921318133493377}\n",
      "Best trial final validation mse: 0.7724632620811462\n",
      "Best trial final validation r2: 0.9489880800247192\n",
      "Best trial final validation rmse: 0.8788988918420289\n",
      "Best trial final validation pcip: 91.08635097493037\n"
     ]
    }
   ],
   "source": [
    "best_trials = analysis.get_best_trial(\"score_avg\", \"max\", \"all\")\n",
    "print(f\"Best trial config: {best_trials.config}\")\n",
    "print(f\"Best trial final validation mse: {best_trials.last_result['mse']}\")\n",
    "print(f\"Best trial final validation r2: {best_trials.last_result['r2']}\")\n",
    "print(f\"Best trial final validation rmse: {best_trials.last_result['rmse']}\")\n",
    "print(f\"Best trial final validation pcip: {best_trials.last_result['pcip']}\")\n",
    "#print(f\"Best trial final validation score_avg: {best_trials.last_result['optimal_dim']}\")\n",
    "#print(f\"Best trial final validation l1_coef: {best_trial.last_result['l1_coef']}\")\n",
    "best_checkpoints = best_trials.checkpoint.to_air_checkpoint()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omondiagbep\\ray_results\\pbt_carbon\\objective_2aefe_00005_5_batch_size=86,bias_std=0.1327,epochs=243,lr=0.0004,momentum=0.0091,num_dims=3,weight_std=0.0016_2023-09-30_11-19-14\\checkpoint_000000\n"
     ]
    }
   ],
   "source": [
    "gpus_per_trial = 2\n",
    "best_trained_model=BayesianCNN(num_feature=X_train.shape[1],dims=best_trials.config[\"dims\"],\n",
    "                           weight_std=best_trials.config[\"weight_std\"],\n",
    "                          bias_std=best_trials.config[\"bias_std\"],\n",
    "                               bias_mean=best_trials.config[\"bias_mean\"],weight_mean=best_trials.config[\"weight_mean\"])\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "best_trained_model.to(device)\n",
    "with best_checkpoints.as_directory() as dir_path:\n",
    "    print(dir_path)\n",
    "    model_state, optimizer_state = torch.load(os.path.join(dir_path, \"checkpoint.pt\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "best_checkpoint_data = best_checkpoints.to_dict()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def train_func(model,optimizer_=None,n_epochs = 1000,batch_size = 16,lr=0.0001):\n",
    "    loss_fn = nn.MSELoss()  # mean square error\n",
    "\n",
    "    if optimizer_ is None:\n",
    "        #Regularization helps prevent overfitting by penalizing large weights\n",
    "        optimizer = torch.optim.Adam(model.parameters(),  lr=lr)\n",
    "    if optimizer_ == Optimiser.RMSprop:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), momentum=0.0009, lr=lr)\n",
    "    elif optimizer_ == Optimiser.LBFGS:\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(),  lr=lr)\n",
    "    elif optimizer_ == Optimiser.Adams:\n",
    "        optimizer = torch.optim.Adam(model.parameters(),  lr=lr)\n",
    "    elif optimizer_ == Optimiser.SGD:\n",
    "        optimizer = torch.optim.SGD(model.parameters(),  lr=lr,momentum=0.0009)\n",
    "    elif optimizer_ == Optimiser.Nadam:\n",
    "        optimizer = torch.optim.NAdam(model.parameters(),  lr=lr,momentum_decay=0.0009)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_mse = np.inf # init to infinity\n",
    "\n",
    "    history = []\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    early_stop=False\n",
    "    n_epochs_stop = 40\n",
    "    min_val_loss=float('inf')\n",
    "    best_r2 = 0.0000\n",
    "    best_pcip = 0.0000\n",
    "    best_score_avg =0.0000\n",
    "    criterion=nn.MSELoss()\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "    val = torch.utils.data.TensorDataset(X_val,y_val)\n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val, batch_size=batch_size)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss =0\n",
    "        val_loss=0\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] Loss: {loss.item():.5f}\")\n",
    "\n",
    "        for inputs, targets in val_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "        avg_val_loss = val_loss / len(inputs)\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] Val Loss: {loss.item():.5f}\")\n",
    "\n",
    "        #at the start\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            epochs_no_improve = 0\n",
    "            min_val_loss = avg_val_loss\n",
    "            print(\"restarting counter\")\n",
    "            print(epochs_no_improve)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(\"increasing counter\")\n",
    "            print(epochs_no_improve)\n",
    "\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!')\n",
    "            early_stop = True\n",
    "            break\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        mse,r2,rmse,pcip = get_metrics(X_val,y_val,model)\n",
    "        score_avg= ((r2*100)+pcip)/2\n",
    "        print(f'r2: {r2:.4f}')\n",
    "        print(f'pcip: {pcip:.4f}')\n",
    "        y_pred = model(X_val)\n",
    "\n",
    "        mse = loss_fn(y_pred, y_val)\n",
    "        mse = float(mse)\n",
    "        rmse= np.sqrt(mse)\n",
    "        print(f'RMSE: {rmse:.4f}')\n",
    "        history.append(rmse)\n",
    "        if rmse < best_mse:\n",
    "            best_mse = rmse\n",
    "\n",
    "        if r2 > best_r2 :\n",
    "            best_r2 = r2\n",
    "            print(\"save rmse state\")\n",
    "            modelrmse_state_dict = model.state_dict()\n",
    "        if pcip > best_pcip :\n",
    "            best_pcip = pcip\n",
    "            print(\"save pcip state\")\n",
    "            modelpcip_state_dict = model.state_dict()\n",
    "        if score_avg  > best_score_avg:\n",
    "            best_score_avg = score_avg\n",
    "            print(\"save score avg state\")\n",
    "            modelscore_avg_state_dict = model.state_dict()\n",
    "\n",
    "        if early_stop:\n",
    "            print('Training stopped early.')\n",
    "\n",
    "\n",
    "        print(f'Best RMSE so far: {best_mse:.4f}')\n",
    "        print(f'Best r2 so far: {best_r2:.4f}')\n",
    "        print(f'best_score_avg so far: {best_score_avg:.4f}')\n",
    "        print(f'Best pcip so far: {best_pcip:.4f}')\n",
    "\n",
    "\n",
    "    #modelrmse_state_dict.update(modelpcip_state_dict)\n",
    "\n",
    "    # Load the updated state_dict back into model\n",
    "    #model.load_state_dict(modelrmse_state_dict)\n",
    "    model.load_state_dict(modelscore_avg_state_dict)\n",
    "    #model_pcip=model.load_state_dict(modelpcip_state_dict)\n",
    "    #model_r2=model.load_state_dict(modelrmse_state_dict)\n",
    "    print(\"RMSE: %.2f\" % best_mse)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history)\n",
    "    plt.show()\n",
    "    return model,history,best_mse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best architecture found\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_feature: int, dims=[512, 28, 64], bias_mean=0.0003,weight_mean=0.0002,weight_std=0.0001, bias_std=0.0005):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "\n",
    "        # Define the fully connected layers based on the specified dimensions\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = num_feature\n",
    "        for out_dim in dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.fc_out = nn.Linear(in_dim, 1)  # Output layer for pret\n",
    "\n",
    "        # Update the dimensions of weight_mu and weight_rho to match the output dimension\n",
    "        self.weight_dim = out_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_std = weight_std\n",
    "        self.bias_std = bias_std\n",
    "        self.weight_mean = weight_mean\n",
    "        self.bias_mean = bias_mean\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weight means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.weight_mu, self.weight_mean, std=self.weight_std)\n",
    "        nn.init.normal_(self.weight_rho, self.weight_mean, std=self.weight_std)\n",
    "\n",
    "        # Initialize bias means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.bias_mu, self.bias_mean, std=self.bias_std)\n",
    "        nn.init.normal_(self.bias_rho, self.bias_mean, std=self.bias_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "\n",
    "        # Pass through fully connected layers with specified dimensions\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.rrelu(fc_layer(x))\n",
    "            #Dropout randomly deactivates a fraction of neurons during training,\n",
    "            # which can help prevent overfitting.\n",
    "            x = F.dropout(x,p=0.30)\n",
    "\n",
    "\n",
    "        # Re-parameterization trick for sampling weights\n",
    "        #To reduce variability, you can reduce the standard deviation of this sampling.\n",
    "        weight_epsilon = Normal(0.0,1).sample(self.weight_mu.size())\n",
    "        weight_sigma = torch.log(1 + torch.exp(self.weight_rho))\n",
    "        weight = self.weight_mu + (weight_sigma * weight_epsilon)\n",
    "\n",
    "        bias_epsilon = Normal(0.0,1).sample(self.bias_mu.size())\n",
    "        bias_sigma = torch.log(1 + torch.exp(self.bias_rho))\n",
    "        bias = self.bias_mu + (bias_sigma * bias_epsilon)\n",
    "\n",
    "        # Enforce non-negativity on weights and biases\n",
    "        weight = torch.clamp(weight, min=0)\n",
    "        bias = torch.clamp(bias, min=0)\n",
    "\n",
    "\n",
    "        # Final linear layer operation\n",
    "        output = F.linear(x, weight, bias)\n",
    "        # Apply sigmoid activation to squash values between 0 and 1\n",
    "        #output = torch.sigmoid(output)\n",
    "\n",
    "        # Scale the values to the desired range (0 to 100)\n",
    "        #output = output * 100\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Optimiser(str, Enum):\n",
    "    Adams = \"Adams\"\n",
    "    SGD =\"SGD\"\n",
    "    Adadelta = \"Adadelta\"\n",
    "    Adagrad = \"Adagrad\"\n",
    "    Adamax = \"Adamax\"\n",
    "    Nadam = \"Nadam\"\n",
    "    Ftrl = \"Ftrl\"\n",
    "    RMSprop=\"RMSprop\"\n",
    "    LBFGS =\"LBFGS\"\n",
    "    #LBFGS =\"LBFGS\"\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "#fine tune if needed - not use"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/325] Loss: 140.58434\n",
      "Epoch [1/325] Val Loss: 65.43999\n",
      "restarting counter\n",
      "0\n",
      "MSE: 149.1555,R2: 0.3580,RMSE: 12.2129, PCIP: 50.66,score_avg:43.2303\n",
      "r2: 0.3580\n",
      "pcip: 50.6623\n",
      "RMSE: 12.3897\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 12.3897\n",
      "Best r2 so far: 0.3580\n",
      "best_score_avg so far: 43.2303\n",
      "Best pcip so far: 50.6623\n",
      "Epoch [2/325] Loss: 111.82915\n",
      "Epoch [2/325] Val Loss: 74.62492\n",
      "restarting counter\n",
      "0\n",
      "MSE: 110.2881,R2: 0.5253,RMSE: 10.5018, PCIP: 59.27,score_avg:55.8999\n",
      "r2: 0.5253\n",
      "pcip: 59.2715\n",
      "RMSE: 10.9641\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 10.9641\n",
      "Best r2 so far: 0.5253\n",
      "best_score_avg so far: 55.8999\n",
      "Best pcip so far: 59.2715\n",
      "Epoch [3/325] Loss: 88.54852\n",
      "Epoch [3/325] Val Loss: 70.68606\n",
      "restarting counter\n",
      "0\n",
      "MSE: 98.4600,R2: 0.5762,RMSE: 9.9227, PCIP: 65.23,score_avg:61.4256\n",
      "r2: 0.5762\n",
      "pcip: 65.2318\n",
      "RMSE: 9.8904\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 9.8904\n",
      "Best r2 so far: 0.5762\n",
      "best_score_avg so far: 61.4256\n",
      "Best pcip so far: 65.2318\n",
      "Epoch [4/325] Loss: 44.38005\n",
      "Epoch [4/325] Val Loss: 56.97098\n",
      "restarting counter\n",
      "0\n",
      "MSE: 78.8720,R2: 0.6605,RMSE: 8.8810, PCIP: 75.99,score_avg:71.0221\n",
      "r2: 0.6605\n",
      "pcip: 75.9934\n",
      "RMSE: 9.3989\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 9.3989\n",
      "Best r2 so far: 0.6605\n",
      "best_score_avg so far: 71.0221\n",
      "Best pcip so far: 75.9934\n",
      "Epoch [5/325] Loss: 47.59607\n",
      "Epoch [5/325] Val Loss: 60.48652\n",
      "restarting counter\n",
      "0\n",
      "MSE: 97.4246,R2: 0.5807,RMSE: 9.8704, PCIP: 76.99,score_avg:67.5259\n",
      "r2: 0.5807\n",
      "pcip: 76.9868\n",
      "RMSE: 8.7691\n",
      "save pcip state\n",
      "Best RMSE so far: 8.7691\n",
      "Best r2 so far: 0.6605\n",
      "best_score_avg so far: 71.0221\n",
      "Best pcip so far: 76.9868\n",
      "Epoch [6/325] Loss: 42.99815\n",
      "Epoch [6/325] Val Loss: 121.60207\n",
      "increasing counter\n",
      "1\n",
      "MSE: 82.5052,R2: 0.6449,RMSE: 9.0832, PCIP: 80.46,score_avg:72.4752\n",
      "r2: 0.6449\n",
      "pcip: 80.4636\n",
      "RMSE: 8.6787\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 8.6787\n",
      "Best r2 so far: 0.6605\n",
      "best_score_avg so far: 72.4752\n",
      "Best pcip so far: 80.4636\n",
      "Epoch [7/325] Loss: 51.88683\n",
      "Epoch [7/325] Val Loss: 77.05803\n",
      "restarting counter\n",
      "0\n",
      "MSE: 70.9635,R2: 0.6945,RMSE: 8.4240, PCIP: 80.13,score_avg:74.7937\n",
      "r2: 0.6945\n",
      "pcip: 80.1325\n",
      "RMSE: 8.3392\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 8.3392\n",
      "Best r2 so far: 0.6945\n",
      "best_score_avg so far: 74.7937\n",
      "Best pcip so far: 80.4636\n",
      "Epoch [8/325] Loss: 33.83475\n",
      "Epoch [8/325] Val Loss: 94.79399\n",
      "increasing counter\n",
      "1\n",
      "MSE: 71.4100,R2: 0.6926,RMSE: 8.4504, PCIP: 80.30,score_avg:74.7804\n",
      "r2: 0.6926\n",
      "pcip: 80.2980\n",
      "RMSE: 8.5682\n",
      "Best RMSE so far: 8.3392\n",
      "Best r2 so far: 0.6945\n",
      "best_score_avg so far: 74.7937\n",
      "Best pcip so far: 80.4636\n",
      "Epoch [9/325] Loss: 50.98727\n",
      "Epoch [9/325] Val Loss: 81.02338\n",
      "increasing counter\n",
      "2\n",
      "MSE: 64.7562,R2: 0.7213,RMSE: 8.0471, PCIP: 80.13,score_avg:76.1296\n",
      "r2: 0.7213\n",
      "pcip: 80.1325\n",
      "RMSE: 8.5688\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 8.3392\n",
      "Best r2 so far: 0.7213\n",
      "best_score_avg so far: 76.1296\n",
      "Best pcip so far: 80.4636\n",
      "Epoch [10/325] Loss: 24.64949\n",
      "Epoch [10/325] Val Loss: 71.30461\n",
      "restarting counter\n",
      "0\n",
      "MSE: 64.0900,R2: 0.7241,RMSE: 8.0056, PCIP: 80.79,score_avg:76.6041\n",
      "r2: 0.7241\n",
      "pcip: 80.7947\n",
      "RMSE: 7.8067\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 7.8067\n",
      "Best r2 so far: 0.7241\n",
      "best_score_avg so far: 76.6041\n",
      "Best pcip so far: 80.7947\n",
      "Epoch [11/325] Loss: 39.63480\n",
      "Epoch [11/325] Val Loss: 102.81221\n",
      "restarting counter\n",
      "0\n",
      "MSE: 72.1345,R2: 0.6895,RMSE: 8.4932, PCIP: 83.44,score_avg:76.1973\n",
      "r2: 0.6895\n",
      "pcip: 83.4437\n",
      "RMSE: 7.5822\n",
      "save pcip state\n",
      "Best RMSE so far: 7.5822\n",
      "Best r2 so far: 0.7241\n",
      "best_score_avg so far: 76.6041\n",
      "Best pcip so far: 83.4437\n",
      "Epoch [12/325] Loss: 31.02305\n",
      "Epoch [12/325] Val Loss: 75.50626\n",
      "restarting counter\n",
      "0\n",
      "MSE: 69.4615,R2: 0.7010,RMSE: 8.3344, PCIP: 82.95,score_avg:76.5242\n",
      "r2: 0.7010\n",
      "pcip: 82.9470\n",
      "RMSE: 7.9657\n",
      "Best RMSE so far: 7.5822\n",
      "Best r2 so far: 0.7241\n",
      "best_score_avg so far: 76.6041\n",
      "Best pcip so far: 83.4437\n",
      "Epoch [13/325] Loss: 47.30730\n",
      "Epoch [13/325] Val Loss: 76.91818\n",
      "restarting counter\n",
      "0\n",
      "MSE: 56.5098,R2: 0.7568,RMSE: 7.5173, PCIP: 83.77,score_avg:79.7255\n",
      "r2: 0.7568\n",
      "pcip: 83.7748\n",
      "RMSE: 7.3522\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 7.3522\n",
      "Best r2 so far: 0.7568\n",
      "best_score_avg so far: 79.7255\n",
      "Best pcip so far: 83.7748\n",
      "Epoch [14/325] Loss: 52.14264\n",
      "Epoch [14/325] Val Loss: 84.95815\n",
      "increasing counter\n",
      "1\n",
      "MSE: 59.3390,R2: 0.7446,RMSE: 7.7032, PCIP: 83.11,score_avg:78.7855\n",
      "r2: 0.7446\n",
      "pcip: 83.1126\n",
      "RMSE: 7.7087\n",
      "Best RMSE so far: 7.3522\n",
      "Best r2 so far: 0.7568\n",
      "best_score_avg so far: 79.7255\n",
      "Best pcip so far: 83.7748\n",
      "Epoch [15/325] Loss: 33.99337\n",
      "Epoch [15/325] Val Loss: 109.90147\n",
      "increasing counter\n",
      "2\n",
      "MSE: 55.0076,R2: 0.7632,RMSE: 7.4167, PCIP: 82.95,score_avg:79.6349\n",
      "r2: 0.7632\n",
      "pcip: 82.9470\n",
      "RMSE: 7.1050\n",
      "save rmse state\n",
      "Best RMSE so far: 7.1050\n",
      "Best r2 so far: 0.7632\n",
      "best_score_avg so far: 79.7255\n",
      "Best pcip so far: 83.7748\n",
      "Epoch [16/325] Loss: 44.61516\n",
      "Epoch [16/325] Val Loss: 54.00914\n",
      "restarting counter\n",
      "0\n",
      "MSE: 58.2244,R2: 0.7494,RMSE: 7.6305, PCIP: 83.77,score_avg:79.3565\n",
      "r2: 0.7494\n",
      "pcip: 83.7748\n",
      "RMSE: 7.2665\n",
      "Best RMSE so far: 7.1050\n",
      "Best r2 so far: 0.7632\n",
      "best_score_avg so far: 79.7255\n",
      "Best pcip so far: 83.7748\n",
      "Epoch [17/325] Loss: 32.61443\n",
      "Epoch [17/325] Val Loss: 46.01353\n",
      "restarting counter\n",
      "0\n",
      "MSE: 61.3803,R2: 0.7358,RMSE: 7.8346, PCIP: 83.44,score_avg:78.5118\n",
      "r2: 0.7358\n",
      "pcip: 83.4437\n",
      "RMSE: 7.0104\n",
      "Best RMSE so far: 7.0104\n",
      "Best r2 so far: 0.7632\n",
      "best_score_avg so far: 79.7255\n",
      "Best pcip so far: 83.7748\n",
      "Epoch [18/325] Loss: 32.35582\n",
      "Epoch [18/325] Val Loss: 30.01368\n",
      "restarting counter\n",
      "0\n",
      "MSE: 51.0023,R2: 0.7805,RMSE: 7.1416, PCIP: 84.27,score_avg:81.1592\n",
      "r2: 0.7805\n",
      "pcip: 84.2715\n",
      "RMSE: 6.9091\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.9091\n",
      "Best r2 so far: 0.7805\n",
      "best_score_avg so far: 81.1592\n",
      "Best pcip so far: 84.2715\n",
      "Epoch [19/325] Loss: 40.57845\n",
      "Epoch [19/325] Val Loss: 37.96210\n",
      "increasing counter\n",
      "1\n",
      "MSE: 48.9887,R2: 0.7891,RMSE: 6.9992, PCIP: 84.60,score_avg:81.7581\n",
      "r2: 0.7891\n",
      "pcip: 84.6026\n",
      "RMSE: 6.4532\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.7891\n",
      "best_score_avg so far: 81.7581\n",
      "Best pcip so far: 84.6026\n",
      "Epoch [20/325] Loss: 37.92995\n",
      "Epoch [20/325] Val Loss: 91.42720\n",
      "increasing counter\n",
      "2\n",
      "MSE: 45.4684,R2: 0.8043,RMSE: 6.7430, PCIP: 83.61,score_avg:82.0191\n",
      "r2: 0.8043\n",
      "pcip: 83.6093\n",
      "RMSE: 7.0004\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8043\n",
      "best_score_avg so far: 82.0191\n",
      "Best pcip so far: 84.6026\n",
      "Epoch [21/325] Loss: 29.55662\n",
      "Epoch [21/325] Val Loss: 71.78960\n",
      "restarting counter\n",
      "0\n",
      "MSE: 44.2376,R2: 0.8096,RMSE: 6.6511, PCIP: 83.94,score_avg:82.4495\n",
      "r2: 0.8096\n",
      "pcip: 83.9404\n",
      "RMSE: 7.0219\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8096\n",
      "best_score_avg so far: 82.4495\n",
      "Best pcip so far: 84.6026\n",
      "Epoch [22/325] Loss: 28.15416\n",
      "Epoch [22/325] Val Loss: 92.40358\n",
      "increasing counter\n",
      "1\n",
      "MSE: 46.9744,R2: 0.7978,RMSE: 6.8538, PCIP: 83.77,score_avg:81.7777\n",
      "r2: 0.7978\n",
      "pcip: 83.7748\n",
      "RMSE: 7.1215\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8096\n",
      "best_score_avg so far: 82.4495\n",
      "Best pcip so far: 84.6026\n",
      "Epoch [23/325] Loss: 30.24893\n",
      "Epoch [23/325] Val Loss: 47.51410\n",
      "increasing counter\n",
      "2\n",
      "MSE: 43.2525,R2: 0.8138,RMSE: 6.5767, PCIP: 83.94,score_avg:82.6615\n",
      "r2: 0.8138\n",
      "pcip: 83.9404\n",
      "RMSE: 6.7909\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8138\n",
      "best_score_avg so far: 82.6615\n",
      "Best pcip so far: 84.6026\n",
      "Epoch [24/325] Loss: 28.30630\n",
      "Epoch [24/325] Val Loss: 53.78640\n",
      "increasing counter\n",
      "3\n",
      "MSE: 46.1134,R2: 0.8015,RMSE: 6.7907, PCIP: 86.09,score_avg:83.1220\n",
      "r2: 0.8015\n",
      "pcip: 86.0927\n",
      "RMSE: 7.2521\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8138\n",
      "best_score_avg so far: 83.1220\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [25/325] Loss: 30.84707\n",
      "Epoch [25/325] Val Loss: 73.14436\n",
      "increasing counter\n",
      "4\n",
      "MSE: 47.6639,R2: 0.7948,RMSE: 6.9039, PCIP: 83.94,score_avg:81.7121\n",
      "r2: 0.7948\n",
      "pcip: 83.9404\n",
      "RMSE: 6.7454\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8138\n",
      "best_score_avg so far: 83.1220\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [26/325] Loss: 26.01859\n",
      "Epoch [26/325] Val Loss: 76.36475\n",
      "increasing counter\n",
      "5\n",
      "MSE: 46.1252,R2: 0.8015,RMSE: 6.7916, PCIP: 84.44,score_avg:82.2916\n",
      "r2: 0.8015\n",
      "pcip: 84.4371\n",
      "RMSE: 7.2751\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8138\n",
      "best_score_avg so far: 83.1220\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [27/325] Loss: 35.28713\n",
      "Epoch [27/325] Val Loss: 33.70139\n",
      "restarting counter\n",
      "0\n",
      "MSE: 48.7150,R2: 0.7903,RMSE: 6.9796, PCIP: 81.95,score_avg:80.4925\n",
      "r2: 0.7903\n",
      "pcip: 81.9536\n",
      "RMSE: 6.7275\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8138\n",
      "best_score_avg so far: 83.1220\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [28/325] Loss: 29.40939\n",
      "Epoch [28/325] Val Loss: 66.70714\n",
      "restarting counter\n",
      "0\n",
      "MSE: 43.0037,R2: 0.8149,RMSE: 6.5577, PCIP: 83.28,score_avg:82.3839\n",
      "r2: 0.8149\n",
      "pcip: 83.2781\n",
      "RMSE: 6.4613\n",
      "save rmse state\n",
      "Best RMSE so far: 6.4532\n",
      "Best r2 so far: 0.8149\n",
      "best_score_avg so far: 83.1220\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [29/325] Loss: 33.58117\n",
      "Epoch [29/325] Val Loss: 64.92705\n",
      "increasing counter\n",
      "1\n",
      "MSE: 37.5577,R2: 0.8383,RMSE: 6.1284, PCIP: 83.44,score_avg:83.6388\n",
      "r2: 0.8383\n",
      "pcip: 83.4437\n",
      "RMSE: 6.3174\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.3174\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.6388\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [30/325] Loss: 49.86534\n",
      "Epoch [30/325] Val Loss: 86.36237\n",
      "increasing counter\n",
      "2\n",
      "MSE: 46.6982,R2: 0.7990,RMSE: 6.8336, PCIP: 83.28,score_avg:81.5888\n",
      "r2: 0.7990\n",
      "pcip: 83.2781\n",
      "RMSE: 6.0997\n",
      "Best RMSE so far: 6.0997\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.6388\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [31/325] Loss: 25.15461\n",
      "Epoch [31/325] Val Loss: 60.44749\n",
      "restarting counter\n",
      "0\n",
      "MSE: 41.1577,R2: 0.8228,RMSE: 6.4154, PCIP: 85.43,score_avg:83.8574\n",
      "r2: 0.8228\n",
      "pcip: 85.4305\n",
      "RMSE: 6.2575\n",
      "save score avg state\n",
      "Best RMSE so far: 6.0997\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.8574\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [32/325] Loss: 28.09921\n",
      "Epoch [32/325] Val Loss: 46.69564\n",
      "increasing counter\n",
      "1\n",
      "MSE: 50.6296,R2: 0.7821,RMSE: 7.1155, PCIP: 85.93,score_avg:82.0672\n",
      "r2: 0.7821\n",
      "pcip: 85.9272\n",
      "RMSE: 6.2640\n",
      "Best RMSE so far: 6.0997\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.8574\n",
      "Best pcip so far: 86.0927\n",
      "Epoch [33/325] Loss: 21.59462\n",
      "Epoch [33/325] Val Loss: 50.64254\n",
      "restarting counter\n",
      "0\n",
      "MSE: 42.5404,R2: 0.8169,RMSE: 6.5223, PCIP: 86.26,score_avg:83.9737\n",
      "r2: 0.8169\n",
      "pcip: 86.2583\n",
      "RMSE: 6.0599\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.9737\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [34/325] Loss: 28.50222\n",
      "Epoch [34/325] Val Loss: 59.02338\n",
      "increasing counter\n",
      "1\n",
      "MSE: 43.7637,R2: 0.8116,RMSE: 6.6154, PCIP: 84.44,score_avg:82.7998\n",
      "r2: 0.8116\n",
      "pcip: 84.4371\n",
      "RMSE: 6.5900\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.9737\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [35/325] Loss: 38.51149\n",
      "Epoch [35/325] Val Loss: 64.40800\n",
      "increasing counter\n",
      "2\n",
      "MSE: 39.6337,R2: 0.8294,RMSE: 6.2955, PCIP: 83.11,score_avg:83.0264\n",
      "r2: 0.8294\n",
      "pcip: 83.1126\n",
      "RMSE: 6.6684\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.9737\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [36/325] Loss: 43.46060\n",
      "Epoch [36/325] Val Loss: 62.68829\n",
      "increasing counter\n",
      "3\n",
      "MSE: 41.4919,R2: 0.8214,RMSE: 6.4414, PCIP: 83.94,score_avg:83.0404\n",
      "r2: 0.8214\n",
      "pcip: 83.9404\n",
      "RMSE: 6.3550\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.9737\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [37/325] Loss: 19.41663\n",
      "Epoch [37/325] Val Loss: 56.22648\n",
      "increasing counter\n",
      "4\n",
      "MSE: 42.2528,R2: 0.8181,RMSE: 6.5002, PCIP: 84.60,score_avg:83.2078\n",
      "r2: 0.8181\n",
      "pcip: 84.6026\n",
      "RMSE: 6.9106\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.9737\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [38/325] Loss: 18.64834\n",
      "Epoch [38/325] Val Loss: 62.53817\n",
      "increasing counter\n",
      "5\n",
      "MSE: 47.4770,R2: 0.7956,RMSE: 6.8904, PCIP: 84.27,score_avg:81.9179\n",
      "r2: 0.7956\n",
      "pcip: 84.2715\n",
      "RMSE: 6.4190\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 83.9737\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [39/325] Loss: 22.01951\n",
      "Epoch [39/325] Val Loss: 67.04973\n",
      "increasing counter\n",
      "6\n",
      "MSE: 38.0285,R2: 0.8363,RMSE: 6.1667, PCIP: 85.26,score_avg:84.4481\n",
      "r2: 0.8363\n",
      "pcip: 85.2649\n",
      "RMSE: 6.5162\n",
      "save score avg state\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8383\n",
      "best_score_avg so far: 84.4481\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [40/325] Loss: 25.31204\n",
      "Epoch [40/325] Val Loss: 65.26980\n",
      "increasing counter\n",
      "7\n",
      "MSE: 36.7844,R2: 0.8417,RMSE: 6.0650, PCIP: 83.77,score_avg:83.9708\n",
      "r2: 0.8417\n",
      "pcip: 83.7748\n",
      "RMSE: 6.6930\n",
      "save rmse state\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8417\n",
      "best_score_avg so far: 84.4481\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [41/325] Loss: 25.66031\n",
      "Epoch [41/325] Val Loss: 68.00693\n",
      "increasing counter\n",
      "8\n",
      "MSE: 39.5895,R2: 0.8296,RMSE: 6.2920, PCIP: 85.93,score_avg:84.4432\n",
      "r2: 0.8296\n",
      "pcip: 85.9272\n",
      "RMSE: 6.2695\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8417\n",
      "best_score_avg so far: 84.4481\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [42/325] Loss: 19.33833\n",
      "Epoch [42/325] Val Loss: 74.61973\n",
      "increasing counter\n",
      "9\n",
      "MSE: 43.3709,R2: 0.8133,RMSE: 6.5857, PCIP: 83.94,score_avg:82.6360\n",
      "r2: 0.8133\n",
      "pcip: 83.9404\n",
      "RMSE: 6.4376\n",
      "Best RMSE so far: 6.0599\n",
      "Best r2 so far: 0.8417\n",
      "best_score_avg so far: 84.4481\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [43/325] Loss: 30.07110\n",
      "Epoch [43/325] Val Loss: 70.44408\n",
      "increasing counter\n",
      "10\n",
      "MSE: 36.7214,R2: 0.8419,RMSE: 6.0598, PCIP: 84.60,score_avg:84.3983\n",
      "r2: 0.8419\n",
      "pcip: 84.6026\n",
      "RMSE: 6.0128\n",
      "save rmse state\n",
      "Best RMSE so far: 6.0128\n",
      "Best r2 so far: 0.8419\n",
      "best_score_avg so far: 84.4481\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [44/325] Loss: 21.03977\n",
      "Epoch [44/325] Val Loss: 57.64315\n",
      "restarting counter\n",
      "0\n",
      "MSE: 35.8437,R2: 0.8457,RMSE: 5.9870, PCIP: 84.60,score_avg:84.5872\n",
      "r2: 0.8457\n",
      "pcip: 84.6026\n",
      "RMSE: 6.3557\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 6.0128\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.5872\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [45/325] Loss: 25.01461\n",
      "Epoch [45/325] Val Loss: 62.11992\n",
      "increasing counter\n",
      "1\n",
      "MSE: 38.6405,R2: 0.8337,RMSE: 6.2162, PCIP: 83.44,score_avg:83.4058\n",
      "r2: 0.8337\n",
      "pcip: 83.4437\n",
      "RMSE: 6.4143\n",
      "Best RMSE so far: 6.0128\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.5872\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [46/325] Loss: 22.60052\n",
      "Epoch [46/325] Val Loss: 60.66125\n",
      "increasing counter\n",
      "2\n",
      "MSE: 35.8431,R2: 0.8457,RMSE: 5.9869, PCIP: 83.77,score_avg:84.1734\n",
      "r2: 0.8457\n",
      "pcip: 83.7748\n",
      "RMSE: 5.9418\n",
      "save rmse state\n",
      "Best RMSE so far: 5.9418\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.5872\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [47/325] Loss: 22.02290\n",
      "Epoch [47/325] Val Loss: 76.29324\n",
      "increasing counter\n",
      "3\n",
      "MSE: 42.6055,R2: 0.8166,RMSE: 6.5273, PCIP: 85.76,score_avg:83.7114\n",
      "r2: 0.8166\n",
      "pcip: 85.7616\n",
      "RMSE: 5.8813\n",
      "Best RMSE so far: 5.8813\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.5872\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [48/325] Loss: 40.63646\n",
      "Epoch [48/325] Val Loss: 25.75248\n",
      "restarting counter\n",
      "0\n",
      "MSE: 39.1614,R2: 0.8314,RMSE: 6.2579, PCIP: 86.09,score_avg:84.6181\n",
      "r2: 0.8314\n",
      "pcip: 86.0927\n",
      "RMSE: 5.9233\n",
      "save score avg state\n",
      "Best RMSE so far: 5.8813\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.6181\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [49/325] Loss: 22.60220\n",
      "Epoch [49/325] Val Loss: 57.03112\n",
      "increasing counter\n",
      "1\n",
      "MSE: 35.9669,R2: 0.8452,RMSE: 5.9972, PCIP: 82.78,score_avg:83.6500\n",
      "r2: 0.8452\n",
      "pcip: 82.7815\n",
      "RMSE: 6.4281\n",
      "Best RMSE so far: 5.8813\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.6181\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [50/325] Loss: 39.37322\n",
      "Epoch [50/325] Val Loss: 24.99364\n",
      "increasing counter\n",
      "2\n",
      "MSE: 40.1803,R2: 0.8271,RMSE: 6.3388, PCIP: 83.94,score_avg:83.3227\n",
      "r2: 0.8271\n",
      "pcip: 83.9404\n",
      "RMSE: 5.8630\n",
      "Best RMSE so far: 5.8630\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.6181\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [51/325] Loss: 32.55309\n",
      "Epoch [51/325] Val Loss: 47.32840\n",
      "increasing counter\n",
      "3\n",
      "MSE: 43.5031,R2: 0.8127,RMSE: 6.5957, PCIP: 83.77,score_avg:82.5248\n",
      "r2: 0.8127\n",
      "pcip: 83.7748\n",
      "RMSE: 6.0040\n",
      "Best RMSE so far: 5.8630\n",
      "Best r2 so far: 0.8457\n",
      "best_score_avg so far: 84.6181\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [52/325] Loss: 18.24156\n",
      "Epoch [52/325] Val Loss: 45.92943\n",
      "increasing counter\n",
      "4\n",
      "MSE: 33.5734,R2: 0.8555,RMSE: 5.7943, PCIP: 85.60,score_avg:85.5725\n",
      "r2: 0.8555\n",
      "pcip: 85.5960\n",
      "RMSE: 6.3101\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 5.8630\n",
      "Best r2 so far: 0.8555\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [53/325] Loss: 26.68047\n",
      "Epoch [53/325] Val Loss: 60.91473\n",
      "increasing counter\n",
      "5\n",
      "MSE: 41.3750,R2: 0.8219,RMSE: 6.4323, PCIP: 84.27,score_avg:83.2312\n",
      "r2: 0.8219\n",
      "pcip: 84.2715\n",
      "RMSE: 6.3389\n",
      "Best RMSE so far: 5.8630\n",
      "Best r2 so far: 0.8555\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [54/325] Loss: 37.33247\n",
      "Epoch [54/325] Val Loss: 47.91511\n",
      "increasing counter\n",
      "6\n",
      "MSE: 38.4875,R2: 0.8343,RMSE: 6.2038, PCIP: 84.60,score_avg:84.0182\n",
      "r2: 0.8343\n",
      "pcip: 84.6026\n",
      "RMSE: 6.4314\n",
      "Best RMSE so far: 5.8630\n",
      "Best r2 so far: 0.8555\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [55/325] Loss: 24.68081\n",
      "Epoch [55/325] Val Loss: 52.39962\n",
      "increasing counter\n",
      "7\n",
      "MSE: 34.9497,R2: 0.8496,RMSE: 5.9118, PCIP: 84.93,score_avg:84.9451\n",
      "r2: 0.8496\n",
      "pcip: 84.9338\n",
      "RMSE: 6.0946\n",
      "Best RMSE so far: 5.8630\n",
      "Best r2 so far: 0.8555\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [56/325] Loss: 22.69008\n",
      "Epoch [56/325] Val Loss: 72.75436\n",
      "increasing counter\n",
      "8\n",
      "MSE: 38.7124,R2: 0.8334,RMSE: 6.2219, PCIP: 84.11,score_avg:83.7214\n",
      "r2: 0.8334\n",
      "pcip: 84.1060\n",
      "RMSE: 5.7633\n",
      "Best RMSE so far: 5.7633\n",
      "Best r2 so far: 0.8555\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [57/325] Loss: 19.32818\n",
      "Epoch [57/325] Val Loss: 52.33800\n",
      "increasing counter\n",
      "9\n",
      "MSE: 33.1594,R2: 0.8573,RMSE: 5.7584, PCIP: 84.60,score_avg:85.1649\n",
      "r2: 0.8573\n",
      "pcip: 84.6026\n",
      "RMSE: 5.8944\n",
      "save rmse state\n",
      "Best RMSE so far: 5.7633\n",
      "Best r2 so far: 0.8573\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [58/325] Loss: 19.39086\n",
      "Epoch [58/325] Val Loss: 30.30434\n",
      "increasing counter\n",
      "10\n",
      "MSE: 41.5610,R2: 0.8211,RMSE: 6.4468, PCIP: 83.77,score_avg:82.9428\n",
      "r2: 0.8211\n",
      "pcip: 83.7748\n",
      "RMSE: 6.0856\n",
      "Best RMSE so far: 5.7633\n",
      "Best r2 so far: 0.8573\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [59/325] Loss: 22.51461\n",
      "Epoch [59/325] Val Loss: 35.05914\n",
      "increasing counter\n",
      "11\n",
      "MSE: 33.3946,R2: 0.8563,RMSE: 5.7788, PCIP: 83.77,score_avg:84.7003\n",
      "r2: 0.8563\n",
      "pcip: 83.7748\n",
      "RMSE: 6.2328\n",
      "Best RMSE so far: 5.7633\n",
      "Best r2 so far: 0.8573\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [60/325] Loss: 22.77230\n",
      "Epoch [60/325] Val Loss: 71.06808\n",
      "increasing counter\n",
      "12\n",
      "MSE: 35.5413,R2: 0.8470,RMSE: 5.9617, PCIP: 84.93,score_avg:84.8178\n",
      "r2: 0.8470\n",
      "pcip: 84.9338\n",
      "RMSE: 5.9791\n",
      "Best RMSE so far: 5.7633\n",
      "Best r2 so far: 0.8573\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [61/325] Loss: 18.07618\n",
      "Epoch [61/325] Val Loss: 43.28814\n",
      "increasing counter\n",
      "13\n",
      "MSE: 38.5782,R2: 0.8339,RMSE: 6.2111, PCIP: 83.77,score_avg:83.5847\n",
      "r2: 0.8339\n",
      "pcip: 83.7748\n",
      "RMSE: 5.6929\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8573\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [62/325] Loss: 17.18954\n",
      "Epoch [62/325] Val Loss: 50.63264\n",
      "increasing counter\n",
      "14\n",
      "MSE: 38.2530,R2: 0.8353,RMSE: 6.1849, PCIP: 84.44,score_avg:83.9858\n",
      "r2: 0.8353\n",
      "pcip: 84.4371\n",
      "RMSE: 5.9674\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8573\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [63/325] Loss: 19.42838\n",
      "Epoch [63/325] Val Loss: 83.19864\n",
      "increasing counter\n",
      "15\n",
      "MSE: 32.9584,R2: 0.8581,RMSE: 5.7409, PCIP: 84.44,score_avg:85.1253\n",
      "r2: 0.8581\n",
      "pcip: 84.4371\n",
      "RMSE: 5.8069\n",
      "save rmse state\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8581\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [64/325] Loss: 22.15030\n",
      "Epoch [64/325] Val Loss: 46.55707\n",
      "increasing counter\n",
      "16\n",
      "MSE: 31.9340,R2: 0.8625,RMSE: 5.6510, PCIP: 84.60,score_avg:85.4286\n",
      "r2: 0.8625\n",
      "pcip: 84.6026\n",
      "RMSE: 5.8302\n",
      "save rmse state\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8625\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [65/325] Loss: 20.03253\n",
      "Epoch [65/325] Val Loss: 46.54548\n",
      "increasing counter\n",
      "17\n",
      "MSE: 35.2678,R2: 0.8482,RMSE: 5.9387, PCIP: 86.26,score_avg:85.5389\n",
      "r2: 0.8482\n",
      "pcip: 86.2583\n",
      "RMSE: 5.7420\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8625\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [66/325] Loss: 16.86139\n",
      "Epoch [66/325] Val Loss: 47.48285\n",
      "increasing counter\n",
      "18\n",
      "MSE: 34.5041,R2: 0.8515,RMSE: 5.8740, PCIP: 82.95,score_avg:84.0476\n",
      "r2: 0.8515\n",
      "pcip: 82.9470\n",
      "RMSE: 6.1561\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8625\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [67/325] Loss: 17.38851\n",
      "Epoch [67/325] Val Loss: 54.38749\n",
      "increasing counter\n",
      "19\n",
      "MSE: 34.6665,R2: 0.8508,RMSE: 5.8878, PCIP: 84.60,score_avg:84.8405\n",
      "r2: 0.8508\n",
      "pcip: 84.6026\n",
      "RMSE: 5.9046\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8625\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [68/325] Loss: 14.34980\n",
      "Epoch [68/325] Val Loss: 55.91851\n",
      "increasing counter\n",
      "20\n",
      "MSE: 31.4480,R2: 0.8646,RMSE: 5.6079, PCIP: 82.95,score_avg:84.7054\n",
      "r2: 0.8646\n",
      "pcip: 82.9470\n",
      "RMSE: 5.9286\n",
      "save rmse state\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8646\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [69/325] Loss: 16.90446\n",
      "Epoch [69/325] Val Loss: 44.91993\n",
      "increasing counter\n",
      "21\n",
      "MSE: 31.9580,R2: 0.8624,RMSE: 5.6531, PCIP: 84.60,score_avg:85.4234\n",
      "r2: 0.8624\n",
      "pcip: 84.6026\n",
      "RMSE: 5.8701\n",
      "Best RMSE so far: 5.6929\n",
      "Best r2 so far: 0.8646\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [70/325] Loss: 21.33043\n",
      "Epoch [70/325] Val Loss: 58.95054\n",
      "increasing counter\n",
      "22\n",
      "MSE: 36.0315,R2: 0.8449,RMSE: 6.0026, PCIP: 86.26,score_avg:85.3745\n",
      "r2: 0.8449\n",
      "pcip: 86.2583\n",
      "RMSE: 5.6023\n",
      "Best RMSE so far: 5.6023\n",
      "Best r2 so far: 0.8646\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [71/325] Loss: 15.50585\n",
      "Epoch [71/325] Val Loss: 47.80394\n",
      "restarting counter\n",
      "0\n",
      "MSE: 34.4770,R2: 0.8516,RMSE: 5.8717, PCIP: 84.60,score_avg:84.8813\n",
      "r2: 0.8516\n",
      "pcip: 84.6026\n",
      "RMSE: 5.9500\n",
      "Best RMSE so far: 5.6023\n",
      "Best r2 so far: 0.8646\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [72/325] Loss: 8.23730\n",
      "Epoch [72/325] Val Loss: 44.07087\n",
      "increasing counter\n",
      "1\n",
      "MSE: 32.8304,R2: 0.8587,RMSE: 5.7298, PCIP: 84.11,score_avg:84.9873\n",
      "r2: 0.8587\n",
      "pcip: 84.1060\n",
      "RMSE: 5.8368\n",
      "Best RMSE so far: 5.6023\n",
      "Best r2 so far: 0.8646\n",
      "best_score_avg so far: 85.5725\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [73/325] Loss: 24.40308\n",
      "Epoch [73/325] Val Loss: 54.43732\n",
      "increasing counter\n",
      "2\n",
      "MSE: 30.4170,R2: 0.8691,RMSE: 5.5152, PCIP: 84.77,score_avg:85.8378\n",
      "r2: 0.8691\n",
      "pcip: 84.7682\n",
      "RMSE: 6.2153\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 5.6023\n",
      "Best r2 so far: 0.8691\n",
      "best_score_avg so far: 85.8378\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [74/325] Loss: 37.69001\n",
      "Epoch [74/325] Val Loss: 35.04012\n",
      "increasing counter\n",
      "3\n",
      "MSE: 31.0452,R2: 0.8664,RMSE: 5.5718, PCIP: 85.93,score_avg:86.2821\n",
      "r2: 0.8664\n",
      "pcip: 85.9272\n",
      "RMSE: 5.7616\n",
      "save score avg state\n",
      "Best RMSE so far: 5.6023\n",
      "Best r2 so far: 0.8691\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [75/325] Loss: 20.34672\n",
      "Epoch [75/325] Val Loss: 42.96086\n",
      "increasing counter\n",
      "4\n",
      "MSE: 30.8424,R2: 0.8672,RMSE: 5.5536, PCIP: 84.93,score_avg:85.8291\n",
      "r2: 0.8672\n",
      "pcip: 84.9338\n",
      "RMSE: 5.4056\n",
      "Best RMSE so far: 5.4056\n",
      "Best r2 so far: 0.8691\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [76/325] Loss: 21.97455\n",
      "Epoch [76/325] Val Loss: 40.07729\n",
      "restarting counter\n",
      "0\n",
      "MSE: 31.8113,R2: 0.8631,RMSE: 5.6402, PCIP: 84.93,score_avg:85.6205\n",
      "r2: 0.8631\n",
      "pcip: 84.9338\n",
      "RMSE: 5.6241\n",
      "Best RMSE so far: 5.4056\n",
      "Best r2 so far: 0.8691\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [77/325] Loss: 16.64739\n",
      "Epoch [77/325] Val Loss: 50.16796\n",
      "increasing counter\n",
      "1\n",
      "MSE: 36.6040,R2: 0.8424,RMSE: 6.0501, PCIP: 84.93,score_avg:84.5891\n",
      "r2: 0.8424\n",
      "pcip: 84.9338\n",
      "RMSE: 5.2789\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8691\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [78/325] Loss: 19.42408\n",
      "Epoch [78/325] Val Loss: 42.88242\n",
      "increasing counter\n",
      "2\n",
      "MSE: 31.0015,R2: 0.8666,RMSE: 5.5679, PCIP: 85.60,score_avg:86.1260\n",
      "r2: 0.8666\n",
      "pcip: 85.5960\n",
      "RMSE: 5.6040\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8691\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [79/325] Loss: 14.04595\n",
      "Epoch [79/325] Val Loss: 45.25394\n",
      "increasing counter\n",
      "3\n",
      "MSE: 31.8802,R2: 0.8628,RMSE: 5.6463, PCIP: 82.78,score_avg:84.5296\n",
      "r2: 0.8628\n",
      "pcip: 82.7815\n",
      "RMSE: 5.4469\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8691\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [80/325] Loss: 27.60242\n",
      "Epoch [80/325] Val Loss: 31.44018\n",
      "restarting counter\n",
      "0\n",
      "MSE: 29.1217,R2: 0.8747,RMSE: 5.3965, PCIP: 84.60,score_avg:86.0338\n",
      "r2: 0.8747\n",
      "pcip: 84.6026\n",
      "RMSE: 6.0149\n",
      "save rmse state\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [81/325] Loss: 40.60117\n",
      "Epoch [81/325] Val Loss: 67.06670\n",
      "increasing counter\n",
      "1\n",
      "MSE: 31.2117,R2: 0.8657,RMSE: 5.5867, PCIP: 84.77,score_avg:85.6668\n",
      "r2: 0.8657\n",
      "pcip: 84.7682\n",
      "RMSE: 6.0979\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [82/325] Loss: 20.63206\n",
      "Epoch [82/325] Val Loss: 37.24192\n",
      "restarting counter\n",
      "0\n",
      "MSE: 33.0354,R2: 0.8578,RMSE: 5.7476, PCIP: 85.43,score_avg:85.6055\n",
      "r2: 0.8578\n",
      "pcip: 85.4305\n",
      "RMSE: 5.6367\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [83/325] Loss: 29.06603\n",
      "Epoch [83/325] Val Loss: 59.22840\n",
      "increasing counter\n",
      "1\n",
      "MSE: 33.3597,R2: 0.8564,RMSE: 5.7758, PCIP: 83.77,score_avg:84.7078\n",
      "r2: 0.8564\n",
      "pcip: 83.7748\n",
      "RMSE: 5.6237\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [84/325] Loss: 13.98102\n",
      "Epoch [84/325] Val Loss: 52.88325\n",
      "increasing counter\n",
      "2\n",
      "MSE: 34.3624,R2: 0.8521,RMSE: 5.8619, PCIP: 84.77,score_avg:84.9887\n",
      "r2: 0.8521\n",
      "pcip: 84.7682\n",
      "RMSE: 5.5821\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [85/325] Loss: 13.93707\n",
      "Epoch [85/325] Val Loss: 38.93530\n",
      "increasing counter\n",
      "3\n",
      "MSE: 29.1117,R2: 0.8747,RMSE: 5.3955, PCIP: 84.11,score_avg:85.7876\n",
      "r2: 0.8747\n",
      "pcip: 84.1060\n",
      "RMSE: 5.7686\n",
      "save rmse state\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [86/325] Loss: 43.04581\n",
      "Epoch [86/325] Val Loss: 49.46036\n",
      "increasing counter\n",
      "4\n",
      "MSE: 34.1417,R2: 0.8530,RMSE: 5.8431, PCIP: 85.43,score_avg:85.3674\n",
      "r2: 0.8530\n",
      "pcip: 85.4305\n",
      "RMSE: 5.7834\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.2821\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [87/325] Loss: 25.28212\n",
      "Epoch [87/325] Val Loss: 41.32451\n",
      "restarting counter\n",
      "0\n",
      "MSE: 29.8979,R2: 0.8713,RMSE: 5.4679, PCIP: 85.60,score_avg:86.3635\n",
      "r2: 0.8713\n",
      "pcip: 85.5960\n",
      "RMSE: 5.9800\n",
      "save score avg state\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.3635\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [88/325] Loss: 20.41450\n",
      "Epoch [88/325] Val Loss: 31.75872\n",
      "increasing counter\n",
      "1\n",
      "MSE: 29.3382,R2: 0.8737,RMSE: 5.4165, PCIP: 84.27,score_avg:85.8217\n",
      "r2: 0.8737\n",
      "pcip: 84.2715\n",
      "RMSE: 5.4560\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8747\n",
      "best_score_avg so far: 86.3635\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [89/325] Loss: 13.31120\n",
      "Epoch [89/325] Val Loss: 50.66961\n",
      "increasing counter\n",
      "2\n",
      "MSE: 24.0369,R2: 0.8965,RMSE: 4.9027, PCIP: 84.60,score_avg:87.1282\n",
      "r2: 0.8965\n",
      "pcip: 84.6026\n",
      "RMSE: 5.4762\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1282\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [90/325] Loss: 13.71222\n",
      "Epoch [90/325] Val Loss: 59.68884\n",
      "increasing counter\n",
      "3\n",
      "MSE: 30.9825,R2: 0.8666,RMSE: 5.5662, PCIP: 85.26,score_avg:85.9645\n",
      "r2: 0.8666\n",
      "pcip: 85.2649\n",
      "RMSE: 5.4470\n",
      "Best RMSE so far: 5.2789\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1282\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [91/325] Loss: 31.39962\n",
      "Epoch [91/325] Val Loss: 29.53256\n",
      "increasing counter\n",
      "4\n",
      "MSE: 28.2703,R2: 0.8783,RMSE: 5.3170, PCIP: 85.10,score_avg:86.4654\n",
      "r2: 0.8783\n",
      "pcip: 85.0993\n",
      "RMSE: 5.1213\n",
      "Best RMSE so far: 5.1213\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1282\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [92/325] Loss: 25.68347\n",
      "Epoch [92/325] Val Loss: 39.83611\n",
      "increasing counter\n",
      "5\n",
      "MSE: 28.9016,R2: 0.8756,RMSE: 5.3760, PCIP: 85.43,score_avg:86.4951\n",
      "r2: 0.8756\n",
      "pcip: 85.4305\n",
      "RMSE: 5.1803\n",
      "Best RMSE so far: 5.1213\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1282\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [93/325] Loss: 20.31746\n",
      "Epoch [93/325] Val Loss: 36.93562\n",
      "increasing counter\n",
      "6\n",
      "MSE: 31.1238,R2: 0.8660,RMSE: 5.5789, PCIP: 85.76,score_avg:86.1824\n",
      "r2: 0.8660\n",
      "pcip: 85.7616\n",
      "RMSE: 5.0404\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1282\n",
      "Best pcip so far: 86.2583\n",
      "Epoch [94/325] Loss: 23.14607\n",
      "Epoch [94/325] Val Loss: 35.35970\n",
      "restarting counter\n",
      "0\n",
      "MSE: 28.9118,R2: 0.8756,RMSE: 5.3770, PCIP: 86.75,score_avg:87.1552\n",
      "r2: 0.8756\n",
      "pcip: 86.7550\n",
      "RMSE: 5.3761\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [95/325] Loss: 18.79779\n",
      "Epoch [95/325] Val Loss: 48.30839\n",
      "increasing counter\n",
      "1\n",
      "MSE: 31.0961,R2: 0.8662,RMSE: 5.5764, PCIP: 85.10,score_avg:85.8572\n",
      "r2: 0.8662\n",
      "pcip: 85.0993\n",
      "RMSE: 5.6670\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [96/325] Loss: 15.06709\n",
      "Epoch [96/325] Val Loss: 26.20700\n",
      "restarting counter\n",
      "0\n",
      "MSE: 31.9479,R2: 0.8625,RMSE: 5.6522, PCIP: 86.09,score_avg:86.1706\n",
      "r2: 0.8625\n",
      "pcip: 86.0927\n",
      "RMSE: 5.9931\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [97/325] Loss: 13.25516\n",
      "Epoch [97/325] Val Loss: 28.44966\n",
      "increasing counter\n",
      "1\n",
      "MSE: 27.8698,R2: 0.8800,RMSE: 5.2792, PCIP: 86.26,score_avg:87.1311\n",
      "r2: 0.8800\n",
      "pcip: 86.2583\n",
      "RMSE: 5.3417\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [98/325] Loss: 13.57285\n",
      "Epoch [98/325] Val Loss: 31.00424\n",
      "increasing counter\n",
      "2\n",
      "MSE: 27.2250,R2: 0.8828,RMSE: 5.2178, PCIP: 85.43,score_avg:86.8559\n",
      "r2: 0.8828\n",
      "pcip: 85.4305\n",
      "RMSE: 5.3086\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [99/325] Loss: 27.37350\n",
      "Epoch [99/325] Val Loss: 51.35259\n",
      "increasing counter\n",
      "3\n",
      "MSE: 29.8576,R2: 0.8715,RMSE: 5.4642, PCIP: 86.09,score_avg:86.6205\n",
      "r2: 0.8715\n",
      "pcip: 86.0927\n",
      "RMSE: 5.7161\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [100/325] Loss: 14.35106\n",
      "Epoch [100/325] Val Loss: 43.86861\n",
      "increasing counter\n",
      "4\n",
      "MSE: 30.8361,R2: 0.8673,RMSE: 5.5530, PCIP: 85.60,score_avg:86.1616\n",
      "r2: 0.8673\n",
      "pcip: 85.5960\n",
      "RMSE: 5.4643\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [101/325] Loss: 34.62197\n",
      "Epoch [101/325] Val Loss: 32.30613\n",
      "increasing counter\n",
      "5\n",
      "MSE: 33.3454,R2: 0.8565,RMSE: 5.7746, PCIP: 84.11,score_avg:84.8765\n",
      "r2: 0.8565\n",
      "pcip: 84.1060\n",
      "RMSE: 5.2256\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8965\n",
      "best_score_avg so far: 87.1552\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [102/325] Loss: 18.88790\n",
      "Epoch [102/325] Val Loss: 41.68258\n",
      "increasing counter\n",
      "6\n",
      "MSE: 23.8107,R2: 0.8975,RMSE: 4.8796, PCIP: 84.93,score_avg:87.3424\n",
      "r2: 0.8975\n",
      "pcip: 84.9338\n",
      "RMSE: 5.5833\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.3424\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [103/325] Loss: 18.69899\n",
      "Epoch [103/325] Val Loss: 56.63577\n",
      "increasing counter\n",
      "7\n",
      "MSE: 34.1436,R2: 0.8530,RMSE: 5.8433, PCIP: 84.44,score_avg:84.8703\n",
      "r2: 0.8530\n",
      "pcip: 84.4371\n",
      "RMSE: 5.3968\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.3424\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [104/325] Loss: 19.11146\n",
      "Epoch [104/325] Val Loss: 36.88300\n",
      "increasing counter\n",
      "8\n",
      "MSE: 27.3277,R2: 0.8824,RMSE: 5.2276, PCIP: 84.77,score_avg:86.5027\n",
      "r2: 0.8824\n",
      "pcip: 84.7682\n",
      "RMSE: 5.3791\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.3424\n",
      "Best pcip so far: 86.7550\n",
      "Epoch [105/325] Loss: 14.62667\n",
      "Epoch [105/325] Val Loss: 28.76885\n",
      "increasing counter\n",
      "9\n",
      "MSE: 27.8281,R2: 0.8802,RMSE: 5.2752, PCIP: 87.25,score_avg:87.6368\n",
      "r2: 0.8802\n",
      "pcip: 87.2517\n",
      "RMSE: 5.1508\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [106/325] Loss: 18.68043\n",
      "Epoch [106/325] Val Loss: 48.26831\n",
      "restarting counter\n",
      "0\n",
      "MSE: 27.7998,R2: 0.8803,RMSE: 5.2726, PCIP: 82.78,score_avg:85.4077\n",
      "r2: 0.8803\n",
      "pcip: 82.7815\n",
      "RMSE: 5.7643\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [107/325] Loss: 16.72928\n",
      "Epoch [107/325] Val Loss: 46.25483\n",
      "increasing counter\n",
      "1\n",
      "MSE: 28.7623,R2: 0.8762,RMSE: 5.3631, PCIP: 83.94,score_avg:85.7801\n",
      "r2: 0.8762\n",
      "pcip: 83.9404\n",
      "RMSE: 5.2040\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [108/325] Loss: 13.02342\n",
      "Epoch [108/325] Val Loss: 26.86380\n",
      "increasing counter\n",
      "2\n",
      "MSE: 28.6333,R2: 0.8768,RMSE: 5.3510, PCIP: 84.77,score_avg:86.2217\n",
      "r2: 0.8768\n",
      "pcip: 84.7682\n",
      "RMSE: 5.2805\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [109/325] Loss: 13.95867\n",
      "Epoch [109/325] Val Loss: 22.64346\n",
      "increasing counter\n",
      "3\n",
      "MSE: 30.2391,R2: 0.8698,RMSE: 5.4990, PCIP: 86.42,score_avg:86.7040\n",
      "r2: 0.8698\n",
      "pcip: 86.4238\n",
      "RMSE: 5.1717\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [110/325] Loss: 19.21358\n",
      "Epoch [110/325] Val Loss: 39.34002\n",
      "increasing counter\n",
      "4\n",
      "MSE: 31.3226,R2: 0.8652,RMSE: 5.5967, PCIP: 85.10,score_avg:85.8085\n",
      "r2: 0.8652\n",
      "pcip: 85.0993\n",
      "RMSE: 5.1433\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [111/325] Loss: 21.73904\n",
      "Epoch [111/325] Val Loss: 28.82929\n",
      "increasing counter\n",
      "5\n",
      "MSE: 31.1012,R2: 0.8661,RMSE: 5.5768, PCIP: 84.27,score_avg:85.4423\n",
      "r2: 0.8661\n",
      "pcip: 84.2715\n",
      "RMSE: 6.0060\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [112/325] Loss: 9.15532\n",
      "Epoch [112/325] Val Loss: 34.38527\n",
      "increasing counter\n",
      "6\n",
      "MSE: 29.4584,R2: 0.8732,RMSE: 5.4276, PCIP: 84.11,score_avg:85.7130\n",
      "r2: 0.8732\n",
      "pcip: 84.1060\n",
      "RMSE: 5.2637\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [113/325] Loss: 26.79125\n",
      "Epoch [113/325] Val Loss: 43.01030\n",
      "increasing counter\n",
      "7\n",
      "MSE: 31.1983,R2: 0.8657,RMSE: 5.5855, PCIP: 85.10,score_avg:85.8353\n",
      "r2: 0.8657\n",
      "pcip: 85.0993\n",
      "RMSE: 5.9704\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [114/325] Loss: 18.97828\n",
      "Epoch [114/325] Val Loss: 42.66532\n",
      "increasing counter\n",
      "8\n",
      "MSE: 30.2256,R2: 0.8699,RMSE: 5.4978, PCIP: 85.76,score_avg:86.3757\n",
      "r2: 0.8699\n",
      "pcip: 85.7616\n",
      "RMSE: 5.0650\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [115/325] Loss: 15.00017\n",
      "Epoch [115/325] Val Loss: 50.34797\n",
      "increasing counter\n",
      "9\n",
      "MSE: 26.0336,R2: 0.8879,RMSE: 5.1023, PCIP: 85.76,score_avg:87.2779\n",
      "r2: 0.8879\n",
      "pcip: 85.7616\n",
      "RMSE: 5.0720\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.6368\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [116/325] Loss: 20.03816\n",
      "Epoch [116/325] Val Loss: 33.87209\n",
      "increasing counter\n",
      "10\n",
      "MSE: 25.1347,R2: 0.8918,RMSE: 5.0134, PCIP: 86.59,score_avg:87.8853\n",
      "r2: 0.8918\n",
      "pcip: 86.5894\n",
      "RMSE: 5.3236\n",
      "save score avg state\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.8853\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [117/325] Loss: 24.43160\n",
      "Epoch [117/325] Val Loss: 36.79643\n",
      "increasing counter\n",
      "11\n",
      "MSE: 27.6474,R2: 0.8810,RMSE: 5.2581, PCIP: 84.27,score_avg:86.1856\n",
      "r2: 0.8810\n",
      "pcip: 84.2715\n",
      "RMSE: 5.5719\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.8853\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [118/325] Loss: 12.90840\n",
      "Epoch [118/325] Val Loss: 17.45340\n",
      "restarting counter\n",
      "0\n",
      "MSE: 28.5931,R2: 0.8769,RMSE: 5.3473, PCIP: 85.43,score_avg:86.5615\n",
      "r2: 0.8769\n",
      "pcip: 85.4305\n",
      "RMSE: 5.0686\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.8853\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [119/325] Loss: 15.51309\n",
      "Epoch [119/325] Val Loss: 45.56715\n",
      "increasing counter\n",
      "1\n",
      "MSE: 28.0249,R2: 0.8794,RMSE: 5.2939, PCIP: 84.11,score_avg:86.0216\n",
      "r2: 0.8794\n",
      "pcip: 84.1060\n",
      "RMSE: 5.3271\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.8853\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [120/325] Loss: 22.74825\n",
      "Epoch [120/325] Val Loss: 42.98716\n",
      "increasing counter\n",
      "2\n",
      "MSE: 27.9265,R2: 0.8798,RMSE: 5.2845, PCIP: 84.27,score_avg:86.1255\n",
      "r2: 0.8798\n",
      "pcip: 84.2715\n",
      "RMSE: 5.2903\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.8853\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [121/325] Loss: 13.95289\n",
      "Epoch [121/325] Val Loss: 47.78899\n",
      "increasing counter\n",
      "3\n",
      "MSE: 28.6735,R2: 0.8766,RMSE: 5.3548, PCIP: 83.77,score_avg:85.7164\n",
      "r2: 0.8766\n",
      "pcip: 83.7748\n",
      "RMSE: 5.2390\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.8853\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [122/325] Loss: 15.58008\n",
      "Epoch [122/325] Val Loss: 49.58483\n",
      "increasing counter\n",
      "4\n",
      "MSE: 28.9420,R2: 0.8754,RMSE: 5.3798, PCIP: 84.27,score_avg:85.9069\n",
      "r2: 0.8754\n",
      "pcip: 84.2715\n",
      "RMSE: 5.4880\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 87.8853\n",
      "Best pcip so far: 87.2517\n",
      "Epoch [123/325] Loss: 15.86731\n",
      "Epoch [123/325] Val Loss: 48.18090\n",
      "increasing counter\n",
      "5\n",
      "MSE: 24.9917,R2: 0.8924,RMSE: 4.9992, PCIP: 87.42,score_avg:88.3300\n",
      "r2: 0.8924\n",
      "pcip: 87.4172\n",
      "RMSE: 5.1341\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [124/325] Loss: 15.10106\n",
      "Epoch [124/325] Val Loss: 27.56326\n",
      "increasing counter\n",
      "6\n",
      "MSE: 29.2418,R2: 0.8741,RMSE: 5.4076, PCIP: 86.59,score_avg:87.0014\n",
      "r2: 0.8741\n",
      "pcip: 86.5894\n",
      "RMSE: 5.0563\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [125/325] Loss: 16.89929\n",
      "Epoch [125/325] Val Loss: 51.82548\n",
      "increasing counter\n",
      "7\n",
      "MSE: 32.1665,R2: 0.8615,RMSE: 5.6716, PCIP: 82.45,score_avg:84.3024\n",
      "r2: 0.8615\n",
      "pcip: 82.4503\n",
      "RMSE: 5.2194\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [126/325] Loss: 15.82110\n",
      "Epoch [126/325] Val Loss: 32.47088\n",
      "increasing counter\n",
      "8\n",
      "MSE: 29.1300,R2: 0.8746,RMSE: 5.3972, PCIP: 85.76,score_avg:86.6115\n",
      "r2: 0.8746\n",
      "pcip: 85.7616\n",
      "RMSE: 5.4074\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [127/325] Loss: 17.81549\n",
      "Epoch [127/325] Val Loss: 44.23351\n",
      "increasing counter\n",
      "9\n",
      "MSE: 25.1937,R2: 0.8916,RMSE: 5.0193, PCIP: 86.09,score_avg:87.6242\n",
      "r2: 0.8916\n",
      "pcip: 86.0927\n",
      "RMSE: 5.3418\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [128/325] Loss: 19.03495\n",
      "Epoch [128/325] Val Loss: 40.58898\n",
      "increasing counter\n",
      "10\n",
      "MSE: 26.9899,R2: 0.8838,RMSE: 5.1952, PCIP: 86.09,score_avg:87.2377\n",
      "r2: 0.8838\n",
      "pcip: 86.0927\n",
      "RMSE: 5.3246\n",
      "Best RMSE so far: 5.0404\n",
      "Best r2 so far: 0.8975\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [129/325] Loss: 16.94507\n",
      "Epoch [129/325] Val Loss: 38.17538\n",
      "increasing counter\n",
      "11\n",
      "MSE: 23.5909,R2: 0.8985,RMSE: 4.8570, PCIP: 86.75,score_avg:88.3003\n",
      "r2: 0.8985\n",
      "pcip: 86.7550\n",
      "RMSE: 4.9336\n",
      "save rmse state\n",
      "Best RMSE so far: 4.9336\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [130/325] Loss: 17.42513\n",
      "Epoch [130/325] Val Loss: 49.19535\n",
      "increasing counter\n",
      "12\n",
      "MSE: 27.6071,R2: 0.8812,RMSE: 5.2542, PCIP: 86.09,score_avg:87.1048\n",
      "r2: 0.8812\n",
      "pcip: 86.0927\n",
      "RMSE: 5.0688\n",
      "Best RMSE so far: 4.9336\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [131/325] Loss: 16.22918\n",
      "Epoch [131/325] Val Loss: 46.64478\n",
      "restarting counter\n",
      "0\n",
      "MSE: 27.9008,R2: 0.8799,RMSE: 5.2821, PCIP: 84.11,score_avg:86.0483\n",
      "r2: 0.8799\n",
      "pcip: 84.1060\n",
      "RMSE: 5.1593\n",
      "Best RMSE so far: 4.9336\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [132/325] Loss: 17.69837\n",
      "Epoch [132/325] Val Loss: 37.85472\n",
      "increasing counter\n",
      "1\n",
      "MSE: 29.3572,R2: 0.8736,RMSE: 5.4182, PCIP: 84.77,score_avg:86.0659\n",
      "r2: 0.8736\n",
      "pcip: 84.7682\n",
      "RMSE: 4.9680\n",
      "Best RMSE so far: 4.9336\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [133/325] Loss: 18.21774\n",
      "Epoch [133/325] Val Loss: 74.32438\n",
      "increasing counter\n",
      "2\n",
      "MSE: 31.5382,R2: 0.8642,RMSE: 5.6159, PCIP: 84.93,score_avg:85.6793\n",
      "r2: 0.8642\n",
      "pcip: 84.9338\n",
      "RMSE: 5.3911\n",
      "Best RMSE so far: 4.9336\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [134/325] Loss: 8.38627\n",
      "Epoch [134/325] Val Loss: 18.56435\n",
      "increasing counter\n",
      "3\n",
      "MSE: 27.5716,R2: 0.8813,RMSE: 5.2509, PCIP: 85.43,score_avg:86.7813\n",
      "r2: 0.8813\n",
      "pcip: 85.4305\n",
      "RMSE: 4.8722\n",
      "Best RMSE so far: 4.8722\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [135/325] Loss: 14.14069\n",
      "Epoch [135/325] Val Loss: 27.50879\n",
      "increasing counter\n",
      "4\n",
      "MSE: 26.8746,R2: 0.8843,RMSE: 5.1841, PCIP: 85.76,score_avg:87.0969\n",
      "r2: 0.8843\n",
      "pcip: 85.7616\n",
      "RMSE: 4.9362\n",
      "Best RMSE so far: 4.8722\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [136/325] Loss: 18.13308\n",
      "Epoch [136/325] Val Loss: 22.50890\n",
      "increasing counter\n",
      "5\n",
      "MSE: 27.3061,R2: 0.8825,RMSE: 5.2255, PCIP: 86.42,score_avg:87.3352\n",
      "r2: 0.8825\n",
      "pcip: 86.4238\n",
      "RMSE: 5.2997\n",
      "Best RMSE so far: 4.8722\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [137/325] Loss: 20.05082\n",
      "Epoch [137/325] Val Loss: 49.10575\n",
      "increasing counter\n",
      "6\n",
      "MSE: 27.4819,R2: 0.8817,RMSE: 5.2423, PCIP: 85.76,score_avg:86.9662\n",
      "r2: 0.8817\n",
      "pcip: 85.7616\n",
      "RMSE: 4.7619\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [138/325] Loss: 14.36223\n",
      "Epoch [138/325] Val Loss: 40.09283\n",
      "increasing counter\n",
      "7\n",
      "MSE: 25.1993,R2: 0.8915,RMSE: 5.0199, PCIP: 86.26,score_avg:87.7058\n",
      "r2: 0.8915\n",
      "pcip: 86.2583\n",
      "RMSE: 4.9370\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [139/325] Loss: 20.06929\n",
      "Epoch [139/325] Val Loss: 47.98946\n",
      "increasing counter\n",
      "8\n",
      "MSE: 23.9036,R2: 0.8971,RMSE: 4.8891, PCIP: 84.44,score_avg:87.0741\n",
      "r2: 0.8971\n",
      "pcip: 84.4371\n",
      "RMSE: 5.6797\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [140/325] Loss: 16.19245\n",
      "Epoch [140/325] Val Loss: 38.95818\n",
      "increasing counter\n",
      "9\n",
      "MSE: 26.3352,R2: 0.8866,RMSE: 5.1318, PCIP: 84.93,score_avg:86.7991\n",
      "r2: 0.8866\n",
      "pcip: 84.9338\n",
      "RMSE: 5.3893\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [141/325] Loss: 20.45730\n",
      "Epoch [141/325] Val Loss: 54.95663\n",
      "increasing counter\n",
      "10\n",
      "MSE: 25.3398,R2: 0.8909,RMSE: 5.0339, PCIP: 83.11,score_avg:86.1027\n",
      "r2: 0.8909\n",
      "pcip: 83.1126\n",
      "RMSE: 5.6014\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [142/325] Loss: 13.51477\n",
      "Epoch [142/325] Val Loss: 36.25643\n",
      "increasing counter\n",
      "11\n",
      "MSE: 26.2223,R2: 0.8871,RMSE: 5.1208, PCIP: 81.95,score_avg:85.3333\n",
      "r2: 0.8871\n",
      "pcip: 81.9536\n",
      "RMSE: 4.8751\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [143/325] Loss: 16.34375\n",
      "Epoch [143/325] Val Loss: 24.42300\n",
      "increasing counter\n",
      "12\n",
      "MSE: 29.8156,R2: 0.8717,RMSE: 5.4604, PCIP: 84.11,score_avg:85.6361\n",
      "r2: 0.8717\n",
      "pcip: 84.1060\n",
      "RMSE: 5.4503\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [144/325] Loss: 12.01186\n",
      "Epoch [144/325] Val Loss: 42.16027\n",
      "increasing counter\n",
      "13\n",
      "MSE: 27.0377,R2: 0.8836,RMSE: 5.1998, PCIP: 84.27,score_avg:86.3168\n",
      "r2: 0.8836\n",
      "pcip: 84.2715\n",
      "RMSE: 5.0980\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [145/325] Loss: 31.42920\n",
      "Epoch [145/325] Val Loss: 17.41117\n",
      "increasing counter\n",
      "14\n",
      "MSE: 24.7946,R2: 0.8933,RMSE: 4.9794, PCIP: 86.59,score_avg:87.9585\n",
      "r2: 0.8933\n",
      "pcip: 86.5894\n",
      "RMSE: 5.2534\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [146/325] Loss: 25.28984\n",
      "Epoch [146/325] Val Loss: 31.28822\n",
      "increasing counter\n",
      "15\n",
      "MSE: 26.2965,R2: 0.8868,RMSE: 5.1280, PCIP: 84.77,score_avg:86.7246\n",
      "r2: 0.8868\n",
      "pcip: 84.7682\n",
      "RMSE: 5.3410\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [147/325] Loss: 16.48617\n",
      "Epoch [147/325] Val Loss: 54.18937\n",
      "increasing counter\n",
      "16\n",
      "MSE: 29.1396,R2: 0.8746,RMSE: 5.3981, PCIP: 86.59,score_avg:87.0234\n",
      "r2: 0.8746\n",
      "pcip: 86.5894\n",
      "RMSE: 5.2712\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [148/325] Loss: 11.40625\n",
      "Epoch [148/325] Val Loss: 36.44488\n",
      "increasing counter\n",
      "17\n",
      "MSE: 24.5050,R2: 0.8945,RMSE: 4.9502, PCIP: 83.94,score_avg:86.6963\n",
      "r2: 0.8945\n",
      "pcip: 83.9404\n",
      "RMSE: 5.1340\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.8985\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [149/325] Loss: 14.02203\n",
      "Epoch [149/325] Val Loss: 24.55673\n",
      "increasing counter\n",
      "18\n",
      "MSE: 22.9285,R2: 0.9013,RMSE: 4.7884, PCIP: 85.26,score_avg:87.6978\n",
      "r2: 0.9013\n",
      "pcip: 85.2649\n",
      "RMSE: 5.1901\n",
      "save rmse state\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.9013\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [150/325] Loss: 14.98584\n",
      "Epoch [150/325] Val Loss: 23.96968\n",
      "increasing counter\n",
      "19\n",
      "MSE: 24.3709,R2: 0.8951,RMSE: 4.9367, PCIP: 84.11,score_avg:86.8079\n",
      "r2: 0.8951\n",
      "pcip: 84.1060\n",
      "RMSE: 5.4017\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.9013\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [151/325] Loss: 20.74976\n",
      "Epoch [151/325] Val Loss: 33.37037\n",
      "increasing counter\n",
      "20\n",
      "MSE: 24.9966,R2: 0.8924,RMSE: 4.9997, PCIP: 85.26,score_avg:87.2527\n",
      "r2: 0.8924\n",
      "pcip: 85.2649\n",
      "RMSE: 5.2372\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.9013\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [152/325] Loss: 18.32255\n",
      "Epoch [152/325] Val Loss: 42.19141\n",
      "increasing counter\n",
      "21\n",
      "MSE: 29.4140,R2: 0.8734,RMSE: 5.4235, PCIP: 81.79,score_avg:84.5636\n",
      "r2: 0.8734\n",
      "pcip: 81.7881\n",
      "RMSE: 5.1883\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.9013\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [153/325] Loss: 18.71807\n",
      "Epoch [153/325] Val Loss: 46.09899\n",
      "increasing counter\n",
      "22\n",
      "MSE: 23.3288,R2: 0.8996,RMSE: 4.8300, PCIP: 85.76,score_avg:87.8600\n",
      "r2: 0.8996\n",
      "pcip: 85.7616\n",
      "RMSE: 4.8138\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.9013\n",
      "best_score_avg so far: 88.3300\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [154/325] Loss: 13.79117\n",
      "Epoch [154/325] Val Loss: 33.14118\n",
      "increasing counter\n",
      "23\n",
      "MSE: 22.5127,R2: 0.9031,RMSE: 4.7448, PCIP: 87.42,score_avg:88.8635\n",
      "r2: 0.9031\n",
      "pcip: 87.4172\n",
      "RMSE: 4.9542\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [155/325] Loss: 12.94235\n",
      "Epoch [155/325] Val Loss: 37.14199\n",
      "increasing counter\n",
      "24\n",
      "MSE: 26.0310,R2: 0.8880,RMSE: 5.1021, PCIP: 85.93,score_avg:87.3612\n",
      "r2: 0.8880\n",
      "pcip: 85.9272\n",
      "RMSE: 4.9591\n",
      "Best RMSE so far: 4.7619\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [156/325] Loss: 17.81347\n",
      "Epoch [156/325] Val Loss: 31.91867\n",
      "increasing counter\n",
      "25\n",
      "MSE: 24.0535,R2: 0.8965,RMSE: 4.9044, PCIP: 86.59,score_avg:88.1180\n",
      "r2: 0.8965\n",
      "pcip: 86.5894\n",
      "RMSE: 4.7285\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [157/325] Loss: 13.48519\n",
      "Epoch [157/325] Val Loss: 29.99262\n",
      "increasing counter\n",
      "26\n",
      "MSE: 30.3986,R2: 0.8692,RMSE: 5.5135, PCIP: 85.43,score_avg:86.1729\n",
      "r2: 0.8692\n",
      "pcip: 85.4305\n",
      "RMSE: 5.0904\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [158/325] Loss: 16.91269\n",
      "Epoch [158/325] Val Loss: 27.06334\n",
      "increasing counter\n",
      "27\n",
      "MSE: 28.1727,R2: 0.8787,RMSE: 5.3078, PCIP: 82.12,score_avg:84.9964\n",
      "r2: 0.8787\n",
      "pcip: 82.1192\n",
      "RMSE: 5.2218\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [159/325] Loss: 20.77914\n",
      "Epoch [159/325] Val Loss: 51.78241\n",
      "increasing counter\n",
      "28\n",
      "MSE: 23.7193,R2: 0.8979,RMSE: 4.8702, PCIP: 83.28,score_avg:86.5343\n",
      "r2: 0.8979\n",
      "pcip: 83.2781\n",
      "RMSE: 4.8390\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [160/325] Loss: 20.28665\n",
      "Epoch [160/325] Val Loss: 35.87123\n",
      "increasing counter\n",
      "29\n",
      "MSE: 33.4038,R2: 0.8562,RMSE: 5.7796, PCIP: 83.28,score_avg:84.4500\n",
      "r2: 0.8562\n",
      "pcip: 83.2781\n",
      "RMSE: 5.7975\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [161/325] Loss: 18.34296\n",
      "Epoch [161/325] Val Loss: 38.44283\n",
      "increasing counter\n",
      "30\n",
      "MSE: 25.7170,R2: 0.8893,RMSE: 5.0712, PCIP: 85.43,score_avg:87.1805\n",
      "r2: 0.8893\n",
      "pcip: 85.4305\n",
      "RMSE: 5.1254\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [162/325] Loss: 16.11437\n",
      "Epoch [162/325] Val Loss: 25.73030\n",
      "increasing counter\n",
      "31\n",
      "MSE: 24.8314,R2: 0.8931,RMSE: 4.9831, PCIP: 84.77,score_avg:87.0400\n",
      "r2: 0.8931\n",
      "pcip: 84.7682\n",
      "RMSE: 5.0699\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [163/325] Loss: 14.41285\n",
      "Epoch [163/325] Val Loss: 46.59070\n",
      "increasing counter\n",
      "32\n",
      "MSE: 22.5438,R2: 0.9030,RMSE: 4.7480, PCIP: 86.09,score_avg:88.1945\n",
      "r2: 0.9030\n",
      "pcip: 86.0927\n",
      "RMSE: 5.0292\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [164/325] Loss: 9.44532\n",
      "Epoch [164/325] Val Loss: 54.86486\n",
      "increasing counter\n",
      "33\n",
      "MSE: 24.0292,R2: 0.8966,RMSE: 4.9020, PCIP: 85.60,score_avg:87.6265\n",
      "r2: 0.8966\n",
      "pcip: 85.5960\n",
      "RMSE: 4.8572\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [165/325] Loss: 29.09762\n",
      "Epoch [165/325] Val Loss: 21.12035\n",
      "restarting counter\n",
      "0\n",
      "MSE: 25.4420,R2: 0.8905,RMSE: 5.0440, PCIP: 84.93,score_avg:86.9913\n",
      "r2: 0.8905\n",
      "pcip: 84.9338\n",
      "RMSE: 5.2687\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9031\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [166/325] Loss: 16.99346\n",
      "Epoch [166/325] Val Loss: 13.11948\n",
      "increasing counter\n",
      "1\n",
      "MSE: 21.8712,R2: 0.9059,RMSE: 4.6767, PCIP: 86.09,score_avg:88.3393\n",
      "r2: 0.9059\n",
      "pcip: 86.0927\n",
      "RMSE: 5.0612\n",
      "save rmse state\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [167/325] Loss: 16.16330\n",
      "Epoch [167/325] Val Loss: 45.37653\n",
      "increasing counter\n",
      "2\n",
      "MSE: 25.8750,R2: 0.8886,RMSE: 5.0867, PCIP: 85.26,score_avg:87.0637\n",
      "r2: 0.8886\n",
      "pcip: 85.2649\n",
      "RMSE: 5.0674\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [168/325] Loss: 16.74411\n",
      "Epoch [168/325] Val Loss: 21.26245\n",
      "increasing counter\n",
      "3\n",
      "MSE: 22.2755,R2: 0.9041,RMSE: 4.7197, PCIP: 86.59,score_avg:88.5006\n",
      "r2: 0.9041\n",
      "pcip: 86.5894\n",
      "RMSE: 4.8637\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [169/325] Loss: 19.07207\n",
      "Epoch [169/325] Val Loss: 41.28099\n",
      "increasing counter\n",
      "4\n",
      "MSE: 25.2074,R2: 0.8915,RMSE: 5.0207, PCIP: 86.26,score_avg:87.7041\n",
      "r2: 0.8915\n",
      "pcip: 86.2583\n",
      "RMSE: 5.0933\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [170/325] Loss: 21.28631\n",
      "Epoch [170/325] Val Loss: 34.59989\n",
      "increasing counter\n",
      "5\n",
      "MSE: 24.9815,R2: 0.8925,RMSE: 4.9981, PCIP: 86.92,score_avg:88.0838\n",
      "r2: 0.8925\n",
      "pcip: 86.9205\n",
      "RMSE: 5.0240\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [171/325] Loss: 15.37228\n",
      "Epoch [171/325] Val Loss: 33.52714\n",
      "increasing counter\n",
      "6\n",
      "MSE: 27.3556,R2: 0.8823,RMSE: 5.2303, PCIP: 83.11,score_avg:85.6689\n",
      "r2: 0.8823\n",
      "pcip: 83.1126\n",
      "RMSE: 4.7961\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [172/325] Loss: 15.71768\n",
      "Epoch [172/325] Val Loss: 29.48993\n",
      "increasing counter\n",
      "7\n",
      "MSE: 23.5187,R2: 0.8988,RMSE: 4.8496, PCIP: 86.09,score_avg:87.9847\n",
      "r2: 0.8988\n",
      "pcip: 86.0927\n",
      "RMSE: 4.7861\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [173/325] Loss: 26.74692\n",
      "Epoch [173/325] Val Loss: 32.92307\n",
      "increasing counter\n",
      "8\n",
      "MSE: 25.3381,R2: 0.8909,RMSE: 5.0337, PCIP: 84.44,score_avg:86.7654\n",
      "r2: 0.8909\n",
      "pcip: 84.4371\n",
      "RMSE: 5.2000\n",
      "Best RMSE so far: 4.7285\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [174/325] Loss: 18.88185\n",
      "Epoch [174/325] Val Loss: 23.41463\n",
      "increasing counter\n",
      "9\n",
      "MSE: 24.7371,R2: 0.8935,RMSE: 4.9736, PCIP: 85.60,score_avg:87.4742\n",
      "r2: 0.8935\n",
      "pcip: 85.5960\n",
      "RMSE: 4.5353\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [175/325] Loss: 10.43710\n",
      "Epoch [175/325] Val Loss: 13.27154\n",
      "increasing counter\n",
      "10\n",
      "MSE: 22.8307,R2: 0.9017,RMSE: 4.7781, PCIP: 85.76,score_avg:87.9672\n",
      "r2: 0.9017\n",
      "pcip: 85.7616\n",
      "RMSE: 5.1361\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [176/325] Loss: 22.53146\n",
      "Epoch [176/325] Val Loss: 38.39629\n",
      "increasing counter\n",
      "11\n",
      "MSE: 25.1932,R2: 0.8916,RMSE: 5.0193, PCIP: 83.94,score_avg:86.5482\n",
      "r2: 0.8916\n",
      "pcip: 83.9404\n",
      "RMSE: 4.8779\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9059\n",
      "best_score_avg so far: 88.8635\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [177/325] Loss: 8.31751\n",
      "Epoch [177/325] Val Loss: 23.10788\n",
      "increasing counter\n",
      "12\n",
      "MSE: 21.2651,R2: 0.9085,RMSE: 4.6114, PCIP: 87.25,score_avg:89.0492\n",
      "r2: 0.9085\n",
      "pcip: 87.2517\n",
      "RMSE: 5.0238\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9085\n",
      "best_score_avg so far: 89.0492\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [178/325] Loss: 14.85598\n",
      "Epoch [178/325] Val Loss: 40.84417\n",
      "increasing counter\n",
      "13\n",
      "MSE: 25.2694,R2: 0.8912,RMSE: 5.0269, PCIP: 86.26,score_avg:87.6907\n",
      "r2: 0.8912\n",
      "pcip: 86.2583\n",
      "RMSE: 5.1700\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9085\n",
      "best_score_avg so far: 89.0492\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [179/325] Loss: 16.91808\n",
      "Epoch [179/325] Val Loss: 29.23626\n",
      "increasing counter\n",
      "14\n",
      "MSE: 25.3484,R2: 0.8909,RMSE: 5.0347, PCIP: 85.10,score_avg:87.0943\n",
      "r2: 0.8909\n",
      "pcip: 85.0993\n",
      "RMSE: 4.9991\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9085\n",
      "best_score_avg so far: 89.0492\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [180/325] Loss: 17.68195\n",
      "Epoch [180/325] Val Loss: 21.79086\n",
      "increasing counter\n",
      "15\n",
      "MSE: 24.9408,R2: 0.8926,RMSE: 4.9941, PCIP: 85.93,score_avg:87.5959\n",
      "r2: 0.8926\n",
      "pcip: 85.9272\n",
      "RMSE: 4.9055\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9085\n",
      "best_score_avg so far: 89.0492\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [181/325] Loss: 17.32668\n",
      "Epoch [181/325] Val Loss: 20.62066\n",
      "increasing counter\n",
      "16\n",
      "MSE: 24.5550,R2: 0.8943,RMSE: 4.9553, PCIP: 86.92,score_avg:88.1756\n",
      "r2: 0.8943\n",
      "pcip: 86.9205\n",
      "RMSE: 5.0551\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9085\n",
      "best_score_avg so far: 89.0492\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [182/325] Loss: 11.51166\n",
      "Epoch [182/325] Val Loss: 30.92156\n",
      "increasing counter\n",
      "17\n",
      "MSE: 29.4999,R2: 0.8730,RMSE: 5.4314, PCIP: 86.26,score_avg:86.7803\n",
      "r2: 0.8730\n",
      "pcip: 86.2583\n",
      "RMSE: 5.1920\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9085\n",
      "best_score_avg so far: 89.0492\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [183/325] Loss: 11.98288\n",
      "Epoch [183/325] Val Loss: 24.77678\n",
      "increasing counter\n",
      "18\n",
      "MSE: 22.6263,R2: 0.9026,RMSE: 4.7567, PCIP: 85.60,score_avg:87.9284\n",
      "r2: 0.9026\n",
      "pcip: 85.5960\n",
      "RMSE: 5.2848\n",
      "Best RMSE so far: 4.5353\n",
      "Best r2 so far: 0.9085\n",
      "best_score_avg so far: 89.0492\n",
      "Best pcip so far: 87.4172\n",
      "Epoch [184/325] Loss: 16.94723\n",
      "Epoch [184/325] Val Loss: 20.15416\n",
      "restarting counter\n",
      "0\n",
      "MSE: 20.6447,R2: 0.9111,RMSE: 4.5436, PCIP: 87.91,score_avg:89.5139\n",
      "r2: 0.9111\n",
      "pcip: 87.9139\n",
      "RMSE: 4.3571\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9111\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 87.9139\n",
      "Epoch [185/325] Loss: 13.32883\n",
      "Epoch [185/325] Val Loss: 26.48699\n",
      "increasing counter\n",
      "1\n",
      "MSE: 20.0886,R2: 0.9135,RMSE: 4.4820, PCIP: 87.42,score_avg:89.3852\n",
      "r2: 0.9135\n",
      "pcip: 87.4172\n",
      "RMSE: 4.7121\n",
      "save rmse state\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 87.9139\n",
      "Epoch [186/325] Loss: 10.67137\n",
      "Epoch [186/325] Val Loss: 25.28897\n",
      "restarting counter\n",
      "0\n",
      "MSE: 20.8923,R2: 0.9101,RMSE: 4.5708, PCIP: 87.42,score_avg:89.2122\n",
      "r2: 0.9101\n",
      "pcip: 87.4172\n",
      "RMSE: 4.9803\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 87.9139\n",
      "Epoch [187/325] Loss: 11.61264\n",
      "Epoch [187/325] Val Loss: 38.15403\n",
      "increasing counter\n",
      "1\n",
      "MSE: 21.2513,R2: 0.9085,RMSE: 4.6099, PCIP: 88.08,score_avg:89.4661\n",
      "r2: 0.9085\n",
      "pcip: 88.0795\n",
      "RMSE: 5.0629\n",
      "save pcip state\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [188/325] Loss: 15.75992\n",
      "Epoch [188/325] Val Loss: 29.13607\n",
      "increasing counter\n",
      "2\n",
      "MSE: 26.8015,R2: 0.8846,RMSE: 5.1770, PCIP: 85.43,score_avg:86.9471\n",
      "r2: 0.8846\n",
      "pcip: 85.4305\n",
      "RMSE: 5.1525\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [189/325] Loss: 11.44698\n",
      "Epoch [189/325] Val Loss: 37.62709\n",
      "increasing counter\n",
      "3\n",
      "MSE: 23.6444,R2: 0.8982,RMSE: 4.8626, PCIP: 86.42,score_avg:88.1232\n",
      "r2: 0.8982\n",
      "pcip: 86.4238\n",
      "RMSE: 4.7496\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [190/325] Loss: 9.53207\n",
      "Epoch [190/325] Val Loss: 32.16092\n",
      "increasing counter\n",
      "4\n",
      "MSE: 22.6941,R2: 0.9023,RMSE: 4.7638, PCIP: 86.75,score_avg:88.4933\n",
      "r2: 0.9023\n",
      "pcip: 86.7550\n",
      "RMSE: 5.0768\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [191/325] Loss: 17.00995\n",
      "Epoch [191/325] Val Loss: 41.74003\n",
      "increasing counter\n",
      "5\n",
      "MSE: 24.2856,R2: 0.8955,RMSE: 4.9280, PCIP: 87.25,score_avg:88.3991\n",
      "r2: 0.8955\n",
      "pcip: 87.2517\n",
      "RMSE: 4.8248\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [192/325] Loss: 15.99284\n",
      "Epoch [192/325] Val Loss: 30.09646\n",
      "increasing counter\n",
      "6\n",
      "MSE: 25.8712,R2: 0.8886,RMSE: 5.0864, PCIP: 87.91,score_avg:88.3890\n",
      "r2: 0.8886\n",
      "pcip: 87.9139\n",
      "RMSE: 4.6068\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [193/325] Loss: 13.39806\n",
      "Epoch [193/325] Val Loss: 16.09686\n",
      "increasing counter\n",
      "7\n",
      "MSE: 21.8990,R2: 0.9057,RMSE: 4.6796, PCIP: 86.26,score_avg:88.4161\n",
      "r2: 0.9057\n",
      "pcip: 86.2583\n",
      "RMSE: 4.9092\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [194/325] Loss: 15.92655\n",
      "Epoch [194/325] Val Loss: 23.13233\n",
      "increasing counter\n",
      "8\n",
      "MSE: 23.9002,R2: 0.8971,RMSE: 4.8888, PCIP: 83.77,score_avg:86.7437\n",
      "r2: 0.8971\n",
      "pcip: 83.7748\n",
      "RMSE: 5.1195\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [195/325] Loss: 13.25549\n",
      "Epoch [195/325] Val Loss: 43.77252\n",
      "increasing counter\n",
      "9\n",
      "MSE: 25.3177,R2: 0.8910,RMSE: 5.0317, PCIP: 86.26,score_avg:87.6803\n",
      "r2: 0.8910\n",
      "pcip: 86.2583\n",
      "RMSE: 4.6610\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [196/325] Loss: 17.60171\n",
      "Epoch [196/325] Val Loss: 29.71992\n",
      "increasing counter\n",
      "10\n",
      "MSE: 21.3185,R2: 0.9082,RMSE: 4.6172, PCIP: 85.93,score_avg:88.3755\n",
      "r2: 0.9082\n",
      "pcip: 85.9272\n",
      "RMSE: 4.9609\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [197/325] Loss: 19.94240\n",
      "Epoch [197/325] Val Loss: 44.54370\n",
      "increasing counter\n",
      "11\n",
      "MSE: 26.6244,R2: 0.8854,RMSE: 5.1599, PCIP: 86.09,score_avg:87.3163\n",
      "r2: 0.8854\n",
      "pcip: 86.0927\n",
      "RMSE: 4.8213\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [198/325] Loss: 16.80955\n",
      "Epoch [198/325] Val Loss: 28.21996\n",
      "increasing counter\n",
      "12\n",
      "MSE: 25.3003,R2: 0.8911,RMSE: 5.0299, PCIP: 84.11,score_avg:86.6079\n",
      "r2: 0.8911\n",
      "pcip: 84.1060\n",
      "RMSE: 4.8331\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [199/325] Loss: 22.82347\n",
      "Epoch [199/325] Val Loss: 31.48227\n",
      "increasing counter\n",
      "13\n",
      "MSE: 25.0044,R2: 0.8924,RMSE: 5.0004, PCIP: 87.75,score_avg:88.4928\n",
      "r2: 0.8924\n",
      "pcip: 87.7483\n",
      "RMSE: 5.1507\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [200/325] Loss: 20.52819\n",
      "Epoch [200/325] Val Loss: 31.74899\n",
      "increasing counter\n",
      "14\n",
      "MSE: 21.4078,R2: 0.9079,RMSE: 4.6269, PCIP: 86.26,score_avg:88.5218\n",
      "r2: 0.9079\n",
      "pcip: 86.2583\n",
      "RMSE: 4.8646\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [201/325] Loss: 18.65074\n",
      "Epoch [201/325] Val Loss: 37.23937\n",
      "increasing counter\n",
      "15\n",
      "MSE: 25.1899,R2: 0.8916,RMSE: 5.0190, PCIP: 86.92,score_avg:88.0390\n",
      "r2: 0.8916\n",
      "pcip: 86.9205\n",
      "RMSE: 5.1039\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [202/325] Loss: 13.65862\n",
      "Epoch [202/325] Val Loss: 35.97810\n",
      "increasing counter\n",
      "16\n",
      "MSE: 24.7951,R2: 0.8933,RMSE: 4.9795, PCIP: 85.93,score_avg:87.6272\n",
      "r2: 0.8933\n",
      "pcip: 85.9272\n",
      "RMSE: 4.8731\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [203/325] Loss: 15.70485\n",
      "Epoch [203/325] Val Loss: 16.01783\n",
      "restarting counter\n",
      "0\n",
      "MSE: 24.6792,R2: 0.8938,RMSE: 4.9678, PCIP: 84.77,score_avg:87.0727\n",
      "r2: 0.8938\n",
      "pcip: 84.7682\n",
      "RMSE: 4.7119\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [204/325] Loss: 12.13343\n",
      "Epoch [204/325] Val Loss: 30.37365\n",
      "increasing counter\n",
      "1\n",
      "MSE: 22.5151,R2: 0.9031,RMSE: 4.7450, PCIP: 86.75,score_avg:88.5318\n",
      "r2: 0.9031\n",
      "pcip: 86.7550\n",
      "RMSE: 4.9211\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [205/325] Loss: 11.25130\n",
      "Epoch [205/325] Val Loss: 31.79311\n",
      "increasing counter\n",
      "2\n",
      "MSE: 20.5678,R2: 0.9115,RMSE: 4.5352, PCIP: 84.93,score_avg:88.0403\n",
      "r2: 0.9115\n",
      "pcip: 84.9338\n",
      "RMSE: 5.0503\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [206/325] Loss: 18.68644\n",
      "Epoch [206/325] Val Loss: 31.94306\n",
      "increasing counter\n",
      "3\n",
      "MSE: 21.2917,R2: 0.9084,RMSE: 4.6143, PCIP: 85.76,score_avg:88.2985\n",
      "r2: 0.9084\n",
      "pcip: 85.7616\n",
      "RMSE: 4.6262\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [207/325] Loss: 7.51694\n",
      "Epoch [207/325] Val Loss: 27.22093\n",
      "increasing counter\n",
      "4\n",
      "MSE: 22.4373,R2: 0.9034,RMSE: 4.7368, PCIP: 87.42,score_avg:88.8797\n",
      "r2: 0.9034\n",
      "pcip: 87.4172\n",
      "RMSE: 4.6352\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [208/325] Loss: 15.48376\n",
      "Epoch [208/325] Val Loss: 34.12551\n",
      "increasing counter\n",
      "5\n",
      "MSE: 22.6875,R2: 0.9023,RMSE: 4.7631, PCIP: 86.59,score_avg:88.4120\n",
      "r2: 0.9023\n",
      "pcip: 86.5894\n",
      "RMSE: 4.7276\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [209/325] Loss: 15.63079\n",
      "Epoch [209/325] Val Loss: 15.48307\n",
      "increasing counter\n",
      "6\n",
      "MSE: 24.5429,R2: 0.8944,RMSE: 4.9541, PCIP: 86.75,score_avg:88.0954\n",
      "r2: 0.8944\n",
      "pcip: 86.7550\n",
      "RMSE: 4.6357\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [210/325] Loss: 18.05659\n",
      "Epoch [210/325] Val Loss: 14.60196\n",
      "increasing counter\n",
      "7\n",
      "MSE: 22.0612,R2: 0.9050,RMSE: 4.6969, PCIP: 84.44,score_avg:87.4706\n",
      "r2: 0.9050\n",
      "pcip: 84.4371\n",
      "RMSE: 4.6680\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9135\n",
      "best_score_avg so far: 89.5139\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [211/325] Loss: 16.20885\n",
      "Epoch [211/325] Val Loss: 13.05761\n",
      "restarting counter\n",
      "0\n",
      "MSE: 18.5155,R2: 0.9203,RMSE: 4.3030, PCIP: 87.75,score_avg:89.8893\n",
      "r2: 0.9203\n",
      "pcip: 87.7483\n",
      "RMSE: 4.9205\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [212/325] Loss: 24.41493\n",
      "Epoch [212/325] Val Loss: 25.28806\n",
      "increasing counter\n",
      "1\n",
      "MSE: 20.7138,R2: 0.9108,RMSE: 4.5512, PCIP: 84.93,score_avg:88.0089\n",
      "r2: 0.9108\n",
      "pcip: 84.9338\n",
      "RMSE: 4.6284\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [213/325] Loss: 14.92349\n",
      "Epoch [213/325] Val Loss: 18.75560\n",
      "increasing counter\n",
      "2\n",
      "MSE: 21.3441,R2: 0.9081,RMSE: 4.6200, PCIP: 87.09,score_avg:88.9494\n",
      "r2: 0.9081\n",
      "pcip: 87.0861\n",
      "RMSE: 4.5592\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [214/325] Loss: 12.85196\n",
      "Epoch [214/325] Val Loss: 29.88247\n",
      "increasing counter\n",
      "3\n",
      "MSE: 25.2417,R2: 0.8914,RMSE: 5.0241, PCIP: 85.43,score_avg:87.2828\n",
      "r2: 0.8914\n",
      "pcip: 85.4305\n",
      "RMSE: 4.5565\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [215/325] Loss: 17.52015\n",
      "Epoch [215/325] Val Loss: 31.47175\n",
      "increasing counter\n",
      "4\n",
      "MSE: 20.8337,R2: 0.9103,RMSE: 4.5644, PCIP: 84.77,score_avg:87.9003\n",
      "r2: 0.9103\n",
      "pcip: 84.7682\n",
      "RMSE: 4.7939\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [216/325] Loss: 8.48258\n",
      "Epoch [216/325] Val Loss: 26.02630\n",
      "increasing counter\n",
      "5\n",
      "MSE: 23.4300,R2: 0.8991,RMSE: 4.8405, PCIP: 87.25,score_avg:88.5833\n",
      "r2: 0.8991\n",
      "pcip: 87.2517\n",
      "RMSE: 4.5166\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [217/325] Loss: 14.09511\n",
      "Epoch [217/325] Val Loss: 38.52103\n",
      "increasing counter\n",
      "6\n",
      "MSE: 22.6487,R2: 0.9025,RMSE: 4.7591, PCIP: 86.75,score_avg:88.5031\n",
      "r2: 0.9025\n",
      "pcip: 86.7550\n",
      "RMSE: 4.7486\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [218/325] Loss: 30.79751\n",
      "Epoch [218/325] Val Loss: 36.75079\n",
      "increasing counter\n",
      "7\n",
      "MSE: 20.3921,R2: 0.9122,RMSE: 4.5158, PCIP: 87.42,score_avg:89.3199\n",
      "r2: 0.9122\n",
      "pcip: 87.4172\n",
      "RMSE: 4.8767\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [219/325] Loss: 14.55822\n",
      "Epoch [219/325] Val Loss: 16.53133\n",
      "increasing counter\n",
      "8\n",
      "MSE: 21.7231,R2: 0.9065,RMSE: 4.6608, PCIP: 88.08,score_avg:89.3645\n",
      "r2: 0.9065\n",
      "pcip: 88.0795\n",
      "RMSE: 4.7213\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [220/325] Loss: 19.01065\n",
      "Epoch [220/325] Val Loss: 27.10786\n",
      "increasing counter\n",
      "9\n",
      "MSE: 23.7458,R2: 0.8978,RMSE: 4.8730, PCIP: 85.43,score_avg:87.6047\n",
      "r2: 0.8978\n",
      "pcip: 85.4305\n",
      "RMSE: 4.8818\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [221/325] Loss: 19.86412\n",
      "Epoch [221/325] Val Loss: 24.15528\n",
      "increasing counter\n",
      "10\n",
      "MSE: 26.8167,R2: 0.8846,RMSE: 5.1785, PCIP: 84.93,score_avg:86.6955\n",
      "r2: 0.8846\n",
      "pcip: 84.9338\n",
      "RMSE: 5.1097\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [222/325] Loss: 17.01671\n",
      "Epoch [222/325] Val Loss: 18.51123\n",
      "increasing counter\n",
      "11\n",
      "MSE: 21.9301,R2: 0.9056,RMSE: 4.6830, PCIP: 86.59,score_avg:88.5750\n",
      "r2: 0.9056\n",
      "pcip: 86.5894\n",
      "RMSE: 4.8365\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [223/325] Loss: 14.73661\n",
      "Epoch [223/325] Val Loss: 34.89441\n",
      "increasing counter\n",
      "12\n",
      "MSE: 23.1401,R2: 0.9004,RMSE: 4.8104, PCIP: 85.76,score_avg:87.9007\n",
      "r2: 0.9004\n",
      "pcip: 85.7616\n",
      "RMSE: 4.7091\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [224/325] Loss: 14.78940\n",
      "Epoch [224/325] Val Loss: 47.88407\n",
      "increasing counter\n",
      "13\n",
      "MSE: 21.2860,R2: 0.9084,RMSE: 4.6137, PCIP: 87.42,score_avg:89.1275\n",
      "r2: 0.9084\n",
      "pcip: 87.4172\n",
      "RMSE: 4.6879\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [225/325] Loss: 10.94445\n",
      "Epoch [225/325] Val Loss: 19.81392\n",
      "increasing counter\n",
      "14\n",
      "MSE: 21.9023,R2: 0.9057,RMSE: 4.6800, PCIP: 87.42,score_avg:88.9949\n",
      "r2: 0.9057\n",
      "pcip: 87.4172\n",
      "RMSE: 4.6300\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [226/325] Loss: 13.59900\n",
      "Epoch [226/325] Val Loss: 23.41913\n",
      "increasing counter\n",
      "15\n",
      "MSE: 19.7445,R2: 0.9150,RMSE: 4.4435, PCIP: 87.25,score_avg:89.3765\n",
      "r2: 0.9150\n",
      "pcip: 87.2517\n",
      "RMSE: 4.5542\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [227/325] Loss: 21.86219\n",
      "Epoch [227/325] Val Loss: 18.86366\n",
      "increasing counter\n",
      "16\n",
      "MSE: 20.9849,R2: 0.9097,RMSE: 4.5809, PCIP: 86.75,score_avg:88.8612\n",
      "r2: 0.9097\n",
      "pcip: 86.7550\n",
      "RMSE: 4.4347\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [228/325] Loss: 21.23786\n",
      "Epoch [228/325] Val Loss: 30.32418\n",
      "increasing counter\n",
      "17\n",
      "MSE: 24.8086,R2: 0.8932,RMSE: 4.9808, PCIP: 85.43,score_avg:87.3760\n",
      "r2: 0.8932\n",
      "pcip: 85.4305\n",
      "RMSE: 4.8454\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [229/325] Loss: 16.40275\n",
      "Epoch [229/325] Val Loss: 42.47356\n",
      "increasing counter\n",
      "18\n",
      "MSE: 20.9934,R2: 0.9096,RMSE: 4.5819, PCIP: 86.75,score_avg:88.8593\n",
      "r2: 0.9096\n",
      "pcip: 86.7550\n",
      "RMSE: 4.7921\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [230/325] Loss: 25.95042\n",
      "Epoch [230/325] Val Loss: 24.41401\n",
      "increasing counter\n",
      "19\n",
      "MSE: 20.9296,R2: 0.9099,RMSE: 4.5749, PCIP: 87.58,score_avg:89.2870\n",
      "r2: 0.9099\n",
      "pcip: 87.5828\n",
      "RMSE: 4.6648\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [231/325] Loss: 10.33908\n",
      "Epoch [231/325] Val Loss: 24.09744\n",
      "increasing counter\n",
      "20\n",
      "MSE: 21.8230,R2: 0.9061,RMSE: 4.6715, PCIP: 88.08,score_avg:89.3431\n",
      "r2: 0.9061\n",
      "pcip: 88.0795\n",
      "RMSE: 4.9471\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [232/325] Loss: 14.56621\n",
      "Epoch [232/325] Val Loss: 30.74320\n",
      "increasing counter\n",
      "21\n",
      "MSE: 24.0601,R2: 0.8964,RMSE: 4.9051, PCIP: 85.10,score_avg:87.3715\n",
      "r2: 0.8964\n",
      "pcip: 85.0993\n",
      "RMSE: 5.1587\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [233/325] Loss: 23.52394\n",
      "Epoch [233/325] Val Loss: 27.81393\n",
      "increasing counter\n",
      "22\n",
      "MSE: 30.2728,R2: 0.8697,RMSE: 5.5021, PCIP: 87.58,score_avg:87.2762\n",
      "r2: 0.8697\n",
      "pcip: 87.5828\n",
      "RMSE: 4.9119\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [234/325] Loss: 11.45109\n",
      "Epoch [234/325] Val Loss: 46.63913\n",
      "increasing counter\n",
      "23\n",
      "MSE: 23.3439,R2: 0.8995,RMSE: 4.8316, PCIP: 86.26,score_avg:88.1051\n",
      "r2: 0.8995\n",
      "pcip: 86.2583\n",
      "RMSE: 4.9283\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [235/325] Loss: 18.03801\n",
      "Epoch [235/325] Val Loss: 26.16237\n",
      "increasing counter\n",
      "24\n",
      "MSE: 27.1377,R2: 0.8832,RMSE: 5.2094, PCIP: 86.59,score_avg:87.4542\n",
      "r2: 0.8832\n",
      "pcip: 86.5894\n",
      "RMSE: 5.0049\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [236/325] Loss: 12.19357\n",
      "Epoch [236/325] Val Loss: 37.35833\n",
      "increasing counter\n",
      "25\n",
      "MSE: 21.0419,R2: 0.9094,RMSE: 4.5871, PCIP: 88.08,score_avg:89.5112\n",
      "r2: 0.9094\n",
      "pcip: 88.0795\n",
      "RMSE: 4.6352\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [237/325] Loss: 15.09767\n",
      "Epoch [237/325] Val Loss: 14.60230\n",
      "increasing counter\n",
      "26\n",
      "MSE: 20.6391,R2: 0.9112,RMSE: 4.5430, PCIP: 86.92,score_avg:89.0184\n",
      "r2: 0.9112\n",
      "pcip: 86.9205\n",
      "RMSE: 4.3690\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [238/325] Loss: 14.29773\n",
      "Epoch [238/325] Val Loss: 46.19234\n",
      "increasing counter\n",
      "27\n",
      "MSE: 24.8593,R2: 0.8930,RMSE: 4.9859, PCIP: 85.60,score_avg:87.4479\n",
      "r2: 0.8930\n",
      "pcip: 85.5960\n",
      "RMSE: 4.5611\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [239/325] Loss: 14.03177\n",
      "Epoch [239/325] Val Loss: 13.10009\n",
      "restarting counter\n",
      "0\n",
      "MSE: 23.3288,R2: 0.8996,RMSE: 4.8300, PCIP: 87.58,score_avg:88.7706\n",
      "r2: 0.8996\n",
      "pcip: 87.5828\n",
      "RMSE: 4.4826\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [240/325] Loss: 17.10409\n",
      "Epoch [240/325] Val Loss: 17.49013\n",
      "increasing counter\n",
      "1\n",
      "MSE: 25.5385,R2: 0.8901,RMSE: 5.0536, PCIP: 84.11,score_avg:86.5567\n",
      "r2: 0.8901\n",
      "pcip: 84.1060\n",
      "RMSE: 4.9192\n",
      "Best RMSE so far: 4.3571\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [241/325] Loss: 13.93616\n",
      "Epoch [241/325] Val Loss: 20.69940\n",
      "increasing counter\n",
      "2\n",
      "MSE: 24.2810,R2: 0.8955,RMSE: 4.9276, PCIP: 85.43,score_avg:87.4895\n",
      "r2: 0.8955\n",
      "pcip: 85.4305\n",
      "RMSE: 4.1726\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [242/325] Loss: 15.57380\n",
      "Epoch [242/325] Val Loss: 34.43356\n",
      "increasing counter\n",
      "3\n",
      "MSE: 21.5654,R2: 0.9072,RMSE: 4.6439, PCIP: 87.58,score_avg:89.1502\n",
      "r2: 0.9072\n",
      "pcip: 87.5828\n",
      "RMSE: 4.2462\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [243/325] Loss: 15.65238\n",
      "Epoch [243/325] Val Loss: 23.92723\n",
      "increasing counter\n",
      "4\n",
      "MSE: 22.8440,R2: 0.9017,RMSE: 4.7795, PCIP: 87.42,score_avg:88.7922\n",
      "r2: 0.9017\n",
      "pcip: 87.4172\n",
      "RMSE: 4.8664\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 89.8893\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [244/325] Loss: 15.51041\n",
      "Epoch [244/325] Val Loss: 30.12397\n",
      "increasing counter\n",
      "5\n",
      "MSE: 18.7016,R2: 0.9195,RMSE: 4.3245, PCIP: 88.08,score_avg:90.0148\n",
      "r2: 0.9195\n",
      "pcip: 88.0795\n",
      "RMSE: 4.3696\n",
      "save score avg state\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [245/325] Loss: 19.68899\n",
      "Epoch [245/325] Val Loss: 9.49718\n",
      "increasing counter\n",
      "6\n",
      "MSE: 20.1590,R2: 0.9132,RMSE: 4.4899, PCIP: 86.75,score_avg:89.0389\n",
      "r2: 0.9132\n",
      "pcip: 86.7550\n",
      "RMSE: 4.7765\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [246/325] Loss: 14.64101\n",
      "Epoch [246/325] Val Loss: 36.46197\n",
      "increasing counter\n",
      "7\n",
      "MSE: 20.3544,R2: 0.9124,RMSE: 4.5116, PCIP: 85.43,score_avg:88.3346\n",
      "r2: 0.9124\n",
      "pcip: 85.4305\n",
      "RMSE: 4.8853\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [247/325] Loss: 15.69387\n",
      "Epoch [247/325] Val Loss: 49.94198\n",
      "increasing counter\n",
      "8\n",
      "MSE: 22.7872,R2: 0.9019,RMSE: 4.7736, PCIP: 84.27,score_avg:87.2316\n",
      "r2: 0.9019\n",
      "pcip: 84.2715\n",
      "RMSE: 4.8297\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [248/325] Loss: 20.82897\n",
      "Epoch [248/325] Val Loss: 25.29690\n",
      "increasing counter\n",
      "9\n",
      "MSE: 21.7317,R2: 0.9065,RMSE: 4.6617, PCIP: 87.25,score_avg:88.9488\n",
      "r2: 0.9065\n",
      "pcip: 87.2517\n",
      "RMSE: 4.9400\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [249/325] Loss: 12.17348\n",
      "Epoch [249/325] Val Loss: 28.70499\n",
      "increasing counter\n",
      "10\n",
      "MSE: 20.3250,R2: 0.9125,RMSE: 4.5083, PCIP: 86.75,score_avg:89.0032\n",
      "r2: 0.9125\n",
      "pcip: 86.7550\n",
      "RMSE: 4.8652\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [250/325] Loss: 16.88699\n",
      "Epoch [250/325] Val Loss: 36.32685\n",
      "increasing counter\n",
      "11\n",
      "MSE: 21.3870,R2: 0.9079,RMSE: 4.6246, PCIP: 86.59,score_avg:88.6918\n",
      "r2: 0.9079\n",
      "pcip: 86.5894\n",
      "RMSE: 4.8955\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [251/325] Loss: 12.39876\n",
      "Epoch [251/325] Val Loss: 28.75679\n",
      "increasing counter\n",
      "12\n",
      "MSE: 21.9799,R2: 0.9054,RMSE: 4.6883, PCIP: 86.75,score_avg:88.6470\n",
      "r2: 0.9054\n",
      "pcip: 86.7550\n",
      "RMSE: 4.7091\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [252/325] Loss: 20.98190\n",
      "Epoch [252/325] Val Loss: 32.41702\n",
      "increasing counter\n",
      "13\n",
      "MSE: 21.8930,R2: 0.9058,RMSE: 4.6790, PCIP: 82.78,score_avg:86.6790\n",
      "r2: 0.9058\n",
      "pcip: 82.7815\n",
      "RMSE: 5.1817\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [253/325] Loss: 16.50689\n",
      "Epoch [253/325] Val Loss: 24.09501\n",
      "increasing counter\n",
      "14\n",
      "MSE: 26.2123,R2: 0.8872,RMSE: 5.1198, PCIP: 88.08,score_avg:88.3984\n",
      "r2: 0.8872\n",
      "pcip: 88.0795\n",
      "RMSE: 4.4657\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [254/325] Loss: 21.73631\n",
      "Epoch [254/325] Val Loss: 28.89450\n",
      "increasing counter\n",
      "15\n",
      "MSE: 21.2985,R2: 0.9083,RMSE: 4.6150, PCIP: 87.58,score_avg:89.2076\n",
      "r2: 0.9083\n",
      "pcip: 87.5828\n",
      "RMSE: 4.6984\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [255/325] Loss: 16.05305\n",
      "Epoch [255/325] Val Loss: 30.45218\n",
      "increasing counter\n",
      "16\n",
      "MSE: 23.0800,R2: 0.9007,RMSE: 4.8042, PCIP: 87.58,score_avg:88.8242\n",
      "r2: 0.9007\n",
      "pcip: 87.5828\n",
      "RMSE: 4.3323\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.0795\n",
      "Epoch [256/325] Loss: 13.66440\n",
      "Epoch [256/325] Val Loss: 21.53433\n",
      "increasing counter\n",
      "17\n",
      "MSE: 19.8306,R2: 0.9146,RMSE: 4.4532, PCIP: 88.25,score_avg:89.8546\n",
      "r2: 0.9146\n",
      "pcip: 88.2450\n",
      "RMSE: 4.8545\n",
      "save pcip state\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.2450\n",
      "Epoch [257/325] Loss: 23.24632\n",
      "Epoch [257/325] Val Loss: 21.45029\n",
      "increasing counter\n",
      "18\n",
      "MSE: 21.2835,R2: 0.9084,RMSE: 4.6134, PCIP: 86.26,score_avg:88.5486\n",
      "r2: 0.9084\n",
      "pcip: 86.2583\n",
      "RMSE: 4.3530\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.2450\n",
      "Epoch [258/325] Loss: 11.27119\n",
      "Epoch [258/325] Val Loss: 38.01481\n",
      "increasing counter\n",
      "19\n",
      "MSE: 21.8028,R2: 0.9062,RMSE: 4.6693, PCIP: 86.75,score_avg:88.6851\n",
      "r2: 0.9062\n",
      "pcip: 86.7550\n",
      "RMSE: 4.5896\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.2450\n",
      "Epoch [259/325] Loss: 13.49039\n",
      "Epoch [259/325] Val Loss: 26.65140\n",
      "increasing counter\n",
      "20\n",
      "MSE: 23.8425,R2: 0.8974,RMSE: 4.8829, PCIP: 87.25,score_avg:88.4945\n",
      "r2: 0.8974\n",
      "pcip: 87.2517\n",
      "RMSE: 4.8553\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.2450\n",
      "Epoch [260/325] Loss: 12.81634\n",
      "Epoch [260/325] Val Loss: 24.08949\n",
      "increasing counter\n",
      "21\n",
      "MSE: 21.7707,R2: 0.9063,RMSE: 4.6659, PCIP: 86.59,score_avg:88.6093\n",
      "r2: 0.9063\n",
      "pcip: 86.5894\n",
      "RMSE: 4.9123\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.2450\n",
      "Epoch [261/325] Loss: 15.00026\n",
      "Epoch [261/325] Val Loss: 21.30358\n",
      "increasing counter\n",
      "22\n",
      "MSE: 20.3341,R2: 0.9125,RMSE: 4.5093, PCIP: 87.91,score_avg:89.5807\n",
      "r2: 0.9125\n",
      "pcip: 87.9139\n",
      "RMSE: 4.5950\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.2450\n",
      "Epoch [262/325] Loss: 16.79697\n",
      "Epoch [262/325] Val Loss: 12.11280\n",
      "increasing counter\n",
      "23\n",
      "MSE: 21.3136,R2: 0.9083,RMSE: 4.6167, PCIP: 88.41,score_avg:89.6182\n",
      "r2: 0.9083\n",
      "pcip: 88.4106\n",
      "RMSE: 4.5639\n",
      "save pcip state\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.4106\n",
      "Epoch [263/325] Loss: 12.07725\n",
      "Epoch [263/325] Val Loss: 27.87925\n",
      "increasing counter\n",
      "24\n",
      "MSE: 20.6183,R2: 0.9113,RMSE: 4.5407, PCIP: 86.59,score_avg:88.8573\n",
      "r2: 0.9113\n",
      "pcip: 86.5894\n",
      "RMSE: 4.9214\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.4106\n",
      "Epoch [264/325] Loss: 15.79155\n",
      "Epoch [264/325] Val Loss: 19.57029\n",
      "increasing counter\n",
      "25\n",
      "MSE: 21.5515,R2: 0.9072,RMSE: 4.6424, PCIP: 86.26,score_avg:88.4909\n",
      "r2: 0.9072\n",
      "pcip: 86.2583\n",
      "RMSE: 4.5540\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.4106\n",
      "Epoch [265/325] Loss: 13.58121\n",
      "Epoch [265/325] Val Loss: 12.91582\n",
      "increasing counter\n",
      "26\n",
      "MSE: 21.9224,R2: 0.9056,RMSE: 4.6821, PCIP: 87.25,score_avg:88.9078\n",
      "r2: 0.9056\n",
      "pcip: 87.2517\n",
      "RMSE: 4.7708\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.4106\n",
      "Epoch [266/325] Loss: 13.40609\n",
      "Epoch [266/325] Val Loss: 26.39764\n",
      "increasing counter\n",
      "27\n",
      "MSE: 25.2843,R2: 0.8912,RMSE: 5.0283, PCIP: 85.43,score_avg:87.2736\n",
      "r2: 0.8912\n",
      "pcip: 85.4305\n",
      "RMSE: 5.1014\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.4106\n",
      "Epoch [267/325] Loss: 13.39926\n",
      "Epoch [267/325] Val Loss: 4.19144\n",
      "increasing counter\n",
      "28\n",
      "MSE: 20.4749,R2: 0.9119,RMSE: 4.5249, PCIP: 86.42,score_avg:88.8054\n",
      "r2: 0.9119\n",
      "pcip: 86.4238\n",
      "RMSE: 4.3877\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.4106\n",
      "Epoch [268/325] Loss: 19.66161\n",
      "Epoch [268/325] Val Loss: 35.95851\n",
      "increasing counter\n",
      "29\n",
      "MSE: 22.1483,R2: 0.9047,RMSE: 4.7062, PCIP: 82.78,score_avg:86.6240\n",
      "r2: 0.9047\n",
      "pcip: 82.7815\n",
      "RMSE: 4.9777\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.0148\n",
      "Best pcip so far: 88.4106\n",
      "Epoch [269/325] Loss: 17.34555\n",
      "Epoch [269/325] Val Loss: 9.53903\n",
      "increasing counter\n",
      "30\n",
      "MSE: 21.2944,R2: 0.9083,RMSE: 4.6146, PCIP: 89.74,score_avg:90.2846\n",
      "r2: 0.9083\n",
      "pcip: 89.7351\n",
      "RMSE: 4.7253\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [270/325] Loss: 16.70927\n",
      "Epoch [270/325] Val Loss: 21.54700\n",
      "increasing counter\n",
      "31\n",
      "MSE: 21.8337,R2: 0.9060,RMSE: 4.6727, PCIP: 89.57,score_avg:90.0858\n",
      "r2: 0.9060\n",
      "pcip: 89.5695\n",
      "RMSE: 4.5163\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [271/325] Loss: 17.83768\n",
      "Epoch [271/325] Val Loss: 22.44547\n",
      "increasing counter\n",
      "32\n",
      "MSE: 20.6053,R2: 0.9113,RMSE: 4.5393, PCIP: 88.08,score_avg:89.6051\n",
      "r2: 0.9113\n",
      "pcip: 88.0795\n",
      "RMSE: 4.7223\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [272/325] Loss: 18.44263\n",
      "Epoch [272/325] Val Loss: 30.64155\n",
      "increasing counter\n",
      "33\n",
      "MSE: 25.2977,R2: 0.8911,RMSE: 5.0297, PCIP: 87.91,score_avg:88.5125\n",
      "r2: 0.8911\n",
      "pcip: 87.9139\n",
      "RMSE: 4.5833\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [273/325] Loss: 8.79673\n",
      "Epoch [273/325] Val Loss: 20.22593\n",
      "increasing counter\n",
      "34\n",
      "MSE: 21.5020,R2: 0.9074,RMSE: 4.6370, PCIP: 88.74,score_avg:89.7433\n",
      "r2: 0.9074\n",
      "pcip: 88.7417\n",
      "RMSE: 4.3639\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [274/325] Loss: 19.89985\n",
      "Epoch [274/325] Val Loss: 37.20804\n",
      "increasing counter\n",
      "35\n",
      "MSE: 21.8185,R2: 0.9061,RMSE: 4.6710, PCIP: 88.08,score_avg:89.3440\n",
      "r2: 0.9061\n",
      "pcip: 88.0795\n",
      "RMSE: 4.9396\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [275/325] Loss: 13.72138\n",
      "Epoch [275/325] Val Loss: 6.01868\n",
      "increasing counter\n",
      "36\n",
      "MSE: 21.3116,R2: 0.9083,RMSE: 4.6164, PCIP: 87.42,score_avg:89.1220\n",
      "r2: 0.9083\n",
      "pcip: 87.4172\n",
      "RMSE: 4.5404\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [276/325] Loss: 16.92646\n",
      "Epoch [276/325] Val Loss: 42.73062\n",
      "increasing counter\n",
      "37\n",
      "MSE: 20.5411,R2: 0.9116,RMSE: 4.5322, PCIP: 87.42,score_avg:89.2878\n",
      "r2: 0.9116\n",
      "pcip: 87.4172\n",
      "RMSE: 4.7540\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [277/325] Loss: 11.50717\n",
      "Epoch [277/325] Val Loss: 23.34116\n",
      "increasing counter\n",
      "38\n",
      "MSE: 21.1229,R2: 0.9091,RMSE: 4.5960, PCIP: 88.58,score_avg:89.7421\n",
      "r2: 0.9091\n",
      "pcip: 88.5762\n",
      "RMSE: 4.7628\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [278/325] Loss: 9.95324\n",
      "Epoch [278/325] Val Loss: 35.09738\n",
      "increasing counter\n",
      "39\n",
      "MSE: 24.4164,R2: 0.8949,RMSE: 4.9413, PCIP: 85.26,score_avg:87.3776\n",
      "r2: 0.8949\n",
      "pcip: 85.2649\n",
      "RMSE: 4.7991\n",
      "Best RMSE so far: 4.1726\n",
      "Best r2 so far: 0.9203\n",
      "best_score_avg so far: 90.2846\n",
      "Best pcip so far: 89.7351\n",
      "Epoch [279/325] Loss: 13.20337\n",
      "Epoch [279/325] Val Loss: 20.87426\n",
      "increasing counter\n",
      "40\n",
      "Early stopping!\n",
      "RMSE: 4.17\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Best trial config: {'lr': 0.00011600353938883104,\n",
    "# 'momentum': 0.009328857531182592,\n",
    "# 'opt_momentum': 0.00918667445141735, 'weight_std': 0.0033244058597709614,\n",
    "# 'weight_mean': 6.9949608597674535, 'bias_mean': 0.3226930394143123, 'bias_std': 0.012726759642886507,\n",
    "# 'batch_size': 14, 'epochs': 490, 'dims': [8, 128, 16, 32, 8, 256, 28], 'optimiser':\n",
    "# 'Adams', 'checkpoint_interval': 10}\n",
    "# otimiser': 'Adams', 'checkpoint_interval': 10,\n",
    "# 'opt_momentum': 0.009262583085697922}\n",
    "#4.7621\n",
    "\n",
    "model_ = BayesianCNN(num_feature=X_train.shape[1],dims=[512,28,64,128,256],\n",
    "                       weight_std=4.7621 ,weight_mean=0.0002, bias_mean=0.0003,\n",
    "                      bias_std=16.738).to(\"cpu\")\n",
    "\n",
    "\n",
    "model,history,best_mse =train_func(model_,n_epochs=325,batch_size=66, lr=0.00012,optimizer_=\"Nadam\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "#save model\n",
    "best_model_name = f'mir_clay_model-1.0.1.pt'\n",
    "torch.save(best_trained_model.state_dict(),os.path.join(\"../model-store/\",best_model_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "best_model_name = f'mir_clay_model-1.0.1.pt'\n",
    "best_trained_model=torch.load(os.path.join(\"../model-store/\",best_model_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_feature: int, dims=[512, 28, 64], weight_std=0.001, bias_std=9.5):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "\n",
    "        # Define the fully connected layers based on the specified dimensions\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = num_feature\n",
    "        for out_dim in dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.fc_out = nn.Linear(in_dim, 1)  # Output layer carbon\n",
    "\n",
    "        # Update the dimensions of weight_mu and weight_rho to match the output dimension\n",
    "        self.weight_dim = out_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_std = weight_std\n",
    "        self.bias_std = bias_std\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weight means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.weight_mu, mean=0.0002, std=self.weight_std)\n",
    "        nn.init.normal_(self.weight_rho, mean=0.003, std=self.weight_std)\n",
    "\n",
    "        # Initialize bias means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.bias_mu, mean=0.0003, std=self.bias_std)\n",
    "        nn.init.normal_(self.bias_rho, mean=0.004, std=self.bias_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "\n",
    "        # Pass through fully connected layers with specified dimensions\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.rrelu(fc_layer(x))\n",
    "\n",
    "        # Re-parameterization trick for sampling weights\n",
    "        weight_epsilon = Normal(0, 1).sample(self.weight_mu.size())\n",
    "        weight_sigma = torch.log(1 + torch.exp(self.weight_rho))\n",
    "        weight = self.weight_mu + weight_sigma * weight_epsilon\n",
    "\n",
    "        bias_epsilon = Normal(0, 1).sample(self.bias_mu.size())\n",
    "        bias_sigma = torch.log(1 + torch.exp(self.bias_rho))\n",
    "        bias = self.bias_mu + bias_sigma * bias_epsilon\n",
    "\n",
    "        # Enforce non-negativity on weights and biases\n",
    "        weight = torch.clamp(weight, min=0)\n",
    "        bias = torch.clamp(bias, min=0)\n",
    "\n",
    "        # Final linear layer operation\n",
    "        output = F.linear(x, weight, bias)\n",
    "\n",
    "           # Apply sigmoid activation to squash values between 0 and 1\n",
    "        #output = torch.sigmoid(output)\n",
    "\n",
    "        # Scale the values to the desired range (0 to 100)\n",
    "        #output = output * 100\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m BayesianCNN(num_feature\u001B[38;5;241m=\u001B[39mX_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m],dims\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m512\u001B[39m,\u001B[38;5;241m28\u001B[39m,\u001B[38;5;241m64\u001B[39m],\n\u001B[0;32m      2\u001B[0m                      weight_std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0016\u001B[39m,\n\u001B[0;32m      3\u001B[0m                       bias_std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0015744\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../model-store/\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[43mbest_model_name\u001B[49m)))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'best_model_name' is not defined"
     ]
    }
   ],
   "source": [
    "model = BayesianCNN(num_feature=X_train.shape[1],dims=[512,28,64],\n",
    "                     weight_std=4.7621,\n",
    "                      bias_std=0.0015744)\n",
    "model.load_state_dict(torch.load(os.path.join(\"../model-store/\",best_model_name)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54.779083]] (expected [53.870003])\n",
      "[[46.121532]] (expected [45.39])\n",
      "[[43.307953]] (expected [23.483])\n",
      "[[56.67434]] (expected [49.93])\n",
      "[[62.604366]] (expected [62.72])\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{y_pred.numpy()} (expected {y_test[i].numpy()})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "          lower      upper       pred        obs        type variable\n0     28.086370  39.008720  33.547546  30.570000  Validation     clay\n1      6.469352  16.167919  11.318636   9.000000  Validation     clay\n2     16.429228  26.670122  21.549675  22.690001  Validation     clay\n3     37.988033  54.820881  46.404457  49.000000  Validation     clay\n4     24.168974  33.480473  28.824722  33.000000  Validation     clay\n...         ...        ...        ...        ...         ...      ...\n1000   1.937880   9.121283   5.529581   6.000000  Validation     clay\n1001  12.586367  22.160376  17.373371  16.000000  Validation     clay\n1002  11.830149  21.428568  16.629358  25.910000  Validation     clay\n1003   3.951669  11.230395   7.591032  12.000000  Validation     clay\n1004  31.378937  43.028717  37.203827  38.000000  Validation     clay\n\n[1005 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lower</th>\n      <th>upper</th>\n      <th>pred</th>\n      <th>obs</th>\n      <th>type</th>\n      <th>variable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.086370</td>\n      <td>39.008720</td>\n      <td>33.547546</td>\n      <td>30.570000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.469352</td>\n      <td>16.167919</td>\n      <td>11.318636</td>\n      <td>9.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16.429228</td>\n      <td>26.670122</td>\n      <td>21.549675</td>\n      <td>22.690001</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37.988033</td>\n      <td>54.820881</td>\n      <td>46.404457</td>\n      <td>49.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24.168974</td>\n      <td>33.480473</td>\n      <td>28.824722</td>\n      <td>33.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>1.937880</td>\n      <td>9.121283</td>\n      <td>5.529581</td>\n      <td>6.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>1001</th>\n      <td>12.586367</td>\n      <td>22.160376</td>\n      <td>17.373371</td>\n      <td>16.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>1002</th>\n      <td>11.830149</td>\n      <td>21.428568</td>\n      <td>16.629358</td>\n      <td>25.910000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>1003</th>\n      <td>3.951669</td>\n      <td>11.230395</td>\n      <td>7.591032</td>\n      <td>12.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>1004</th>\n      <td>31.378937</td>\n      <td>43.028717</td>\n      <td>37.203827</td>\n      <td>38.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n  </tbody>\n</table>\n<p>1005 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result= evaluate_soil_property(model,X_test_raw,sample_size=1000)\n",
    "test_result[\"obs\"] = y_test\n",
    "test_result['type'] ='Validation'\n",
    "test_result['variable'] ='clay'\n",
    "test_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "82.90924"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result['pred'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "          lower      upper       pred        obs          type variable\n0     47.758270  73.449791  60.604031  34.650002   Calibration     clay\n1     43.306068  63.737701  53.521885  55.040001   Calibration     clay\n2     37.072853  52.108490  44.590672  43.020000   Calibration     clay\n3     43.185425  72.119888  57.652657  59.730000   Calibration     clay\n4      7.160495  16.485027  11.822762  12.000000   Calibration     clay\n...         ...        ...        ...        ...           ...      ...\n2407  33.533619  47.570034  40.551826  43.000000   Calibration     clay\n2408   2.683642   9.664970   6.174306   3.000000   Calibration     clay\n2409  17.660717  24.589886  21.125301  21.000000   Calibration     clay\n2410   5.502793  13.005546   9.254169   6.000000   Calibration     clay\n2411  18.788982  24.850605  21.819794  21.000000   Calibration     clay\n\n[2412 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lower</th>\n      <th>upper</th>\n      <th>pred</th>\n      <th>obs</th>\n      <th>type</th>\n      <th>variable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>47.758270</td>\n      <td>73.449791</td>\n      <td>60.604031</td>\n      <td>34.650002</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43.306068</td>\n      <td>63.737701</td>\n      <td>53.521885</td>\n      <td>55.040001</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37.072853</td>\n      <td>52.108490</td>\n      <td>44.590672</td>\n      <td>43.020000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>43.185425</td>\n      <td>72.119888</td>\n      <td>57.652657</td>\n      <td>59.730000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.160495</td>\n      <td>16.485027</td>\n      <td>11.822762</td>\n      <td>12.000000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2407</th>\n      <td>33.533619</td>\n      <td>47.570034</td>\n      <td>40.551826</td>\n      <td>43.000000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>2.683642</td>\n      <td>9.664970</td>\n      <td>6.174306</td>\n      <td>3.000000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>17.660717</td>\n      <td>24.589886</td>\n      <td>21.125301</td>\n      <td>21.000000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>5.502793</td>\n      <td>13.005546</td>\n      <td>9.254169</td>\n      <td>6.000000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>18.788982</td>\n      <td>24.850605</td>\n      <td>21.819794</td>\n      <td>21.000000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n  </tbody>\n</table>\n<p>2412 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result= evaluate_soil_property(model,X_train_raw,sample_size=1000)\n",
    "train_result[\"obs\"] = y_train\n",
    "train_result['type'] =' Calibration'\n",
    "train_result['variable'] ='clay'\n",
    "train_result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "final_df=pd.merge(train_result,test_result,how=\"outer\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "          lower      upper       pred        obs          type variable\n0     47.758270  73.449791  60.604031  34.650002   Calibration     clay\n1     43.306068  63.737701  53.521885  55.040001   Calibration     clay\n2     37.072853  52.108490  44.590672  43.020000   Calibration     clay\n3     43.185425  72.119888  57.652657  59.730000   Calibration     clay\n4      7.160495  16.485027  11.822762  12.000000   Calibration     clay\n...         ...        ...        ...        ...           ...      ...\n3412   1.937880   9.121283   5.529581   6.000000    Validation     clay\n3413  12.586367  22.160376  17.373371  16.000000    Validation     clay\n3414  11.830149  21.428568  16.629358  25.910000    Validation     clay\n3415   3.951669  11.230395   7.591032  12.000000    Validation     clay\n3416  31.378937  43.028717  37.203827  38.000000    Validation     clay\n\n[3417 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lower</th>\n      <th>upper</th>\n      <th>pred</th>\n      <th>obs</th>\n      <th>type</th>\n      <th>variable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>47.758270</td>\n      <td>73.449791</td>\n      <td>60.604031</td>\n      <td>34.650002</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43.306068</td>\n      <td>63.737701</td>\n      <td>53.521885</td>\n      <td>55.040001</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37.072853</td>\n      <td>52.108490</td>\n      <td>44.590672</td>\n      <td>43.020000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>43.185425</td>\n      <td>72.119888</td>\n      <td>57.652657</td>\n      <td>59.730000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.160495</td>\n      <td>16.485027</td>\n      <td>11.822762</td>\n      <td>12.000000</td>\n      <td>Calibration</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3412</th>\n      <td>1.937880</td>\n      <td>9.121283</td>\n      <td>5.529581</td>\n      <td>6.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>3413</th>\n      <td>12.586367</td>\n      <td>22.160376</td>\n      <td>17.373371</td>\n      <td>16.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>3414</th>\n      <td>11.830149</td>\n      <td>21.428568</td>\n      <td>16.629358</td>\n      <td>25.910000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>3415</th>\n      <td>3.951669</td>\n      <td>11.230395</td>\n      <td>7.591032</td>\n      <td>12.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n    <tr>\n      <th>3416</th>\n      <td>31.378937</td>\n      <td>43.028717</td>\n      <td>37.203827</td>\n      <td>38.000000</td>\n      <td>Validation</td>\n      <td>clay</td>\n    </tr>\n  </tbody>\n</table>\n<p>3417 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.to_csv(\"C:/Projects/ResearchProjects/Research-SoilSpectroscopy/Smap/Models/CNN/Mir/Results/cnn_clay.csv\")\n",
    "final_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Interval Coverage Probability (PCIP): 76.12%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_pcip(y_true, lower_bounds, upper_bounds):\n",
    "    num_samples = len(y_true)\n",
    "    num_covering_intervals = np.sum((lower_bounds <= y_true) & (y_true <= upper_bounds))\n",
    "    pcip = (num_covering_intervals / num_samples) * 100\n",
    "    return pcip\n",
    "\n",
    "pcip = calculate_pcip(test_result['obs'], test_result['lower'], test_result['upper'])\n",
    "print(f\"Prediction Interval Coverage Probability (PCIP): {pcip:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Prediction Interval Width (mpiw): 10.17\n"
     ]
    }
   ],
   "source": [
    "#mean predition interval width\n",
    "def calculate_mpiw(lower_bounds, upper_bounds):\n",
    "    num_samples = len(lower_bounds)\n",
    "    num_covering_intervals = np.sum(upper_bounds -lower_bounds)\n",
    "    mpiw = (num_covering_intervals / num_samples)\n",
    "    return mpiw\n",
    "\n",
    "mpiw = calculate_mpiw(test_result['lower'], test_result['upper'])\n",
    "print(f\"Mean Prediction Interval Width (mpiw): {mpiw:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9418567689040978"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_train_raw,y_train,type=\"r2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8909694030059376"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_test_raw,y_test,type=\"r2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "5.419767"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_test_raw,y_test,type=\"rmse\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "7.890963"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_train_raw,y_train,type=\"rmse\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}