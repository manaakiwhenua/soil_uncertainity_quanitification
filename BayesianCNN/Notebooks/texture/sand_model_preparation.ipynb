{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.distributions import Normal, beta\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.regression import R2Score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "PREDICT_PROPERTIES=['sand']\n",
    "\n",
    "def get_spectra_data(train_csv, test_csv=None,mineral=False,target_dataframe=None,over_write_csv=False):\n",
    "\n",
    "\n",
    "    SPECTRA_COLUMN_STARTING = \"nir.\"\n",
    "    ID=['labSampleId']\n",
    "\n",
    "    # check if this is a target and make the target csv to be the same with the global file used for training\n",
    "    if isinstance(train_csv, list):\n",
    "        df_source = pd.concat(map(pd.read_csv, train_csv), ignore_index=True)\n",
    "        #remove spectra signal which is more than 1\n",
    "        print(\"removing out of bound spectra from source\")\n",
    "\n",
    "        #subset only the mineral from the lucas data\n",
    "        if mineral:\n",
    "            reflectance_out_of_bound = df_source.loc[df_source[df_source[df_source.columns[pd.Series(df_source.columns).str.startswith(SPECTRA_COLUMN_STARTING)]] > 1].dropna(\n",
    "            how='all', axis=0).index]\n",
    "            df_source = df_source.loc[set(df_source.index) - set(reflectance_out_of_bound.index)]\n",
    "\n",
    "            if 'mineral' in df_source.columns:\n",
    "                df_source=df_source.loc[df_source['mineral'] == 'mineral']\n",
    "\n",
    "\n",
    "    if target_dataframe is not None:\n",
    "        df_target = target_dataframe\n",
    "\n",
    "    if isinstance(test_csv, list) and target_dataframe is None:\n",
    "        df_target = pd.concat(map(pd.read_csv, test_csv), ignore_index=True)\n",
    "\n",
    "        # remove spectra signal which is more than 1\n",
    "        print(\"removing out of bound spectra from target\")\n",
    "\n",
    "        if mineral:\n",
    "            reflectance_out_of_bound = df_target.loc[df_target[df_target[df_target.columns[\n",
    "                pd.Series(df_target.columns).str.startswith(SPECTRA_COLUMN_STARTING)]] > 1].dropna(\n",
    "                how='all', axis=0).index]\n",
    "\n",
    "            df_target = df_target.loc[set(df_target.index) - set(reflectance_out_of_bound.index)]\n",
    "\n",
    "    #common_cols = list(set.intersection(set(df_target), set(df_source)))\n",
    "    #print(common_cols)\n",
    "\n",
    "    common_cols = df_source.columns.intersection(df_target.columns)\n",
    "\n",
    "    # use this list to perform column selection\n",
    "    df_target_ = df_target[common_cols]\n",
    "    df_source_ = df_source[common_cols]\n",
    "\n",
    "\n",
    "    # extract properties for source and target\n",
    "    df_source_ = df_source_.filter(regex='^(nir.|sand|labSampleId)')\n",
    "    df_target_ = df_target_.filter(regex='^(nir.|sand|labSampleId)')\n",
    "\n",
    "\n",
    "\n",
    "    # Define the character to remove\n",
    "    character_to_remove = 'nir.'\n",
    "\n",
    "    # Remove the character from column names\n",
    "    df_source_.columns = df_source_.columns.str.replace(character_to_remove, '')\n",
    "    df_target_.columns = df_target_.columns.str.replace(character_to_remove, '')\n",
    "\n",
    "\n",
    "    # Extract column names with numbers greater than the threshold\n",
    "    source_columns_to_drop = [col for col in df_source_.filter(regex='^(nir.)').columns if int(col) > 4000]\n",
    "    target_columns_to_drop = [col for col in df_target_.filter(regex='^(nir.)').columns if int(col) > 4000]\n",
    "\n",
    "    # Drop the selected columns from the DataFrame\n",
    "    df_source_ = df_source_.drop(columns=source_columns_to_drop)\n",
    "    df_target_ = df_target_.drop(columns=target_columns_to_drop)\n",
    "\n",
    "\n",
    "    #split train datasest into train and amin_val_loss\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df_source_train, df_source_val= train_test_split(df_source_,  test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    df_source_train_= df_source_train.drop(columns=PREDICT_PROPERTIES)\n",
    "    df_source_val_= df_source_val.drop(columns=PREDICT_PROPERTIES)\n",
    "    df_target__= df_target_.drop(columns=PREDICT_PROPERTIES)\n",
    "\n",
    "    df_source_spectral_train = get_spectra(df_source_train_)\n",
    "\n",
    "    df_source_spectral_val = get_spectra(df_source_val_)\n",
    "    print(len(df_source_spectral_train))\n",
    "    print(len(df_source_spectral_val))\n",
    "\n",
    "\n",
    "    df_target_spectral = get_spectra(df_target__)\n",
    "    print(len(df_target_spectral))\n",
    "\n",
    "    if over_write_csv:\n",
    "        pd.DataFrame(df_source_spectral_train).head(2).to_csv(\"../Data/sand_source_file.csv\", index=False)\n",
    "\n",
    "    df_source_spectral_train=df_source_spectral_train.to_numpy()\n",
    "    df_source_spectral_val=df_source_spectral_val.to_numpy()\n",
    "    df_target_spectral=df_target_spectral.to_numpy()\n",
    "\n",
    "    print(df_source_spectral_train.shape)\n",
    "\n",
    "    for i in range(len(PREDICT_PROPERTIES)):\n",
    "        if PREDICT_PROPERTIES[i] not in df_source_train.columns:\n",
    "            df_source_train[PREDICT_PROPERTIES[i]] =df_source_train[PREDICT_PROPERTIES[i]]\n",
    "\n",
    "        if PREDICT_PROPERTIES[i] not in df_source_val.columns:\n",
    "            df_source_val[PREDICT_PROPERTIES[i]] =df_source_val[PREDICT_PROPERTIES[i]]\n",
    "\n",
    "        if PREDICT_PROPERTIES[i] not in df_target_.columns and PREDICT_PROPERTIES[i] in  df_target:\n",
    "            df_target_[PREDICT_PROPERTIES[i]] =df_target[PREDICT_PROPERTIES[i]]\n",
    "\n",
    "    # remove the 2500 column in the USA/NZ data\n",
    "    #df_source = df_source.drop([SPECTRA_COLUMN_STARTING, SPECTRA_COLUMN_STARTING + '2500'], axis=1, errors='ignore')\n",
    "    #df_target = df_target.drop([SPECTRA_COLUMN_STARTING, SPECTRA_COLUMN_STARTING + '2500'], axis=1, errors='ignore')\n",
    "\n",
    "    return df_source_train, df_source_val,df_target_, df_source_spectral_train,df_source_spectral_val, df_target_spectral\n",
    "\n",
    "\n",
    "def get_spectra(df):\n",
    "\n",
    "    filter_col = [col for col in df.columns]\n",
    "\n",
    "    data = df[filter_col]\n",
    "\n",
    "    data=data.iloc[:, 2::10]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing out of bound spectra from source\n",
      "removing out of bound spectra from target\n",
      "2412\n",
      "604\n",
      "1005\n",
      "(2412, 340)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "GLOBAL_CSV=[ \"C:/Projects/SmapProjects/SpectraData2023/mir_texture_cali.csv\"]\n",
    "#df_sources, df_targets, df_sources_spectral, df_targets_spectral=get_spectra_data(train_csv=GLOBAL_CSV,target_dataframe=df)\n",
    "\n",
    "TARGET_CSV=[ \"C:/Projects/SmapProjects/SpectraData2023/mir_texture_val.csv\"]\n",
    "df_source_train, df_source_val,df_target, df_source_spectral_train, df_source_spectral_val,df_target_spectral=get_spectra_data(train_csv=GLOBAL_CSV,test_csv=TARGET_CSV,over_write_csv=True)\n",
    "\n",
    "\n",
    "X_train_raw = df_source_spectral_train#.reshape(df_source_spectral_train.shape[0], df_source_spectral_train.shape[1], 1)\n",
    "#X_train=np.log(1 / X_train)\n",
    "y_train = np.array(df_source_train[PREDICT_PROPERTIES].values)\n",
    "\n",
    "X_val_raw = df_source_spectral_val#.reshape(df_source_spectral_val.shape[0], df_source_spectral_val.shape[1], 1)\n",
    "y_val= np.array(df_source_val[PREDICT_PROPERTIES].values)\n",
    "\n",
    "\n",
    "X_test_raw =df_target_spectral#.reshape(df_target_spectral.shape[0], df_target_spectral.shape[1], 1)\n",
    "y_test =df_target[PREDICT_PROPERTIES].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "     labSampleId       sand      4000      3999      3998      3997      3996  \\\n63      SB10134B   2.070000 -0.074913 -0.075331 -0.075767 -0.076227 -0.076636   \n2808    SB09894C  66.209999 -0.047917 -0.047805 -0.047687 -0.047565 -0.047452   \n102     SB09842E  55.630001 -0.076082 -0.079010 -0.082063 -0.085302 -0.088164   \n2692    SB09745B  63.259998 -0.110468 -0.111472 -0.112524 -0.113588 -0.114633   \n416     M18/0291  19.000000 -0.083878 -0.084949 -0.086063 -0.087265 -0.088286   \n...          ...        ...       ...       ...       ...       ...       ...   \n283     M17/7911  21.000000 -0.090098 -0.090718 -0.091368 -0.092011 -0.092674   \n231     M18/0811  21.000000 -0.080462 -0.081107 -0.081777 -0.082507 -0.083111   \n637     SB09963C  10.037003 -0.085436 -0.085541 -0.085652 -0.085755 -0.085874   \n1034    SB10004E   4.445000 -0.036036 -0.037472 -0.038972 -0.040534 -0.041974   \n96      SB09652A  28.629997 -0.094164 -0.093897 -0.093617 -0.093330 -0.093056   \n\n          3995      3994      3993  ...       609       608       607  \\\n63   -0.076476 -0.074446 -0.069726  ... -2.907370 -2.949399 -2.862831   \n2808 -0.047326 -0.047063 -0.046522  ...  0.603775  0.605955  0.611350   \n102  -0.090187 -0.092096 -0.093131  ... -2.526272 -2.665675 -2.753467   \n2692 -0.115279 -0.114402 -0.111095  ... -0.468744 -0.444032 -0.424865   \n416  -0.088509 -0.087096 -0.083157  ... -2.701379 -2.822400 -2.850712   \n...        ...       ...       ...  ...       ...       ...       ...   \n283  -0.093187 -0.092868 -0.091343  ... -2.656169 -2.794697 -2.843566   \n231  -0.083062 -0.081519 -0.077925  ... -2.980524 -3.094550 -3.129518   \n637  -0.085871 -0.085200 -0.083394  ... -2.115041 -2.256682 -2.295815   \n1034 -0.043056 -0.043747 -0.043782  ... -2.354397 -2.438833 -2.449846   \n96   -0.092578 -0.091154 -0.088506  ... -2.753741 -2.927823 -3.028015   \n\n           606       605       604       603       602       601       600  \n63   -2.670668 -2.419237 -2.119185 -1.775398 -1.428811 -1.094719 -0.778498  \n2808  0.621533  0.637356  0.658497  0.683174  0.708055  0.732013  0.754703  \n102  -2.788477 -2.773178 -2.718126 -2.641012 -2.563073 -2.489146 -2.418586  \n2692 -0.412824 -0.407294 -0.409310 -0.418121 -0.427112 -0.435122 -0.443024  \n416  -2.786940 -2.651300 -2.450232 -2.197872 -1.943115 -1.699641 -1.468165  \n...        ...       ...       ...       ...       ...       ...       ...  \n283  -2.806568 -2.702206 -2.539186 -2.332690 -2.124212 -1.925092 -1.735724  \n231  -3.083343 -2.969656 -2.795273 -2.575735 -2.354108 -2.142313 -1.940948  \n637  -2.241302 -2.119427 -1.941005 -1.719844 -1.496625 -1.283015 -1.080069  \n1034 -2.392724 -2.285748 -2.135217 -1.950689 -1.764472 -1.586103 -1.416722  \n96   -3.050720 -3.002509 -2.892837 -2.742251 -2.590065 -2.445654 -2.307850  \n\n[604 rows x 3403 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labSampleId</th>\n      <th>sand</th>\n      <th>4000</th>\n      <th>3999</th>\n      <th>3998</th>\n      <th>3997</th>\n      <th>3996</th>\n      <th>3995</th>\n      <th>3994</th>\n      <th>3993</th>\n      <th>...</th>\n      <th>609</th>\n      <th>608</th>\n      <th>607</th>\n      <th>606</th>\n      <th>605</th>\n      <th>604</th>\n      <th>603</th>\n      <th>602</th>\n      <th>601</th>\n      <th>600</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63</th>\n      <td>SB10134B</td>\n      <td>2.070000</td>\n      <td>-0.074913</td>\n      <td>-0.075331</td>\n      <td>-0.075767</td>\n      <td>-0.076227</td>\n      <td>-0.076636</td>\n      <td>-0.076476</td>\n      <td>-0.074446</td>\n      <td>-0.069726</td>\n      <td>...</td>\n      <td>-2.907370</td>\n      <td>-2.949399</td>\n      <td>-2.862831</td>\n      <td>-2.670668</td>\n      <td>-2.419237</td>\n      <td>-2.119185</td>\n      <td>-1.775398</td>\n      <td>-1.428811</td>\n      <td>-1.094719</td>\n      <td>-0.778498</td>\n    </tr>\n    <tr>\n      <th>2808</th>\n      <td>SB09894C</td>\n      <td>66.209999</td>\n      <td>-0.047917</td>\n      <td>-0.047805</td>\n      <td>-0.047687</td>\n      <td>-0.047565</td>\n      <td>-0.047452</td>\n      <td>-0.047326</td>\n      <td>-0.047063</td>\n      <td>-0.046522</td>\n      <td>...</td>\n      <td>0.603775</td>\n      <td>0.605955</td>\n      <td>0.611350</td>\n      <td>0.621533</td>\n      <td>0.637356</td>\n      <td>0.658497</td>\n      <td>0.683174</td>\n      <td>0.708055</td>\n      <td>0.732013</td>\n      <td>0.754703</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>SB09842E</td>\n      <td>55.630001</td>\n      <td>-0.076082</td>\n      <td>-0.079010</td>\n      <td>-0.082063</td>\n      <td>-0.085302</td>\n      <td>-0.088164</td>\n      <td>-0.090187</td>\n      <td>-0.092096</td>\n      <td>-0.093131</td>\n      <td>...</td>\n      <td>-2.526272</td>\n      <td>-2.665675</td>\n      <td>-2.753467</td>\n      <td>-2.788477</td>\n      <td>-2.773178</td>\n      <td>-2.718126</td>\n      <td>-2.641012</td>\n      <td>-2.563073</td>\n      <td>-2.489146</td>\n      <td>-2.418586</td>\n    </tr>\n    <tr>\n      <th>2692</th>\n      <td>SB09745B</td>\n      <td>63.259998</td>\n      <td>-0.110468</td>\n      <td>-0.111472</td>\n      <td>-0.112524</td>\n      <td>-0.113588</td>\n      <td>-0.114633</td>\n      <td>-0.115279</td>\n      <td>-0.114402</td>\n      <td>-0.111095</td>\n      <td>...</td>\n      <td>-0.468744</td>\n      <td>-0.444032</td>\n      <td>-0.424865</td>\n      <td>-0.412824</td>\n      <td>-0.407294</td>\n      <td>-0.409310</td>\n      <td>-0.418121</td>\n      <td>-0.427112</td>\n      <td>-0.435122</td>\n      <td>-0.443024</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>M18/0291</td>\n      <td>19.000000</td>\n      <td>-0.083878</td>\n      <td>-0.084949</td>\n      <td>-0.086063</td>\n      <td>-0.087265</td>\n      <td>-0.088286</td>\n      <td>-0.088509</td>\n      <td>-0.087096</td>\n      <td>-0.083157</td>\n      <td>...</td>\n      <td>-2.701379</td>\n      <td>-2.822400</td>\n      <td>-2.850712</td>\n      <td>-2.786940</td>\n      <td>-2.651300</td>\n      <td>-2.450232</td>\n      <td>-2.197872</td>\n      <td>-1.943115</td>\n      <td>-1.699641</td>\n      <td>-1.468165</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>M17/7911</td>\n      <td>21.000000</td>\n      <td>-0.090098</td>\n      <td>-0.090718</td>\n      <td>-0.091368</td>\n      <td>-0.092011</td>\n      <td>-0.092674</td>\n      <td>-0.093187</td>\n      <td>-0.092868</td>\n      <td>-0.091343</td>\n      <td>...</td>\n      <td>-2.656169</td>\n      <td>-2.794697</td>\n      <td>-2.843566</td>\n      <td>-2.806568</td>\n      <td>-2.702206</td>\n      <td>-2.539186</td>\n      <td>-2.332690</td>\n      <td>-2.124212</td>\n      <td>-1.925092</td>\n      <td>-1.735724</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>M18/0811</td>\n      <td>21.000000</td>\n      <td>-0.080462</td>\n      <td>-0.081107</td>\n      <td>-0.081777</td>\n      <td>-0.082507</td>\n      <td>-0.083111</td>\n      <td>-0.083062</td>\n      <td>-0.081519</td>\n      <td>-0.077925</td>\n      <td>...</td>\n      <td>-2.980524</td>\n      <td>-3.094550</td>\n      <td>-3.129518</td>\n      <td>-3.083343</td>\n      <td>-2.969656</td>\n      <td>-2.795273</td>\n      <td>-2.575735</td>\n      <td>-2.354108</td>\n      <td>-2.142313</td>\n      <td>-1.940948</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>SB09963C</td>\n      <td>10.037003</td>\n      <td>-0.085436</td>\n      <td>-0.085541</td>\n      <td>-0.085652</td>\n      <td>-0.085755</td>\n      <td>-0.085874</td>\n      <td>-0.085871</td>\n      <td>-0.085200</td>\n      <td>-0.083394</td>\n      <td>...</td>\n      <td>-2.115041</td>\n      <td>-2.256682</td>\n      <td>-2.295815</td>\n      <td>-2.241302</td>\n      <td>-2.119427</td>\n      <td>-1.941005</td>\n      <td>-1.719844</td>\n      <td>-1.496625</td>\n      <td>-1.283015</td>\n      <td>-1.080069</td>\n    </tr>\n    <tr>\n      <th>1034</th>\n      <td>SB10004E</td>\n      <td>4.445000</td>\n      <td>-0.036036</td>\n      <td>-0.037472</td>\n      <td>-0.038972</td>\n      <td>-0.040534</td>\n      <td>-0.041974</td>\n      <td>-0.043056</td>\n      <td>-0.043747</td>\n      <td>-0.043782</td>\n      <td>...</td>\n      <td>-2.354397</td>\n      <td>-2.438833</td>\n      <td>-2.449846</td>\n      <td>-2.392724</td>\n      <td>-2.285748</td>\n      <td>-2.135217</td>\n      <td>-1.950689</td>\n      <td>-1.764472</td>\n      <td>-1.586103</td>\n      <td>-1.416722</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>SB09652A</td>\n      <td>28.629997</td>\n      <td>-0.094164</td>\n      <td>-0.093897</td>\n      <td>-0.093617</td>\n      <td>-0.093330</td>\n      <td>-0.093056</td>\n      <td>-0.092578</td>\n      <td>-0.091154</td>\n      <td>-0.088506</td>\n      <td>...</td>\n      <td>-2.753741</td>\n      <td>-2.927823</td>\n      <td>-3.028015</td>\n      <td>-3.050720</td>\n      <td>-3.002509</td>\n      <td>-2.892837</td>\n      <td>-2.742251</td>\n      <td>-2.590065</td>\n      <td>-2.445654</td>\n      <td>-2.307850</td>\n    </tr>\n  </tbody>\n</table>\n<p>604 rows × 3403 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "scaler.fit(X_val_raw)\n",
    "X_val = scaler.transform(X_val_raw)\n",
    "\n",
    "X_test = X_test_raw\n",
    "\n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def evaluate(model,test_x,test,type=\"mae\",sample_size = 100):\n",
    "    df =evaluate_soil_property(model,test_x,sample_size = sample_size)\n",
    "\n",
    "    result_total_pret= pd.DataFrame(columns=['upper', 'lower','pred', 'obs'])\n",
    "\n",
    "    result_total_pret['pred'] = df['pred']\n",
    "\n",
    "    result_total_pret['lower'] =df['lower']\n",
    "    result_total_pret['upper'] =df['upper']\n",
    "\n",
    "    result_total_pret['obs']=test[:,0]\n",
    "\n",
    "    if type==\"mae\":\n",
    "        r2_total_pret =mean_absolute_error(test[:,0],result_total_pret['pred'])\n",
    "    elif type==\"r2\":\n",
    "        r2_total_pret =r2_score(test[:,0],result_total_pret['pred'])\n",
    "    elif type==\"mse\":\n",
    "        r2_total_pret =mean_squared_error(test[:,0],result_total_pret['pred'])\n",
    "    elif type==\"rmse\":\n",
    "        r2_total_pret=np.sqrt(mean_squared_error(test[:,0],result_total_pret['pred']))\n",
    "\n",
    "\n",
    "    return r2_total_pret\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "def evaluate_soil_property(model,\n",
    "                         test,\n",
    "                        sample_size = 100):\n",
    "\n",
    "     df = pd.DataFrame(columns=['lower','upper','pred'])\n",
    "\n",
    "     with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(test)\n",
    "        predictions=[]\n",
    "        test = scaler.transform(test)\n",
    "        test = torch.tensor(test, dtype=torch.float32)\n",
    "        for _ in range(sample_size):\n",
    "\n",
    "            output = model(test)  # Replace 'input_data' with your test input\n",
    "            predictions.append(output.numpy())\n",
    "\n",
    "        predictions = torch.tensor(predictions)\n",
    "\n",
    "        prediction_mean = (torch.mean(predictions, dim=0)).detach().numpy()\n",
    "\n",
    "\n",
    "        prediction_std = torch.std(predictions, dim=0).detach().numpy()\n",
    "\n",
    "        # Calculate lower and upper bounds for the prediction interval (e.g., 95% interval)\n",
    "        lower_bound_ = prediction_mean - (1.645 * prediction_std)\n",
    "        upper_bound_ = prediction_mean + (1.645 * prediction_std)\n",
    "\n",
    "\n",
    "        for i in range(0,len(prediction_mean)):\n",
    "\n",
    "            #get the first element since we ar predicting just one at a time\n",
    "            sample_pred = prediction_mean[i][0]\n",
    "\n",
    "            lower_bound = lower_bound_[i][0]\n",
    "            upper_bound = upper_bound_[i][0]\n",
    "\n",
    "            # Check and adjust upper bound if greater than 100\n",
    "            upper_bound = min(upper_bound, 99.9)\n",
    "\n",
    "            # Check and adjust prediction if greater than 100\n",
    "            sample_pred = min(sample_pred, 99.5)\n",
    "\n",
    "            row = {'upper':upper_bound,'lower':abs(lower_bound),'pred':sample_pred}\n",
    "\n",
    "            df.loc[i] = row\n",
    "        return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Define Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal,Uniform\n",
    "\n",
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_feature: int, dims=[512, 28, 64], bias_mean=0.0003,weight_mean=0.0002,weight_std=0.0001, bias_std=0.0005):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "\n",
    "        # Define the fully connected layers based on the specified dimensions\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = num_feature\n",
    "        for out_dim in dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.fc_out = nn.Linear(in_dim, 1)  # Output layer for pret\n",
    "\n",
    "        # Update the dimensions of weight_mu and weight_rho to match the output dimension\n",
    "        self.weight_dim = out_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_std = weight_std\n",
    "        self.bias_std = bias_std\n",
    "        self.weight_mean = weight_mean\n",
    "        self.bias_mean = bias_mean\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weight means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.weight_mu, self.weight_mean, std=self.weight_std)\n",
    "        nn.init.normal_(self.weight_rho, self.weight_mean, std=self.weight_std)\n",
    "\n",
    "        # Initialize bias means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.bias_mu, self.bias_mean, std=self.bias_std)\n",
    "        nn.init.normal_(self.bias_rho, self.bias_mean, std=self.bias_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "\n",
    "        # Pass through fully connected layers with specified dimensions\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.rrelu(fc_layer(x))\n",
    "            #Dropout randomly deactivates a fraction of neurons during training,\n",
    "            # which can help prevent overfitting.\n",
    "            #x = F.dropout(x,p=0.30)\n",
    "\n",
    "\n",
    "        # Re-parameterization trick for sampling weights\n",
    "        #To reduce variability, you can reduce the standard deviation of this sampling.\n",
    "        weight_epsilon = Normal(0.0,1).sample(self.weight_mu.size())\n",
    "        weight_sigma = torch.log(1 + torch.exp(self.weight_rho))\n",
    "        weight = self.weight_mu + (weight_sigma * weight_epsilon)\n",
    "\n",
    "        bias_epsilon = Normal(0.0,1).sample(self.bias_mu.size())\n",
    "        bias_sigma = torch.log(1 + torch.exp(self.bias_rho))\n",
    "        bias = self.bias_mu + (bias_sigma * bias_epsilon)\n",
    "\n",
    "        # Enforce non-negativity on weights and biases\n",
    "        weight = torch.clamp(weight, min=0)\n",
    "        bias = torch.clamp(bias, min=0)\n",
    "\n",
    "\n",
    "        # Final linear layer operation\n",
    "        output = F.linear(x, weight, bias)\n",
    "        # Apply sigmoid activation to squash values between 0 and 1\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        # Scale the values to the desired range (0 to 100)\n",
    "        output = output * 100\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "train_dataloader = DataLoader(train, batch_size=16)\n",
    "\n",
    "val= torch.utils.data.TensorDataset(X_val,y_val)\n",
    "val_dataloader = DataLoader(val, batch_size=16)\n",
    "\n",
    "test= torch.utils.data.TensorDataset(X_test,y_test)\n",
    "test_dataloader = DataLoader(test, batch_size=16)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#Manually compute the L1 loss over all model parameters:\n",
    "def l1_penalty(model):\n",
    "    l1_loss = 0.0\n",
    "    for param in model.parameters():\n",
    "        l1_loss += torch.abs(param).sum()\n",
    "    return l1_loss\n",
    "\n",
    "def train_epoch(train,val,model,loss_fn,optimizer,batch_size,n_epochs):\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size)\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] Loss: {loss.item():.5f}\")\n",
    "        #evaluate accuracy at end of each epoch\n",
    "        test_epoch(val,model, loss_fn ,optimizer,batch_size,n_epochs)\n",
    "\n",
    "\n",
    "def test_epoch(val,model,loss_fn,optimizer,batch_size,n_epochs):\n",
    "    val_dataloader = DataLoader(val, batch_size=batch_size)\n",
    "    val_loss=0\n",
    "    model.eval()\n",
    "    for inputs, targets in val_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "    avg_val_loss = val_loss / len(inputs)\n",
    "    print(f\"Epoch [{n_epochs+1}/{n_epochs}] Val Loss: {loss.item():.5f}\")\n",
    "\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def get_metrics(test,y_test,model):\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    rmse= np.sqrt(mse)\n",
    "\n",
    "    test_result= evaluate_soil_property(model,test)\n",
    "    test_result[\"obs\"] = y_test\n",
    "    pcip = calculate_pcip(test_result[\"obs\"], test_result['lower'], test_result['upper'])\n",
    "    r2score = R2Score()\n",
    "    r2= r2score(y_pred, y_test).item()\n",
    "\n",
    "    r2_pent =r2*100\n",
    "    #Balancing Trade-offs between r2 and pcip:\n",
    "    score_avg = (pcip +  r2_pent)/2\n",
    "    print(f'MSE: {mse:.4f},R2: {r2:.4f},RMSE: {rmse:.4f}, PCIP: {pcip:.2f},score_avg:{score_avg:.4f}')\n",
    "    return [mse,r2,rmse,pcip]\n",
    "\n",
    "def calculate_pcip(y_true, lower_bounds, upper_bounds):\n",
    "    num_samples = len(y_true)\n",
    "    num_covering_intervals = np.sum((lower_bounds <= y_true) & (y_true <= upper_bounds))\n",
    "    pcip = (num_covering_intervals / num_samples) * 100\n",
    "    return pcip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Hyperparamter tuning to find best Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 10:18:30,185\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2023-11-24 10:18:30,419\tINFO tensorboardx.py:178 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-11-24 10:18:30,420\tWARNING callback.py:144 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called resume (PROMPT) when no checkpoint exists in local directory (C:\\Users\\omondiagbep\\ray_results\\pbt_carbon). If you want to start a new experiment, use `resume=\"AUTO\"` or `resume=None`. If you expected an experiment to already exist, check if you supplied the correct `local_dir` to `air.RunConfig()`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 156\u001B[0m\n\u001B[0;32m    134\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m HyperBandForBOHB(\n\u001B[0;32m    135\u001B[0m     time_attr\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_iteration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    136\u001B[0m     max_t\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    140\u001B[0m     stop_last_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    141\u001B[0m )\n\u001B[0;32m    142\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m PB2(\n\u001B[0;32m    143\u001B[0m     time_attr\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_iteration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    144\u001B[0m     perturbation_interval\u001B[38;5;241m=\u001B[39mperturbation_interval,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    154\u001B[0m     },\n\u001B[0;32m    155\u001B[0m )\n\u001B[1;32m--> 156\u001B[0m analysis \u001B[38;5;241m=\u001B[39m \u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# the core training/testing of your model\u001B[39;49;00m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#storage_path=os.getcwd(), # for saving the log files\u001B[39;49;00m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpbt_carbon\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# name for the result directory\u001B[39;49;00m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#resume=\"REMOTE\",\u001B[39;49;00m\n\u001B[0;32m    161\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#metric=\"rmse\",\u001B[39;49;00m\n\u001B[0;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPROMPT\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#mode='min',\u001B[39;49;00m\n\u001B[0;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#search_alg=algo,\u001B[39;49;00m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_iteration\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdone\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrmse\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresources_per_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\n\u001B[0;32m    174\u001B[0m \u001B[43m         \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# 50 trials\u001B[39;49;00m\n\u001B[0;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_reporter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mJupyterNotebookReporter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore_avg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43moverwrite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mmax_report_frequency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m    178\u001B[0m \n\u001B[0;32m    179\u001B[0m \u001B[43m               \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloguniform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.00001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m                  \u001B[49m\u001B[38;5;66;43;03m#\"pred_interval_constant\":ray.tune.loguniform(0.28, 12.05),\u001B[39;49;00m\n\u001B[0;32m    181\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmomentum\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloguniform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.009\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_std\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloguniform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.0001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m10.02\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbias_std\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloguniform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m17.7\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdims\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_from\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mspec\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoices\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_dims\u001B[49m\u001B[43m,\u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimiser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_from\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mspec\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer_names\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m                 \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcheckpoint_interval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mperturbation_interval\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\tune\\tune.py:1036\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001B[0m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     trial_runner_cls \u001B[38;5;241m=\u001B[39m TrialRunner\n\u001B[1;32m-> 1036\u001B[0m runner \u001B[38;5;241m=\u001B[39m trial_runner_cls(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrunner_kwargs)\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mresumed:\n\u001B[0;32m   1039\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m exp \u001B[38;5;129;01min\u001B[39;00m experiments:\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:149\u001B[0m, in \u001B[0;36mTuneController.__init__\u001B[1;34m(self, search_alg, placeholder_resolvers, scheduler, experiment_path, experiment_dir_name, sync_config, stopper, resume, server_port, fail_fast, checkpoint_period, callbacks, metric, trial_checkpoint_config, chdir_to_trial_dir, reuse_actors, resource_manager_factory, _trainer_api)\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer_min_time_s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(os\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTUNE_RESULT_BUFFER_MIN_TIME_S\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0.0\u001B[39m))\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer_max_time_s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\n\u001B[0;32m    146\u001B[0m     os\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTUNE_RESULT_BUFFER_MAX_TIME_S\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m100.0\u001B[39m)\n\u001B[0;32m    147\u001B[0m )\n\u001B[1;32m--> 149\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[43msearch_alg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msearch_alg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplaceholder_resolvers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mplaceholder_resolvers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexperiment_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexperiment_dir_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_dir_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43msync_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msync_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstopper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mserver_port\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_port\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfail_fast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfail_fast\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_period\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpoint_period\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial_checkpoint_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial_checkpoint_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_trainer_api\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_trainer_api\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\tune\\execution\\trial_runner.py:254\u001B[0m, in \u001B[0;36m_TuneControllerBase.__init__\u001B[1;34m(self, search_alg, placeholder_resolvers, scheduler, experiment_path, sync_config, experiment_dir_name, stopper, resume, server_port, fail_fast, checkpoint_period, callbacks, metric, trial_checkpoint_config, _trainer_api)\u001B[0m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_manager \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_checkpoint_manager()\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 254\u001B[0m resume_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_checkpoint_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresume\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresume_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resume_config:\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\tune\\execution\\experiment_state.py:474\u001B[0m, in \u001B[0;36m_ExperimentCheckpointManager.resume\u001B[1;34m(self, resume_type)\u001B[0m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resume_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLOCAL\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPROMPT\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    473\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _experiment_checkpoint_exists(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_checkpoint_dir):\n\u001B[1;32m--> 474\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    475\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou called resume (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresume_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) when no checkpoint \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    476\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexists in local directory \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    477\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_checkpoint_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). If you want to start \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    478\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma new experiment, use `resume=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAUTO\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m` or \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    479\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`resume=None`. If you expected an experiment to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    480\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malready exist, check if you supplied the correct \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    481\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`local_dir` to `air.RunConfig()`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    482\u001B[0m         )\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m resume_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPROMPT\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    484\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m click\u001B[38;5;241m.\u001B[39mconfirm(\n\u001B[0;32m    485\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResume from local directory? \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_checkpoint_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    486\u001B[0m         ):\n",
      "\u001B[1;31mValueError\u001B[0m: You called resume (PROMPT) when no checkpoint exists in local directory (C:\\Users\\omondiagbep\\ray_results\\pbt_carbon). If you want to start a new experiment, use `resume=\"AUTO\"` or `resume=None`. If you expected an experiment to already exist, check if you supplied the correct `local_dir` to `air.RunConfig()`."
     ]
    }
   ],
   "source": [
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import ray\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.tune.schedulers import PopulationBasedTraining, HyperBandForBOHB\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from enum import Enum\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "class Optimiser(str, Enum):\n",
    "    Adams = \"Adams\"\n",
    "    SGD =\"SGD\"\n",
    "    Adadelta = \"Adadelta\"\n",
    "    Adagrad = \"Adagrad\"\n",
    "    Adamax = \"Adamax\"\n",
    "    Nadam = \"Nadam\"\n",
    "    Ftrl = \"Ftrl\"\n",
    "    RMSprop=\"RMSprop\"\n",
    "    LBFGS =\"LBFGS\"\n",
    "    #LBFGS =\"LBFGS\"\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    " # set your desired L1 regularization strength\n",
    "def objective(config):  # ①\n",
    "\n",
    "    #dataset\n",
    "    train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "\n",
    "    val= torch.utils.data.TensorDataset(X_val,y_val)\n",
    "\n",
    "\n",
    "    #dim = random.sample(output_dims, config[\"n_layers\"])\n",
    "\n",
    "    criterion=nn.MSELoss()\n",
    "\n",
    "    #  dimensions to select\n",
    "    dims = config[\"dims\"]  # You can change this to any number you want\n",
    "\n",
    "\n",
    "\n",
    "    model = BayesianCNN(num_feature=X_train.shape[1],dims=dims,\n",
    "                           weight_std=config[\"weight_std\"],\n",
    "                          bias_std=config[\"bias_std\"],\n",
    "                        bias_mean=config[\"bias_mean\"],weight_mean=config[\"weight_mean\"]).to(\"cpu\")   # Create a PyTorch conv net\n",
    "\n",
    "    if config[\"optimiser\"] == Optimiser.RMSprop:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), momentum=config[\"opt_momentum\"], lr=config[\"lr\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.LBFGS:\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(),  lr=config[\"lr\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.Adams:\n",
    "        optimizer = torch.optim.Adam(model.parameters(),  lr=config[\"lr\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.SGD:\n",
    "        optimizer = torch.optim.SGD(model.parameters(),  lr=config[\"lr\"],momentum=config[\"opt_momentum\"])\n",
    "    elif config[\"optimiser\"] == Optimiser.Nadam:\n",
    "        optimizer = torch.optim.NAdam(model.parameters(),  lr=config[\"lr\"],momentum_decay=config[\"opt_momentum\"])\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        with checkpoint.as_directory() as dir_path:\n",
    "            print(\"test\")\n",
    "            model_state, optimizer_state = torch.load(os.path.join(dir_path, \"checkpoint.pt\"))\n",
    "\n",
    "            # Load optimizer state (needed since we're using momentum),\n",
    "            # then set the `lr` and `momentum` according to the config.\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "            model.load_state_dict(model_state)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            if \"lr\" in config:\n",
    "                param_group[\"lr\"] = config[\"lr\"]\n",
    "            if \"opt_momentum\" in config:\n",
    "                param_group[\"opt_momentum\"] = config[\"opt_momentum\"]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        train_epoch(train,val,model, criterion ,optimizer,config[\"batch_size\"],config[\"epochs\"])  # Train the model\n",
    "\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        torch.save(\n",
    "            (model.state_dict(), optimizer.state_dict()), \"model/checkpoint.pt\")\n",
    "\n",
    "        mse,r2,rmse,pcip = get_metrics(X_test,y_test,model)  # Compute test accuracy\n",
    "        #optimise with pcip and r2\n",
    "        r2_pent =r2*100\n",
    "        score_avg = (pcip+r2_pent)/2\n",
    "        checkpoint = Checkpoint.from_directory(\"model\")\n",
    "\n",
    "        session.report({\"done\": pcip > 99 and rmse < 0.25,\"mse\": mse, \"r2\": r2,\"rmse\": rmse, \"pcip\": pcip,\"score_avg\":score_avg},checkpoint=checkpoint)  # Report to Tune\n",
    "\n",
    "##### RUN  ##############\n",
    "import ray\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "from ray.tune import tune\n",
    "import random\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "# Your original list of dimensions\n",
    "original_dims = [512, 256,  128, 64, 32,  16,  8, 512,256,128,64,28]\n",
    "# List of optimizer names\n",
    "optimizer_names = [\"Adams\", \"SGD\",\"Nadam\"]\n",
    "optimizer_names = [\"Adams\", \"Nadam\"]\n",
    "ray.init(num_cpus=12, num_gpus=0,_temp_dir=\"/ray\") # assign the total # of cpus and gpus, make sure you have ray.init in the beginning and ray.shutdown at the end\n",
    "sched = AsyncHyperBandScheduler(  time_attr=\"training_iteration\",\n",
    "    reduction_factor=2,\n",
    "    metric='rmse',\n",
    "    mode='min')  # set a scheduler\n",
    "\n",
    "perturbation_interval = 10\n",
    "#use population based training\n",
    "scheduler =PopulationBasedTraining(\n",
    "       time_attr=\"training_iteration\",\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    metric=\"rmse\",\n",
    "    mode=\"min\",\n",
    "    quantile_fraction=0.25,  # copy bottom % with top %\n",
    "    #hyperparam_mutation - for pbt\n",
    "    hyperparam_mutations={\n",
    "        # distribution for resampling\n",
    "        \"lr\": [0.0001, 0.1],\n",
    "        \"momentum\": [0.009,0.01],\n",
    "        \"opt_momentum\": [0.009,0.01],\n",
    "    }\n",
    ")\n",
    "algo =OptunaSearch(metric=[\"rmse\",\"pcip\"], mode=[\"min\",\"max\"])# HyperOptSearch() # if you want to use the Bayesian optimization, import BayesOptSearch instead\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=8)\n",
    "scheduler = HyperBandForBOHB(\n",
    "    time_attr=\"training_iteration\",\n",
    "    max_t=20,\n",
    "    #metric=\"rmse\",\n",
    "    #mode='min',\n",
    "    reduction_factor=0.25,\n",
    "    stop_last_trials=False,\n",
    ")\n",
    "scheduler = PB2(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=perturbation_interval,\n",
    "    metric=\"score_avg\",\n",
    "    mode=\"max\",\n",
    "    quantile_fraction=0.4,  # copy bottom % with top %\n",
    "    #hyperparam_mutation - for pbt\n",
    "    hyperparam_bounds={\n",
    "        # distribution for resampling\n",
    "        \"lr\": [0.00001, 0.1],\n",
    "        \"momentum\": [0.009,0.01],\n",
    "        \"opt_momentum\": [0.009,0.01],\n",
    "    },\n",
    ")\n",
    "analysis = tune.run(\n",
    "        objective,   # the core training/testing of your model\n",
    "        #storage_path=os.getcwd(), # for saving the log files\n",
    "        name=\"pbt_sand\", # name for the result directory\n",
    "        #resume=\"REMOTE\",\n",
    "        #metric=\"rmse\",\n",
    "        #resume=\"PROMPT\",\n",
    "        #mode='min',\n",
    "        #search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        stop={\n",
    "                \"training_iteration\": 20,\n",
    "                \"done\": True,\n",
    "                \"rmse\": 0.05,\n",
    "        },\n",
    "        resources_per_trial={\n",
    "                \"cpu\": 1,\n",
    "                \"gpu\": 0\n",
    "         },\n",
    "        num_samples=30, # 50 trials\n",
    "        progress_reporter=ray.tune.JupyterNotebookReporter(metric=\"score_avg\",overwrite=True,max_report_frequency=7),\n",
    "        config={\n",
    "\n",
    "               \"lr\": ray.tune.loguniform(0.00001, 0.1),\n",
    "                  #\"pred_interval_constant\":ray.tune.loguniform(0.28, 12.05),\n",
    "                 \"momentum\": ray.tune.loguniform(0.009,0.01),\n",
    "                 \"weight_std\": ray.tune.loguniform(0.0001,10.02),\n",
    "                 \"weight_mean\": ray.tune.loguniform(0.0001,10.02),\n",
    "                 \"bias_mean\": ray.tune.loguniform(0.0001,10.02),\n",
    "                 \"bias_std\": ray.tune.loguniform(0.01, 17.7),\n",
    "                 \"batch_size\": ray.tune.randint(2,128),\n",
    "                 \"epochs\":ray.tune.randint(2,1000),\n",
    "                 \"dims\": ray.tune.sample_from(lambda spec: random.choices(original_dims,k=random.randint(2, 7))),\n",
    "                 \"optimiser\": ray.tune.sample_from(lambda spec: random.choice(optimizer_names)),\n",
    "                 \"checkpoint_interval\": perturbation_interval\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.0004056961680898992, 'momentum': 0.009068718679849, 'weight_std': 0.0015744750485797052, 'bias_std': 0.1327185360434992, 'batch_size': 86, 'num_dims': 3, 'epochs': 243, 'dims': [8, 32, 32, 256, 128, 8], 'optimiser': 'Adams', 'checkpoint_interval': 10, 'opt_momentum': 0.00921318133493377}\n",
      "Best trial final validation mse: 0.7724632620811462\n",
      "Best trial final validation r2: 0.9489880800247192\n",
      "Best trial final validation rmse: 0.8788988918420289\n",
      "Best trial final validation pcip: 91.08635097493037\n"
     ]
    }
   ],
   "source": [
    "best_trials = analysis.get_best_trial(\"score_avg\", \"max\", \"all\")\n",
    "print(f\"Best trial config: {best_trials.config}\")\n",
    "print(f\"Best trial final validation mse: {best_trials.last_result['mse']}\")\n",
    "print(f\"Best trial final validation r2: {best_trials.last_result['r2']}\")\n",
    "print(f\"Best trial final validation rmse: {best_trials.last_result['rmse']}\")\n",
    "print(f\"Best trial final validation pcip: {best_trials.last_result['pcip']}\")\n",
    "#print(f\"Best trial final validation score_avg: {best_trials.last_result['optimal_dim']}\")\n",
    "#print(f\"Best trial final validation l1_coef: {best_trial.last_result['l1_coef']}\")\n",
    "best_checkpoints = best_trials.checkpoint.to_air_checkpoint()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omondiagbep\\ray_results\\pbt_carbon\\objective_2aefe_00005_5_batch_size=86,bias_std=0.1327,epochs=243,lr=0.0004,momentum=0.0091,num_dims=3,weight_std=0.0016_2023-09-30_11-19-14\\checkpoint_000000\n"
     ]
    }
   ],
   "source": [
    "gpus_per_trial = 2\n",
    "best_trained_model=BayesianCNN(num_feature=X_train.shape[1],dims=best_trials.config[\"dims\"],\n",
    "                           weight_std=best_trials.config[\"weight_std\"],\n",
    "                          bias_std=best_trials.config[\"bias_std\"],\n",
    "                               bias_mean=best_trials.config[\"bias_mean\"],weight_mean=best_trials.config[\"weight_mean\"])\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "best_trained_model.to(device)\n",
    "with best_checkpoints.as_directory() as dir_path:\n",
    "    print(dir_path)\n",
    "    model_state, optimizer_state = torch.load(os.path.join(dir_path, \"checkpoint.pt\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "best_checkpoint_data = best_checkpoints.to_dict()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def train_func(model,optimizer_=None,n_epochs = 1000,batch_size = 16,lr=0.0001):\n",
    "    loss_fn = nn.MSELoss()  # mean square error\n",
    "\n",
    "    if optimizer_ is None:\n",
    "        #Regularization helps prevent overfitting by penalizing large weights\n",
    "        optimizer = torch.optim.Adam(model.parameters(),  lr=lr)\n",
    "    if optimizer_ == Optimiser.RMSprop:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), momentum=0.0009, lr=lr)\n",
    "    elif optimizer_ == Optimiser.LBFGS:\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(),  lr=lr)\n",
    "    elif optimizer_ == Optimiser.Adams:\n",
    "        optimizer = torch.optim.Adam(model.parameters(),  lr=lr)\n",
    "    elif optimizer_ == Optimiser.SGD:\n",
    "        optimizer = torch.optim.SGD(model.parameters(),  lr=lr,momentum=0.0009)\n",
    "    elif optimizer_ == Optimiser.Nadam:\n",
    "        optimizer = torch.optim.NAdam(model.parameters(),  lr=lr,momentum_decay=0.0009)\n",
    "\n",
    "\n",
    "    # Hold the best model\n",
    "    best_mse = np.inf # init to infinity\n",
    "\n",
    "    history = []\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    early_stop=False\n",
    "    n_epochs_stop = 40\n",
    "    min_val_loss=float('inf')\n",
    "    best_r2 = 0.0000\n",
    "    best_pcip = 0.0000\n",
    "    best_score_avg =0.0000\n",
    "    criterion=nn.MSELoss()\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "    val = torch.utils.data.TensorDataset(X_val,y_val)\n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val, batch_size=batch_size)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss =0\n",
    "        val_loss=0\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] Loss: {loss.item():.5f}\")\n",
    "\n",
    "        for inputs, targets in val_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the training loss for this epoch\n",
    "        avg_val_loss = val_loss / len(inputs)\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] Val Loss: {loss.item():.5f}\")\n",
    "\n",
    "        #at the start\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            epochs_no_improve = 0\n",
    "            min_val_loss = avg_val_loss\n",
    "            print(\"restarting counter\")\n",
    "            print(epochs_no_improve)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(\"increasing counter\")\n",
    "            print(epochs_no_improve)\n",
    "\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!')\n",
    "            early_stop = True\n",
    "            break\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        mse,r2,rmse,pcip = get_metrics(X_val,y_val,model)\n",
    "        score_avg= ((r2*100)+pcip)/2\n",
    "        print(f'r2: {r2:.4f}')\n",
    "        print(f'pcip: {pcip:.4f}')\n",
    "        y_pred = model(X_val)\n",
    "\n",
    "        mse = loss_fn(y_pred, y_val)\n",
    "        mse = float(mse)\n",
    "        rmse= np.sqrt(mse)\n",
    "        print(f'RMSE: {rmse:.4f}')\n",
    "        history.append(rmse)\n",
    "        if rmse < best_mse:\n",
    "            best_mse = rmse\n",
    "\n",
    "        if r2 > best_r2 :\n",
    "            best_r2 = r2\n",
    "            print(\"save rmse state\")\n",
    "            modelrmse_state_dict = model.state_dict()\n",
    "        if pcip > best_pcip :\n",
    "            best_pcip = pcip\n",
    "            print(\"save pcip state\")\n",
    "            modelpcip_state_dict = model.state_dict()\n",
    "        if score_avg  > best_score_avg:\n",
    "            best_score_avg = score_avg\n",
    "            print(\"save score avg state\")\n",
    "            modelscore_avg_state_dict = model.state_dict()\n",
    "\n",
    "        if early_stop:\n",
    "            print('Training stopped early.')\n",
    "\n",
    "\n",
    "        print(f'Best RMSE so far: {best_mse:.4f}')\n",
    "        print(f'Best r2 so far: {best_r2:.4f}')\n",
    "        print(f'best_score_avg so far: {best_score_avg:.4f}')\n",
    "        print(f'Best pcip so far: {best_pcip:.4f}')\n",
    "\n",
    "\n",
    "    #modelrmse_state_dict.update(modelpcip_state_dict)\n",
    "\n",
    "    # Load the updated state_dict back into model\n",
    "    #model.load_state_dict(modelrmse_state_dict)\n",
    "    model.load_state_dict(modelscore_avg_state_dict)\n",
    "    #model_pcip=model.load_state_dict(modelpcip_state_dict)\n",
    "    #model_r2=model.load_state_dict(modelrmse_state_dict)\n",
    "    print(\"RMSE: %.2f\" % best_mse)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history)\n",
    "    plt.show()\n",
    "    return model,history,best_mse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best architecture found\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_feature: int, dims=[512, 28, 64], bias_mean=0.0003,weight_mean=0.0002,weight_std=0.0001, bias_std=0.0005):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "\n",
    "        # Define the fully connected layers based on the specified dimensions\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = num_feature\n",
    "        for out_dim in dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.fc_out = nn.Linear(in_dim, 1)  # Output layer for pret\n",
    "\n",
    "        # Update the dimensions of weight_mu and weight_rho to match the output dimension\n",
    "        self.weight_dim = out_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_std = weight_std\n",
    "        self.bias_std = bias_std\n",
    "        self.weight_mean = weight_mean\n",
    "        self.bias_mean = bias_mean\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weight means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.weight_mu, self.weight_mean, std=self.weight_std)\n",
    "        nn.init.normal_(self.weight_rho, self.weight_mean, std=self.weight_std)\n",
    "\n",
    "        # Initialize bias means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.bias_mu, self.bias_mean, std=self.bias_std)\n",
    "        nn.init.normal_(self.bias_rho, self.bias_mean, std=self.bias_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "\n",
    "        # Pass through fully connected layers with specified dimensions\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.rrelu(fc_layer(x))\n",
    "            #Dropout randomly deactivates a fraction of neurons during training,\n",
    "            # which can help prevent overfitting.\n",
    "            x = F.dropout(x,p=0.30)\n",
    "\n",
    "\n",
    "        # Re-parameterization trick for sampling weights\n",
    "        #To reduce variability, you can reduce the standard deviation of this sampling.\n",
    "        weight_epsilon = Normal(0.0,1).sample(self.weight_mu.size())\n",
    "        weight_sigma = torch.log(1 + torch.exp(self.weight_rho))\n",
    "        weight = self.weight_mu + (weight_sigma * weight_epsilon)\n",
    "\n",
    "        bias_epsilon = Normal(0.0,1).sample(self.bias_mu.size())\n",
    "        bias_sigma = torch.log(1 + torch.exp(self.bias_rho))\n",
    "        bias = self.bias_mu + (bias_sigma * bias_epsilon)\n",
    "\n",
    "        # Enforce non-negativity on weights and biases\n",
    "        weight = torch.clamp(weight, min=0)\n",
    "        bias = torch.clamp(bias, min=0)\n",
    "\n",
    "\n",
    "        # Final linear layer operation\n",
    "        output = F.linear(x, weight, bias)\n",
    "        # Apply sigmoid activation to squash values between 0 and 1\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        # Scale the values to the desired range (0 to 100)\n",
    "        output = output * 100\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Optimiser(str, Enum):\n",
    "    Adams = \"Adams\"\n",
    "    SGD =\"SGD\"\n",
    "    Adadelta = \"Adadelta\"\n",
    "    Adagrad = \"Adagrad\"\n",
    "    Adamax = \"Adamax\"\n",
    "    Nadam = \"Nadam\"\n",
    "    Ftrl = \"Ftrl\"\n",
    "    RMSprop=\"RMSprop\"\n",
    "    LBFGS =\"LBFGS\"\n",
    "    #LBFGS =\"LBFGS\"\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "#fine tune if needed - not use"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/325] Loss: 5544.83105\n",
      "Epoch [1/325] Val Loss: 6853.65332\n",
      "restarting counter\n",
      "0\n",
      "MSE: 5664.8354,R2: -8.0628,RMSE: 75.2651, PCIP: 0.00,score_avg:-403.1424\n",
      "r2: -8.0628\n",
      "pcip: 0.0000\n",
      "RMSE: 75.2804\n",
      "Best RMSE so far: 75.2804\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 0.0000\n",
      "Epoch [2/325] Loss: 1170.01196\n",
      "Epoch [2/325] Val Loss: 775.82495\n",
      "restarting counter\n",
      "0\n",
      "MSE: 1628.3947,R2: -1.6052,RMSE: 40.3534, PCIP: 26.66,score_avg:-66.9310\n",
      "r2: -1.6052\n",
      "pcip: 26.6556\n",
      "RMSE: 34.1287\n",
      "save pcip state\n",
      "Best RMSE so far: 34.1287\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 26.6556\n",
      "Epoch [3/325] Loss: 1058.28076\n",
      "Epoch [3/325] Val Loss: 795.00403\n",
      "restarting counter\n",
      "0\n",
      "MSE: 1077.1847,R2: -0.7233,RMSE: 32.8205, PCIP: 24.83,score_avg:-23.7491\n",
      "r2: -0.7233\n",
      "pcip: 24.8344\n",
      "RMSE: 36.0292\n",
      "Best RMSE so far: 34.1287\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 26.6556\n",
      "Epoch [4/325] Loss: 888.28375\n",
      "Epoch [4/325] Val Loss: 1321.69727\n",
      "restarting counter\n",
      "0\n",
      "MSE: 1021.7144,R2: -0.6346,RMSE: 31.9643, PCIP: 27.48,score_avg:-17.9874\n",
      "r2: -0.6346\n",
      "pcip: 27.4834\n",
      "RMSE: 31.1835\n",
      "save pcip state\n",
      "Best RMSE so far: 31.1835\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 27.4834\n",
      "Epoch [5/325] Loss: 913.22406\n",
      "Epoch [5/325] Val Loss: 946.16144\n",
      "restarting counter\n",
      "0\n",
      "MSE: 898.8235,R2: -0.4380,RMSE: 29.9804, PCIP: 25.83,score_avg:-8.9849\n",
      "r2: -0.4380\n",
      "pcip: 25.8278\n",
      "RMSE: 30.0994\n",
      "Best RMSE so far: 30.0994\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 27.4834\n",
      "Epoch [6/325] Loss: 675.59222\n",
      "Epoch [6/325] Val Loss: 1435.75110\n",
      "increasing counter\n",
      "1\n",
      "MSE: 833.7780,R2: -0.3339,RMSE: 28.8752, PCIP: 24.83,score_avg:-4.2785\n",
      "r2: -0.3339\n",
      "pcip: 24.8344\n",
      "RMSE: 29.8217\n",
      "Best RMSE so far: 29.8217\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 27.4834\n",
      "Epoch [7/325] Loss: 618.74426\n",
      "Epoch [7/325] Val Loss: 1251.75146\n",
      "restarting counter\n",
      "0\n",
      "MSE: 798.3449,R2: -0.2772,RMSE: 28.2550, PCIP: 26.32,score_avg:-0.6991\n",
      "r2: -0.2772\n",
      "pcip: 26.3245\n",
      "RMSE: 28.7658\n",
      "Best RMSE so far: 28.7658\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 27.4834\n",
      "Epoch [8/325] Loss: 767.33643\n",
      "Epoch [8/325] Val Loss: 918.05664\n",
      "restarting counter\n",
      "0\n",
      "MSE: 795.7872,R2: -0.2731,RMSE: 28.2097, PCIP: 24.34,score_avg:-1.4879\n",
      "r2: -0.2731\n",
      "pcip: 24.3377\n",
      "RMSE: 28.7170\n",
      "Best RMSE so far: 28.7170\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.0000\n",
      "Best pcip so far: 27.4834\n",
      "Epoch [9/325] Loss: 702.89923\n",
      "Epoch [9/325] Val Loss: 1179.82690\n",
      "restarting counter\n",
      "0\n",
      "MSE: 813.9979,R2: -0.3023,RMSE: 28.5306, PCIP: 31.62,score_avg:0.6978\n",
      "r2: -0.3023\n",
      "pcip: 31.6225\n",
      "RMSE: 30.2013\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 28.7170\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.6978\n",
      "Best pcip so far: 31.6225\n",
      "Epoch [10/325] Loss: 582.96051\n",
      "Epoch [10/325] Val Loss: 603.33972\n",
      "increasing counter\n",
      "1\n",
      "MSE: 890.9784,R2: -0.4254,RMSE: 29.8493, PCIP: 28.48,score_avg:-7.0329\n",
      "r2: -0.4254\n",
      "pcip: 28.4768\n",
      "RMSE: 26.7120\n",
      "Best RMSE so far: 26.7120\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 0.6978\n",
      "Best pcip so far: 31.6225\n",
      "Epoch [11/325] Loss: 646.88867\n",
      "Epoch [11/325] Val Loss: 255.51453\n",
      "restarting counter\n",
      "0\n",
      "MSE: 714.6269,R2: -0.1433,RMSE: 26.7325, PCIP: 28.81,score_avg:7.2394\n",
      "r2: -0.1433\n",
      "pcip: 28.8079\n",
      "RMSE: 26.0789\n",
      "save score avg state\n",
      "Best RMSE so far: 26.0789\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 7.2394\n",
      "Best pcip so far: 31.6225\n",
      "Epoch [12/325] Loss: 507.94672\n",
      "Epoch [12/325] Val Loss: 1082.47327\n",
      "increasing counter\n",
      "1\n",
      "MSE: 711.8409,R2: -0.1388,RMSE: 26.6803, PCIP: 32.78,score_avg:9.4490\n",
      "r2: -0.1388\n",
      "pcip: 32.7815\n",
      "RMSE: 27.3327\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 26.0789\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 9.4490\n",
      "Best pcip so far: 32.7815\n",
      "Epoch [13/325] Loss: 547.23315\n",
      "Epoch [13/325] Val Loss: 1607.34558\n",
      "increasing counter\n",
      "2\n",
      "MSE: 994.8939,R2: -0.5917,RMSE: 31.5419, PCIP: 27.98,score_avg:-15.5936\n",
      "r2: -0.5917\n",
      "pcip: 27.9801\n",
      "RMSE: 28.3389\n",
      "Best RMSE so far: 26.0789\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 9.4490\n",
      "Best pcip so far: 32.7815\n",
      "Epoch [14/325] Loss: 655.57397\n",
      "Epoch [14/325] Val Loss: 316.80258\n",
      "restarting counter\n",
      "0\n",
      "MSE: 682.6052,R2: -0.0921,RMSE: 26.1267, PCIP: 25.99,score_avg:8.3936\n",
      "r2: -0.0921\n",
      "pcip: 25.9934\n",
      "RMSE: 28.1706\n",
      "Best RMSE so far: 26.0789\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 9.4490\n",
      "Best pcip so far: 32.7815\n",
      "Epoch [15/325] Loss: 653.30554\n",
      "Epoch [15/325] Val Loss: 283.01938\n",
      "restarting counter\n",
      "0\n",
      "MSE: 669.9489,R2: -0.0718,RMSE: 25.8834, PCIP: 28.48,score_avg:10.6478\n",
      "r2: -0.0718\n",
      "pcip: 28.4768\n",
      "RMSE: 26.6378\n",
      "save score avg state\n",
      "Best RMSE so far: 26.0789\n",
      "Best r2 so far: 0.0000\n",
      "best_score_avg so far: 10.6478\n",
      "Best pcip so far: 32.7815\n",
      "Epoch [16/325] Loss: 495.24377\n",
      "Epoch [16/325] Val Loss: 1921.73853\n",
      "increasing counter\n",
      "1\n",
      "MSE: 605.6080,R2: 0.0311,RMSE: 24.6091, PCIP: 31.95,score_avg:17.5329\n",
      "r2: 0.0311\n",
      "pcip: 31.9536\n",
      "RMSE: 26.0273\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 26.0273\n",
      "Best r2 so far: 0.0311\n",
      "best_score_avg so far: 17.5329\n",
      "Best pcip so far: 32.7815\n",
      "Epoch [17/325] Loss: 583.48297\n",
      "Epoch [17/325] Val Loss: 756.99634\n",
      "increasing counter\n",
      "2\n",
      "MSE: 816.8830,R2: -0.3069,RMSE: 28.5812, PCIP: 31.62,score_avg:0.4670\n",
      "r2: -0.3069\n",
      "pcip: 31.6225\n",
      "RMSE: 24.6651\n",
      "Best RMSE so far: 24.6651\n",
      "Best r2 so far: 0.0311\n",
      "best_score_avg so far: 17.5329\n",
      "Best pcip so far: 32.7815\n",
      "Epoch [18/325] Loss: 501.23502\n",
      "Epoch [18/325] Val Loss: 439.43268\n",
      "increasing counter\n",
      "3\n",
      "MSE: 605.8768,R2: 0.0307,RMSE: 24.6146, PCIP: 34.77,score_avg:18.9187\n",
      "r2: 0.0307\n",
      "pcip: 34.7682\n",
      "RMSE: 24.7317\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 24.6651\n",
      "Best r2 so far: 0.0311\n",
      "best_score_avg so far: 18.9187\n",
      "Best pcip so far: 34.7682\n",
      "Epoch [19/325] Loss: 583.63446\n",
      "Epoch [19/325] Val Loss: 778.66516\n",
      "increasing counter\n",
      "4\n",
      "MSE: 748.3952,R2: -0.1973,RMSE: 27.3568, PCIP: 31.13,score_avg:5.6972\n",
      "r2: -0.1973\n",
      "pcip: 31.1258\n",
      "RMSE: 24.9140\n",
      "Best RMSE so far: 24.6651\n",
      "Best r2 so far: 0.0311\n",
      "best_score_avg so far: 18.9187\n",
      "Best pcip so far: 34.7682\n",
      "Epoch [20/325] Loss: 576.56250\n",
      "Epoch [20/325] Val Loss: 229.57097\n",
      "increasing counter\n",
      "5\n",
      "MSE: 670.5751,R2: -0.0728,RMSE: 25.8955, PCIP: 33.28,score_avg:12.9983\n",
      "r2: -0.0728\n",
      "pcip: 33.2781\n",
      "RMSE: 23.6326\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0311\n",
      "best_score_avg so far: 18.9187\n",
      "Best pcip so far: 34.7682\n",
      "Epoch [21/325] Loss: 536.70898\n",
      "Epoch [21/325] Val Loss: 1175.86816\n",
      "increasing counter\n",
      "6\n",
      "MSE: 653.0681,R2: -0.0448,RMSE: 25.5552, PCIP: 35.10,score_avg:15.3093\n",
      "r2: -0.0448\n",
      "pcip: 35.0993\n",
      "RMSE: 24.5121\n",
      "save pcip state\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0311\n",
      "best_score_avg so far: 18.9187\n",
      "Best pcip so far: 35.0993\n",
      "Epoch [22/325] Loss: 487.27887\n",
      "Epoch [22/325] Val Loss: 267.55621\n",
      "increasing counter\n",
      "7\n",
      "MSE: 573.5599,R2: 0.0824,RMSE: 23.9491, PCIP: 33.28,score_avg:20.7588\n",
      "r2: 0.0824\n",
      "pcip: 33.2781\n",
      "RMSE: 26.3517\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0824\n",
      "best_score_avg so far: 20.7588\n",
      "Best pcip so far: 35.0993\n",
      "Epoch [23/325] Loss: 444.82626\n",
      "Epoch [23/325] Val Loss: 291.30756\n",
      "restarting counter\n",
      "0\n",
      "MSE: 572.8011,R2: 0.0836,RMSE: 23.9333, PCIP: 33.94,score_avg:21.1506\n",
      "r2: 0.0836\n",
      "pcip: 33.9404\n",
      "RMSE: 25.4097\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0836\n",
      "best_score_avg so far: 21.1506\n",
      "Best pcip so far: 35.0993\n",
      "Epoch [24/325] Loss: 458.67209\n",
      "Epoch [24/325] Val Loss: 290.16663\n",
      "restarting counter\n",
      "0\n",
      "MSE: 768.1870,R2: -0.2290,RMSE: 27.7162, PCIP: 33.77,score_avg:5.4385\n",
      "r2: -0.2290\n",
      "pcip: 33.7748\n",
      "RMSE: 25.5233\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0836\n",
      "best_score_avg so far: 21.1506\n",
      "Best pcip so far: 35.0993\n",
      "Epoch [25/325] Loss: 505.36871\n",
      "Epoch [25/325] Val Loss: 392.11432\n",
      "increasing counter\n",
      "1\n",
      "MSE: 574.2073,R2: 0.0814,RMSE: 23.9626, PCIP: 33.28,score_avg:20.7070\n",
      "r2: 0.0814\n",
      "pcip: 33.2781\n",
      "RMSE: 27.9071\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0836\n",
      "best_score_avg so far: 21.1506\n",
      "Best pcip so far: 35.0993\n",
      "Epoch [26/325] Loss: 597.11383\n",
      "Epoch [26/325] Val Loss: 1047.64014\n",
      "increasing counter\n",
      "2\n",
      "MSE: 619.8179,R2: 0.0084,RMSE: 24.8961, PCIP: 35.93,score_avg:18.3830\n",
      "r2: 0.0084\n",
      "pcip: 35.9272\n",
      "RMSE: 24.2650\n",
      "save pcip state\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0836\n",
      "best_score_avg so far: 21.1506\n",
      "Best pcip so far: 35.9272\n",
      "Epoch [27/325] Loss: 458.61996\n",
      "Epoch [27/325] Val Loss: 961.99103\n",
      "increasing counter\n",
      "3\n",
      "MSE: 785.1141,R2: -0.2561,RMSE: 28.0199, PCIP: 34.77,score_avg:4.5811\n",
      "r2: -0.2561\n",
      "pcip: 34.7682\n",
      "RMSE: 36.9089\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.0836\n",
      "best_score_avg so far: 21.1506\n",
      "Best pcip so far: 35.9272\n",
      "Epoch [28/325] Loss: 525.72290\n",
      "Epoch [28/325] Val Loss: 441.31158\n",
      "increasing counter\n",
      "4\n",
      "MSE: 534.8253,R2: 0.1444,RMSE: 23.1263, PCIP: 32.78,score_avg:23.6089\n",
      "r2: 0.1444\n",
      "pcip: 32.7815\n",
      "RMSE: 24.4399\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 35.9272\n",
      "Epoch [29/325] Loss: 572.15533\n",
      "Epoch [29/325] Val Loss: 297.77069\n",
      "increasing counter\n",
      "5\n",
      "MSE: 588.3356,R2: 0.0588,RMSE: 24.2556, PCIP: 31.62,score_avg:18.7490\n",
      "r2: 0.0588\n",
      "pcip: 31.6225\n",
      "RMSE: 23.6575\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 35.9272\n",
      "Epoch [30/325] Loss: 586.02252\n",
      "Epoch [30/325] Val Loss: 304.50470\n",
      "increasing counter\n",
      "6\n",
      "MSE: 670.9478,R2: -0.0734,RMSE: 25.9027, PCIP: 32.95,score_avg:12.8029\n",
      "r2: -0.0734\n",
      "pcip: 32.9470\n",
      "RMSE: 29.7146\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 35.9272\n",
      "Epoch [31/325] Loss: 503.37390\n",
      "Epoch [31/325] Val Loss: 285.14636\n",
      "increasing counter\n",
      "7\n",
      "MSE: 721.8557,R2: -0.1549,RMSE: 26.8674, PCIP: 36.42,score_avg:10.4691\n",
      "r2: -0.1549\n",
      "pcip: 36.4238\n",
      "RMSE: 23.9649\n",
      "save pcip state\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 36.4238\n",
      "Epoch [32/325] Loss: 465.03796\n",
      "Epoch [32/325] Val Loss: 514.39825\n",
      "increasing counter\n",
      "8\n",
      "MSE: 553.3081,R2: 0.1148,RMSE: 23.5225, PCIP: 34.60,score_avg:23.0410\n",
      "r2: 0.1148\n",
      "pcip: 34.6026\n",
      "RMSE: 24.9635\n",
      "Best RMSE so far: 23.6326\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 36.4238\n",
      "Epoch [33/325] Loss: 464.47525\n",
      "Epoch [33/325] Val Loss: 290.62277\n",
      "increasing counter\n",
      "9\n",
      "MSE: 714.6972,R2: -0.1434,RMSE: 26.7338, PCIP: 34.93,score_avg:10.2967\n",
      "r2: -0.1434\n",
      "pcip: 34.9338\n",
      "RMSE: 23.3628\n",
      "Best RMSE so far: 23.3628\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 36.4238\n",
      "Epoch [34/325] Loss: 406.07993\n",
      "Epoch [34/325] Val Loss: 260.85834\n",
      "restarting counter\n",
      "0\n",
      "MSE: 878.1101,R2: -0.4048,RMSE: 29.6329, PCIP: 36.42,score_avg:-2.0300\n",
      "r2: -0.4048\n",
      "pcip: 36.4238\n",
      "RMSE: 22.9907\n",
      "Best RMSE so far: 22.9907\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 36.4238\n",
      "Epoch [35/325] Loss: 358.68408\n",
      "Epoch [35/325] Val Loss: 250.92097\n",
      "increasing counter\n",
      "1\n",
      "MSE: 553.8873,R2: 0.1139,RMSE: 23.5348, PCIP: 35.60,score_avg:23.4914\n",
      "r2: 0.1139\n",
      "pcip: 35.5960\n",
      "RMSE: 23.0867\n",
      "Best RMSE so far: 22.9907\n",
      "Best r2 so far: 0.1444\n",
      "best_score_avg so far: 23.6089\n",
      "Best pcip so far: 36.4238\n",
      "Epoch [36/325] Loss: 677.86200\n",
      "Epoch [36/325] Val Loss: 287.27887\n",
      "increasing counter\n",
      "2\n",
      "MSE: 474.1298,R2: 0.2415,RMSE: 21.7745, PCIP: 39.40,score_avg:31.7753\n",
      "r2: 0.2415\n",
      "pcip: 39.4040\n",
      "RMSE: 22.3199\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 39.4040\n",
      "Epoch [37/325] Loss: 551.04364\n",
      "Epoch [37/325] Val Loss: 289.22818\n",
      "increasing counter\n",
      "3\n",
      "MSE: 548.0189,R2: 0.1233,RMSE: 23.4098, PCIP: 33.94,score_avg:23.1330\n",
      "r2: 0.1233\n",
      "pcip: 33.9404\n",
      "RMSE: 23.1338\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 39.4040\n",
      "Epoch [38/325] Loss: 669.77917\n",
      "Epoch [38/325] Val Loss: 510.65689\n",
      "increasing counter\n",
      "4\n",
      "MSE: 598.0338,R2: 0.0432,RMSE: 24.4547, PCIP: 35.43,score_avg:19.8772\n",
      "r2: 0.0432\n",
      "pcip: 35.4305\n",
      "RMSE: 25.0790\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 39.4040\n",
      "Epoch [39/325] Loss: 397.29724\n",
      "Epoch [39/325] Val Loss: 531.76422\n",
      "increasing counter\n",
      "5\n",
      "MSE: 590.2754,R2: 0.0557,RMSE: 24.2956, PCIP: 37.42,score_avg:21.4912\n",
      "r2: 0.0557\n",
      "pcip: 37.4172\n",
      "RMSE: 29.4084\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 39.4040\n",
      "Epoch [40/325] Loss: 574.91339\n",
      "Epoch [40/325] Val Loss: 248.80119\n",
      "increasing counter\n",
      "6\n",
      "MSE: 513.6881,R2: 0.1782,RMSE: 22.6647, PCIP: 40.73,score_avg:29.2732\n",
      "r2: 0.1782\n",
      "pcip: 40.7285\n",
      "RMSE: 23.0002\n",
      "save pcip state\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [41/325] Loss: 467.62805\n",
      "Epoch [41/325] Val Loss: 1527.30530\n",
      "increasing counter\n",
      "7\n",
      "MSE: 774.7859,R2: -0.2395,RMSE: 27.8350, PCIP: 36.26,score_avg:6.1523\n",
      "r2: -0.2395\n",
      "pcip: 36.2583\n",
      "RMSE: 22.7305\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [42/325] Loss: 429.20300\n",
      "Epoch [42/325] Val Loss: 232.39380\n",
      "increasing counter\n",
      "8\n",
      "MSE: 514.4447,R2: 0.1770,RMSE: 22.6814, PCIP: 33.44,score_avg:25.5703\n",
      "r2: 0.1770\n",
      "pcip: 33.4437\n",
      "RMSE: 24.6354\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [43/325] Loss: 521.17212\n",
      "Epoch [43/325] Val Loss: 629.70367\n",
      "increasing counter\n",
      "9\n",
      "MSE: 811.7291,R2: -0.2986,RMSE: 28.4909, PCIP: 35.10,score_avg:2.6177\n",
      "r2: -0.2986\n",
      "pcip: 35.0993\n",
      "RMSE: 24.1689\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [44/325] Loss: 705.37006\n",
      "Epoch [44/325] Val Loss: 416.50031\n",
      "increasing counter\n",
      "10\n",
      "MSE: 565.6685,R2: 0.0950,RMSE: 23.7838, PCIP: 31.95,score_avg:20.7278\n",
      "r2: 0.0950\n",
      "pcip: 31.9536\n",
      "RMSE: 22.6402\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [45/325] Loss: 517.49762\n",
      "Epoch [45/325] Val Loss: 256.57428\n",
      "increasing counter\n",
      "11\n",
      "MSE: 730.5419,R2: -0.1688,RMSE: 27.0285, PCIP: 32.12,score_avg:7.6220\n",
      "r2: -0.1688\n",
      "pcip: 32.1192\n",
      "RMSE: 24.2267\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [46/325] Loss: 465.21240\n",
      "Epoch [46/325] Val Loss: 262.94467\n",
      "increasing counter\n",
      "12\n",
      "MSE: 510.7322,R2: 0.1829,RMSE: 22.5994, PCIP: 37.91,score_avg:28.1024\n",
      "r2: 0.1829\n",
      "pcip: 37.9139\n",
      "RMSE: 24.8110\n",
      "Best RMSE so far: 22.3199\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [47/325] Loss: 557.22778\n",
      "Epoch [47/325] Val Loss: 349.58456\n",
      "increasing counter\n",
      "13\n",
      "MSE: 576.1807,R2: 0.0782,RMSE: 24.0038, PCIP: 36.92,score_avg:22.3703\n",
      "r2: 0.0782\n",
      "pcip: 36.9205\n",
      "RMSE: 21.7475\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [48/325] Loss: 645.14490\n",
      "Epoch [48/325] Val Loss: 442.43573\n",
      "increasing counter\n",
      "14\n",
      "MSE: 511.8125,R2: 0.1812,RMSE: 22.6233, PCIP: 36.09,score_avg:27.1054\n",
      "r2: 0.1812\n",
      "pcip: 36.0927\n",
      "RMSE: 24.2295\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [49/325] Loss: 314.68259\n",
      "Epoch [49/325] Val Loss: 284.14960\n",
      "increasing counter\n",
      "15\n",
      "MSE: 631.8114,R2: -0.0108,RMSE: 25.1359, PCIP: 35.93,score_avg:17.4236\n",
      "r2: -0.0108\n",
      "pcip: 35.9272\n",
      "RMSE: 23.1020\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [50/325] Loss: 348.74957\n",
      "Epoch [50/325] Val Loss: 287.19977\n",
      "increasing counter\n",
      "16\n",
      "MSE: 613.2359,R2: 0.0189,RMSE: 24.7636, PCIP: 36.59,score_avg:19.2406\n",
      "r2: 0.0189\n",
      "pcip: 36.5894\n",
      "RMSE: 28.1664\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [51/325] Loss: 395.43893\n",
      "Epoch [51/325] Val Loss: 290.96637\n",
      "increasing counter\n",
      "17\n",
      "MSE: 557.7104,R2: 0.1078,RMSE: 23.6159, PCIP: 35.26,score_avg:23.0200\n",
      "r2: 0.1078\n",
      "pcip: 35.2649\n",
      "RMSE: 24.1986\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [52/325] Loss: 490.40201\n",
      "Epoch [52/325] Val Loss: 281.40338\n",
      "increasing counter\n",
      "18\n",
      "MSE: 594.4442,R2: 0.0490,RMSE: 24.3812, PCIP: 34.77,score_avg:19.8332\n",
      "r2: 0.0490\n",
      "pcip: 34.7682\n",
      "RMSE: 26.8449\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [53/325] Loss: 390.48972\n",
      "Epoch [53/325] Val Loss: 537.77277\n",
      "increasing counter\n",
      "19\n",
      "MSE: 538.8420,R2: 0.1379,RMSE: 23.2130, PCIP: 32.45,score_avg:23.1220\n",
      "r2: 0.1379\n",
      "pcip: 32.4503\n",
      "RMSE: 25.1155\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [54/325] Loss: 435.13174\n",
      "Epoch [54/325] Val Loss: 317.67819\n",
      "increasing counter\n",
      "20\n",
      "MSE: 504.6276,R2: 0.1927,RMSE: 22.4639, PCIP: 33.77,score_avg:26.5212\n",
      "r2: 0.1927\n",
      "pcip: 33.7748\n",
      "RMSE: 23.5788\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [55/325] Loss: 309.27209\n",
      "Epoch [55/325] Val Loss: 282.84872\n",
      "increasing counter\n",
      "21\n",
      "MSE: 900.7540,R2: -0.4411,RMSE: 30.0126, PCIP: 36.75,score_avg:-3.6758\n",
      "r2: -0.4411\n",
      "pcip: 36.7550\n",
      "RMSE: 22.8843\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [56/325] Loss: 780.16284\n",
      "Epoch [56/325] Val Loss: 230.27734\n",
      "increasing counter\n",
      "22\n",
      "MSE: 633.9982,R2: -0.0143,RMSE: 25.1793, PCIP: 29.64,score_avg:14.1030\n",
      "r2: -0.0143\n",
      "pcip: 29.6358\n",
      "RMSE: 26.0084\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [57/325] Loss: 419.09946\n",
      "Epoch [57/325] Val Loss: 289.84149\n",
      "increasing counter\n",
      "23\n",
      "MSE: 504.3508,R2: 0.1931,RMSE: 22.4578, PCIP: 35.93,score_avg:27.6195\n",
      "r2: 0.1931\n",
      "pcip: 35.9272\n",
      "RMSE: 22.4288\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [58/325] Loss: 451.27109\n",
      "Epoch [58/325] Val Loss: 287.85712\n",
      "increasing counter\n",
      "24\n",
      "MSE: 590.9896,R2: 0.0545,RMSE: 24.3103, PCIP: 30.79,score_avg:18.1228\n",
      "r2: 0.0545\n",
      "pcip: 30.7947\n",
      "RMSE: 27.5290\n",
      "Best RMSE so far: 21.7475\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [59/325] Loss: 434.15576\n",
      "Epoch [59/325] Val Loss: 296.26324\n",
      "increasing counter\n",
      "25\n",
      "MSE: 498.2010,R2: 0.2030,RMSE: 22.3204, PCIP: 34.60,score_avg:27.4492\n",
      "r2: 0.2030\n",
      "pcip: 34.6026\n",
      "RMSE: 21.7241\n",
      "Best RMSE so far: 21.7241\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [60/325] Loss: 393.06403\n",
      "Epoch [60/325] Val Loss: 308.30927\n",
      "increasing counter\n",
      "26\n",
      "MSE: 681.6324,R2: -0.0905,RMSE: 26.1081, PCIP: 37.09,score_avg:14.0178\n",
      "r2: -0.0905\n",
      "pcip: 37.0861\n",
      "RMSE: 23.7641\n",
      "Best RMSE so far: 21.7241\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [61/325] Loss: 379.53058\n",
      "Epoch [61/325] Val Loss: 361.42371\n",
      "increasing counter\n",
      "27\n",
      "MSE: 503.6704,R2: 0.1942,RMSE: 22.4426, PCIP: 35.10,score_avg:27.2600\n",
      "r2: 0.1942\n",
      "pcip: 35.0993\n",
      "RMSE: 22.0366\n",
      "Best RMSE so far: 21.7241\n",
      "Best r2 so far: 0.2415\n",
      "best_score_avg so far: 31.7753\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [62/325] Loss: 392.96130\n",
      "Epoch [62/325] Val Loss: 269.32883\n",
      "increasing counter\n",
      "28\n",
      "MSE: 456.1672,R2: 0.2702,RMSE: 21.3581, PCIP: 37.09,score_avg:32.0532\n",
      "r2: 0.2702\n",
      "pcip: 37.0861\n",
      "RMSE: 24.7641\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 21.7241\n",
      "Best r2 so far: 0.2702\n",
      "best_score_avg so far: 32.0532\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [63/325] Loss: 518.72552\n",
      "Epoch [63/325] Val Loss: 262.19513\n",
      "increasing counter\n",
      "29\n",
      "MSE: 480.5719,R2: 0.2312,RMSE: 21.9220, PCIP: 39.07,score_avg:31.0944\n",
      "r2: 0.2312\n",
      "pcip: 39.0728\n",
      "RMSE: 24.0389\n",
      "Best RMSE so far: 21.7241\n",
      "Best r2 so far: 0.2702\n",
      "best_score_avg so far: 32.0532\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [64/325] Loss: 963.18066\n",
      "Epoch [64/325] Val Loss: 350.70807\n",
      "restarting counter\n",
      "0\n",
      "MSE: 703.3502,R2: -0.1252,RMSE: 26.5207, PCIP: 32.78,score_avg:10.1282\n",
      "r2: -0.1252\n",
      "pcip: 32.7815\n",
      "RMSE: 21.4375\n",
      "Best RMSE so far: 21.4375\n",
      "Best r2 so far: 0.2702\n",
      "best_score_avg so far: 32.0532\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [65/325] Loss: 694.33038\n",
      "Epoch [65/325] Val Loss: 240.04492\n",
      "increasing counter\n",
      "1\n",
      "MSE: 415.2668,R2: 0.3356,RMSE: 20.3781, PCIP: 36.09,score_avg:34.8283\n",
      "r2: 0.3356\n",
      "pcip: 36.0927\n",
      "RMSE: 25.1768\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 21.4375\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 34.8283\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [66/325] Loss: 531.50220\n",
      "Epoch [66/325] Val Loss: 235.49223\n",
      "increasing counter\n",
      "2\n",
      "MSE: 518.7649,R2: 0.1701,RMSE: 22.7764, PCIP: 38.58,score_avg:27.7910\n",
      "r2: 0.1701\n",
      "pcip: 38.5762\n",
      "RMSE: 27.1141\n",
      "Best RMSE so far: 21.4375\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 34.8283\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [67/325] Loss: 592.11371\n",
      "Epoch [67/325] Val Loss: 642.59637\n",
      "increasing counter\n",
      "3\n",
      "MSE: 629.0336,R2: -0.0064,RMSE: 25.0805, PCIP: 40.40,score_avg:19.8809\n",
      "r2: -0.0064\n",
      "pcip: 40.3974\n",
      "RMSE: 19.8944\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 34.8283\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [68/325] Loss: 329.43054\n",
      "Epoch [68/325] Val Loss: 320.42416\n",
      "increasing counter\n",
      "4\n",
      "MSE: 462.4004,R2: 0.2602,RMSE: 21.5035, PCIP: 37.91,score_avg:31.9686\n",
      "r2: 0.2602\n",
      "pcip: 37.9139\n",
      "RMSE: 20.3951\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 34.8283\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [69/325] Loss: 341.95114\n",
      "Epoch [69/325] Val Loss: 653.97009\n",
      "restarting counter\n",
      "0\n",
      "MSE: 491.0517,R2: 0.2144,RMSE: 22.1597, PCIP: 36.59,score_avg:29.0144\n",
      "r2: 0.2144\n",
      "pcip: 36.5894\n",
      "RMSE: 21.4833\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 34.8283\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [70/325] Loss: 380.87674\n",
      "Epoch [70/325] Val Loss: 263.52441\n",
      "increasing counter\n",
      "1\n",
      "MSE: 433.0396,R2: 0.3072,RMSE: 20.8096, PCIP: 39.40,score_avg:35.0622\n",
      "r2: 0.3072\n",
      "pcip: 39.4040\n",
      "RMSE: 21.4018\n",
      "save score avg state\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [71/325] Loss: 334.14404\n",
      "Epoch [71/325] Val Loss: 631.51263\n",
      "increasing counter\n",
      "2\n",
      "MSE: 719.0732,R2: -0.1504,RMSE: 26.8155, PCIP: 38.08,score_avg:11.5195\n",
      "r2: -0.1504\n",
      "pcip: 38.0795\n",
      "RMSE: 22.5563\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [72/325] Loss: 443.49835\n",
      "Epoch [72/325] Val Loss: 340.95786\n",
      "increasing counter\n",
      "3\n",
      "MSE: 502.5319,R2: 0.1960,RMSE: 22.4172, PCIP: 34.11,score_avg:26.8544\n",
      "r2: 0.1960\n",
      "pcip: 34.1060\n",
      "RMSE: 26.7749\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [73/325] Loss: 357.62152\n",
      "Epoch [73/325] Val Loss: 282.50415\n",
      "restarting counter\n",
      "0\n",
      "MSE: 536.6537,R2: 0.1414,RMSE: 23.1658, PCIP: 36.75,score_avg:25.4494\n",
      "r2: 0.1414\n",
      "pcip: 36.7550\n",
      "RMSE: 20.9857\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [74/325] Loss: 367.20294\n",
      "Epoch [74/325] Val Loss: 713.26929\n",
      "increasing counter\n",
      "1\n",
      "MSE: 545.7401,R2: 0.1269,RMSE: 23.3611, PCIP: 33.77,score_avg:23.2325\n",
      "r2: 0.1269\n",
      "pcip: 33.7748\n",
      "RMSE: 22.5860\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [75/325] Loss: 389.44003\n",
      "Epoch [75/325] Val Loss: 526.18463\n",
      "increasing counter\n",
      "2\n",
      "MSE: 669.5467,R2: -0.0712,RMSE: 25.8756, PCIP: 33.28,score_avg:13.0806\n",
      "r2: -0.0712\n",
      "pcip: 33.2781\n",
      "RMSE: 21.0189\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [76/325] Loss: 648.49817\n",
      "Epoch [76/325] Val Loss: 316.39212\n",
      "restarting counter\n",
      "0\n",
      "MSE: 506.8399,R2: 0.1891,RMSE: 22.5131, PCIP: 36.92,score_avg:27.9170\n",
      "r2: 0.1891\n",
      "pcip: 36.9205\n",
      "RMSE: 21.7361\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [77/325] Loss: 518.91980\n",
      "Epoch [77/325] Val Loss: 229.32683\n",
      "increasing counter\n",
      "1\n",
      "MSE: 440.5193,R2: 0.2952,RMSE: 20.9886, PCIP: 33.77,score_avg:31.6493\n",
      "r2: 0.2952\n",
      "pcip: 33.7748\n",
      "RMSE: 24.9744\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 35.0622\n",
      "Best pcip so far: 40.7285\n",
      "Epoch [78/325] Loss: 381.88535\n",
      "Epoch [78/325] Val Loss: 298.56100\n",
      "increasing counter\n",
      "2\n",
      "MSE: 438.4021,R2: 0.2986,RMSE: 20.9381, PCIP: 42.55,score_avg:36.2061\n",
      "r2: 0.2986\n",
      "pcip: 42.5497\n",
      "RMSE: 21.9404\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 36.2061\n",
      "Best pcip so far: 42.5497\n",
      "Epoch [79/325] Loss: 457.62869\n",
      "Epoch [79/325] Val Loss: 578.83923\n",
      "increasing counter\n",
      "3\n",
      "MSE: 670.1425,R2: -0.0721,RMSE: 25.8871, PCIP: 38.58,score_avg:15.6819\n",
      "r2: -0.0721\n",
      "pcip: 38.5762\n",
      "RMSE: 20.0332\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 36.2061\n",
      "Best pcip so far: 42.5497\n",
      "Epoch [80/325] Loss: 786.94415\n",
      "Epoch [80/325] Val Loss: 284.42926\n",
      "increasing counter\n",
      "4\n",
      "MSE: 579.8439,R2: 0.0723,RMSE: 24.0799, PCIP: 39.07,score_avg:23.1535\n",
      "r2: 0.0723\n",
      "pcip: 39.0728\n",
      "RMSE: 25.0945\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 36.2061\n",
      "Best pcip so far: 42.5497\n",
      "Epoch [81/325] Loss: 444.03677\n",
      "Epoch [81/325] Val Loss: 395.33478\n",
      "increasing counter\n",
      "5\n",
      "MSE: 712.5872,R2: -0.1400,RMSE: 26.6943, PCIP: 36.09,score_avg:11.0450\n",
      "r2: -0.1400\n",
      "pcip: 36.0927\n",
      "RMSE: 20.8003\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 36.2061\n",
      "Best pcip so far: 42.5497\n",
      "Epoch [82/325] Loss: 377.12622\n",
      "Epoch [82/325] Val Loss: 261.03949\n",
      "increasing counter\n",
      "6\n",
      "MSE: 498.7952,R2: 0.2020,RMSE: 22.3337, PCIP: 38.25,score_avg:29.2228\n",
      "r2: 0.2020\n",
      "pcip: 38.2450\n",
      "RMSE: 26.3195\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3356\n",
      "best_score_avg so far: 36.2061\n",
      "Best pcip so far: 42.5497\n",
      "Epoch [83/325] Loss: 485.75488\n",
      "Epoch [83/325] Val Loss: 698.97028\n",
      "restarting counter\n",
      "0\n",
      "MSE: 393.0815,R2: 0.3711,RMSE: 19.8263, PCIP: 44.21,score_avg:40.6592\n",
      "r2: 0.3711\n",
      "pcip: 44.2053\n",
      "RMSE: 21.2392\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [84/325] Loss: 361.77591\n",
      "Epoch [84/325] Val Loss: 245.37871\n",
      "increasing counter\n",
      "1\n",
      "MSE: 844.1306,R2: -0.3505,RMSE: 29.0539, PCIP: 40.73,score_avg:2.8404\n",
      "r2: -0.3505\n",
      "pcip: 40.7285\n",
      "RMSE: 22.3634\n",
      "Best RMSE so far: 19.8944\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [85/325] Loss: 284.01810\n",
      "Epoch [85/325] Val Loss: 226.49411\n",
      "restarting counter\n",
      "0\n",
      "MSE: 616.7742,R2: 0.0133,RMSE: 24.8349, PCIP: 42.22,score_avg:21.7722\n",
      "r2: 0.0133\n",
      "pcip: 42.2185\n",
      "RMSE: 19.6210\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [86/325] Loss: 444.07782\n",
      "Epoch [86/325] Val Loss: 445.13809\n",
      "increasing counter\n",
      "1\n",
      "MSE: 467.2984,R2: 0.2524,RMSE: 21.6171, PCIP: 38.41,score_avg:31.8251\n",
      "r2: 0.2524\n",
      "pcip: 38.4106\n",
      "RMSE: 21.2284\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [87/325] Loss: 362.80563\n",
      "Epoch [87/325] Val Loss: 239.39230\n",
      "increasing counter\n",
      "2\n",
      "MSE: 735.7890,R2: -0.1771,RMSE: 27.1254, PCIP: 42.55,score_avg:12.4175\n",
      "r2: -0.1771\n",
      "pcip: 42.5497\n",
      "RMSE: 19.6754\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [88/325] Loss: 475.09537\n",
      "Epoch [88/325] Val Loss: 353.67572\n",
      "increasing counter\n",
      "3\n",
      "MSE: 422.7341,R2: 0.3237,RMSE: 20.5605, PCIP: 36.42,score_avg:34.3965\n",
      "r2: 0.3237\n",
      "pcip: 36.4238\n",
      "RMSE: 20.0883\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [89/325] Loss: 599.11450\n",
      "Epoch [89/325] Val Loss: 251.94695\n",
      "increasing counter\n",
      "4\n",
      "MSE: 441.4560,R2: 0.2937,RMSE: 21.0109, PCIP: 37.09,score_avg:33.2300\n",
      "r2: 0.2937\n",
      "pcip: 37.0861\n",
      "RMSE: 22.5356\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [90/325] Loss: 336.54822\n",
      "Epoch [90/325] Val Loss: 272.01782\n",
      "restarting counter\n",
      "0\n",
      "MSE: 399.4671,R2: 0.3609,RMSE: 19.9867, PCIP: 42.22,score_avg:39.1550\n",
      "r2: 0.3609\n",
      "pcip: 42.2185\n",
      "RMSE: 22.6952\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [91/325] Loss: 392.89597\n",
      "Epoch [91/325] Val Loss: 241.94295\n",
      "increasing counter\n",
      "1\n",
      "MSE: 757.9481,R2: -0.2126,RMSE: 27.5309, PCIP: 38.25,score_avg:8.4926\n",
      "r2: -0.2126\n",
      "pcip: 38.2450\n",
      "RMSE: 19.9694\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [92/325] Loss: 401.71408\n",
      "Epoch [92/325] Val Loss: 229.43997\n",
      "increasing counter\n",
      "2\n",
      "MSE: 520.3983,R2: 0.1674,RMSE: 22.8122, PCIP: 39.40,score_avg:28.0742\n",
      "r2: 0.1674\n",
      "pcip: 39.4040\n",
      "RMSE: 21.1537\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [93/325] Loss: 477.14551\n",
      "Epoch [93/325] Val Loss: 271.46405\n",
      "increasing counter\n",
      "3\n",
      "MSE: 450.5488,R2: 0.2792,RMSE: 21.2261, PCIP: 37.25,score_avg:32.5855\n",
      "r2: 0.2792\n",
      "pcip: 37.2517\n",
      "RMSE: 30.0976\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [94/325] Loss: 488.34772\n",
      "Epoch [94/325] Val Loss: 264.25958\n",
      "increasing counter\n",
      "4\n",
      "MSE: 469.6480,R2: 0.2486,RMSE: 21.6714, PCIP: 40.89,score_avg:32.8789\n",
      "r2: 0.2486\n",
      "pcip: 40.8940\n",
      "RMSE: 24.2504\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [95/325] Loss: 763.77734\n",
      "Epoch [95/325] Val Loss: 263.45029\n",
      "increasing counter\n",
      "5\n",
      "MSE: 445.7218,R2: 0.2869,RMSE: 21.1121, PCIP: 33.94,score_avg:31.3160\n",
      "r2: 0.2869\n",
      "pcip: 33.9404\n",
      "RMSE: 20.8269\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [96/325] Loss: 215.89740\n",
      "Epoch [96/325] Val Loss: 273.29520\n",
      "increasing counter\n",
      "6\n",
      "MSE: 498.3261,R2: 0.2028,RMSE: 22.3232, PCIP: 35.60,score_avg:27.9358\n",
      "r2: 0.2028\n",
      "pcip: 35.5960\n",
      "RMSE: 20.9819\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [97/325] Loss: 492.31314\n",
      "Epoch [97/325] Val Loss: 512.34735\n",
      "increasing counter\n",
      "7\n",
      "MSE: 469.4130,R2: 0.2490,RMSE: 21.6659, PCIP: 36.92,score_avg:30.9109\n",
      "r2: 0.2490\n",
      "pcip: 36.9205\n",
      "RMSE: 26.5755\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [98/325] Loss: 427.80127\n",
      "Epoch [98/325] Val Loss: 246.03064\n",
      "increasing counter\n",
      "8\n",
      "MSE: 430.5122,R2: 0.3112,RMSE: 20.7488, PCIP: 38.08,score_avg:34.6021\n",
      "r2: 0.3112\n",
      "pcip: 38.0795\n",
      "RMSE: 20.1036\n",
      "Best RMSE so far: 19.6210\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [99/325] Loss: 325.53201\n",
      "Epoch [99/325] Val Loss: 266.75592\n",
      "increasing counter\n",
      "9\n",
      "MSE: 421.0874,R2: 0.3263,RMSE: 20.5204, PCIP: 38.91,score_avg:35.7700\n",
      "r2: 0.3263\n",
      "pcip: 38.9073\n",
      "RMSE: 18.7794\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [100/325] Loss: 678.22821\n",
      "Epoch [100/325] Val Loss: 314.33566\n",
      "increasing counter\n",
      "10\n",
      "MSE: 394.8976,R2: 0.3682,RMSE: 19.8720, PCIP: 40.07,score_avg:38.4444\n",
      "r2: 0.3682\n",
      "pcip: 40.0662\n",
      "RMSE: 20.3730\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [101/325] Loss: 310.00574\n",
      "Epoch [101/325] Val Loss: 232.55783\n",
      "restarting counter\n",
      "0\n",
      "MSE: 434.7469,R2: 0.3045,RMSE: 20.8506, PCIP: 39.90,score_avg:35.1740\n",
      "r2: 0.3045\n",
      "pcip: 39.9007\n",
      "RMSE: 19.5718\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3711\n",
      "best_score_avg so far: 40.6592\n",
      "Best pcip so far: 44.2053\n",
      "Epoch [102/325] Loss: 315.26572\n",
      "Epoch [102/325] Val Loss: 250.68192\n",
      "increasing counter\n",
      "1\n",
      "MSE: 383.1559,R2: 0.3870,RMSE: 19.5744, PCIP: 46.19,score_avg:42.4466\n",
      "r2: 0.3870\n",
      "pcip: 46.1921\n",
      "RMSE: 22.9653\n",
      "save rmse state\n",
      "save pcip state\n",
      "save score avg state\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [103/325] Loss: 323.34949\n",
      "Epoch [103/325] Val Loss: 327.44968\n",
      "increasing counter\n",
      "2\n",
      "MSE: 454.1066,R2: 0.2735,RMSE: 21.3098, PCIP: 43.71,score_avg:35.5293\n",
      "r2: 0.2735\n",
      "pcip: 43.7086\n",
      "RMSE: 24.8695\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [104/325] Loss: 352.01175\n",
      "Epoch [104/325] Val Loss: 280.13852\n",
      "increasing counter\n",
      "3\n",
      "MSE: 601.4992,R2: 0.0377,RMSE: 24.5255, PCIP: 40.40,score_avg:22.0835\n",
      "r2: 0.0377\n",
      "pcip: 40.3974\n",
      "RMSE: 21.1284\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [105/325] Loss: 399.65756\n",
      "Epoch [105/325] Val Loss: 313.55988\n",
      "increasing counter\n",
      "4\n",
      "MSE: 421.8947,R2: 0.3250,RMSE: 20.5401, PCIP: 39.24,score_avg:35.8709\n",
      "r2: 0.3250\n",
      "pcip: 39.2384\n",
      "RMSE: 20.8617\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [106/325] Loss: 349.22012\n",
      "Epoch [106/325] Val Loss: 278.96954\n",
      "increasing counter\n",
      "5\n",
      "MSE: 459.0998,R2: 0.2655,RMSE: 21.4266, PCIP: 37.91,score_avg:32.2326\n",
      "r2: 0.2655\n",
      "pcip: 37.9139\n",
      "RMSE: 23.5690\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [107/325] Loss: 536.51648\n",
      "Epoch [107/325] Val Loss: 277.12924\n",
      "increasing counter\n",
      "6\n",
      "MSE: 463.8748,R2: 0.2579,RMSE: 21.5378, PCIP: 34.11,score_avg:29.9466\n",
      "r2: 0.2579\n",
      "pcip: 34.1060\n",
      "RMSE: 20.6888\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [108/325] Loss: 352.18054\n",
      "Epoch [108/325] Val Loss: 266.28000\n",
      "increasing counter\n",
      "7\n",
      "MSE: 449.9380,R2: 0.2802,RMSE: 21.2117, PCIP: 37.25,score_avg:32.6343\n",
      "r2: 0.2802\n",
      "pcip: 37.2517\n",
      "RMSE: 19.8256\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [109/325] Loss: 345.85266\n",
      "Epoch [109/325] Val Loss: 319.12543\n",
      "increasing counter\n",
      "8\n",
      "MSE: 387.8848,R2: 0.3794,RMSE: 19.6948, PCIP: 42.72,score_avg:40.3299\n",
      "r2: 0.3794\n",
      "pcip: 42.7152\n",
      "RMSE: 19.0560\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [110/325] Loss: 761.13800\n",
      "Epoch [110/325] Val Loss: 288.71405\n",
      "increasing counter\n",
      "9\n",
      "MSE: 840.6268,R2: -0.3449,RMSE: 28.9936, PCIP: 38.08,score_avg:1.7962\n",
      "r2: -0.3449\n",
      "pcip: 38.0795\n",
      "RMSE: 19.8286\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.3870\n",
      "best_score_avg so far: 42.4466\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [111/325] Loss: 318.83313\n",
      "Epoch [111/325] Val Loss: 289.96387\n",
      "increasing counter\n",
      "10\n",
      "MSE: 361.5147,R2: 0.4216,RMSE: 19.0135, PCIP: 44.37,score_avg:43.2671\n",
      "r2: 0.4216\n",
      "pcip: 44.3709\n",
      "RMSE: 22.8429\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [112/325] Loss: 423.96616\n",
      "Epoch [112/325] Val Loss: 282.70392\n",
      "increasing counter\n",
      "11\n",
      "MSE: 401.7570,R2: 0.3573,RMSE: 20.0439, PCIP: 43.54,score_avg:39.6341\n",
      "r2: 0.3573\n",
      "pcip: 43.5430\n",
      "RMSE: 20.0044\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [113/325] Loss: 441.29550\n",
      "Epoch [113/325] Val Loss: 284.76495\n",
      "increasing counter\n",
      "12\n",
      "MSE: 403.8809,R2: 0.3539,RMSE: 20.0968, PCIP: 42.38,score_avg:38.8848\n",
      "r2: 0.3539\n",
      "pcip: 42.3841\n",
      "RMSE: 19.7246\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 46.1921\n",
      "Epoch [114/325] Loss: 369.90924\n",
      "Epoch [114/325] Val Loss: 275.74933\n",
      "increasing counter\n",
      "13\n",
      "MSE: 511.9005,R2: 0.1810,RMSE: 22.6252, PCIP: 48.18,score_avg:33.1414\n",
      "r2: 0.1810\n",
      "pcip: 48.1788\n",
      "RMSE: 21.1086\n",
      "save pcip state\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [115/325] Loss: 307.72235\n",
      "Epoch [115/325] Val Loss: 268.34653\n",
      "increasing counter\n",
      "14\n",
      "MSE: 431.1046,R2: 0.3103,RMSE: 20.7631, PCIP: 42.88,score_avg:36.9554\n",
      "r2: 0.3103\n",
      "pcip: 42.8808\n",
      "RMSE: 20.5599\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [116/325] Loss: 264.02496\n",
      "Epoch [116/325] Val Loss: 270.16815\n",
      "increasing counter\n",
      "15\n",
      "MSE: 412.3080,R2: 0.3404,RMSE: 20.3054, PCIP: 40.73,score_avg:37.3828\n",
      "r2: 0.3404\n",
      "pcip: 40.7285\n",
      "RMSE: 20.3831\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [117/325] Loss: 326.31006\n",
      "Epoch [117/325] Val Loss: 259.12238\n",
      "increasing counter\n",
      "16\n",
      "MSE: 506.7432,R2: 0.1893,RMSE: 22.5110, PCIP: 43.54,score_avg:31.2360\n",
      "r2: 0.1893\n",
      "pcip: 43.5430\n",
      "RMSE: 19.9953\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [118/325] Loss: 495.45535\n",
      "Epoch [118/325] Val Loss: 517.36188\n",
      "increasing counter\n",
      "17\n",
      "MSE: 402.7943,R2: 0.3556,RMSE: 20.0697, PCIP: 39.24,score_avg:37.3988\n",
      "r2: 0.3556\n",
      "pcip: 39.2384\n",
      "RMSE: 19.5315\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [119/325] Loss: 253.68066\n",
      "Epoch [119/325] Val Loss: 250.79697\n",
      "increasing counter\n",
      "18\n",
      "MSE: 443.4668,R2: 0.2905,RMSE: 21.0587, PCIP: 46.19,score_avg:37.6222\n",
      "r2: 0.2905\n",
      "pcip: 46.1921\n",
      "RMSE: 25.6535\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [120/325] Loss: 508.74219\n",
      "Epoch [120/325] Val Loss: 566.70087\n",
      "increasing counter\n",
      "19\n",
      "MSE: 386.1589,R2: 0.3822,RMSE: 19.6509, PCIP: 43.54,score_avg:40.8818\n",
      "r2: 0.3822\n",
      "pcip: 43.5430\n",
      "RMSE: 19.7803\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [121/325] Loss: 729.40906\n",
      "Epoch [121/325] Val Loss: 639.46674\n",
      "increasing counter\n",
      "20\n",
      "MSE: 380.9661,R2: 0.3905,RMSE: 19.5184, PCIP: 36.92,score_avg:37.9860\n",
      "r2: 0.3905\n",
      "pcip: 36.9205\n",
      "RMSE: 23.3940\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [122/325] Loss: 324.22858\n",
      "Epoch [122/325] Val Loss: 278.10626\n",
      "increasing counter\n",
      "21\n",
      "MSE: 473.9982,R2: 0.2417,RMSE: 21.7715, PCIP: 38.91,score_avg:31.5375\n",
      "r2: 0.2417\n",
      "pcip: 38.9073\n",
      "RMSE: 22.1340\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [123/325] Loss: 442.83646\n",
      "Epoch [123/325] Val Loss: 222.91769\n",
      "increasing counter\n",
      "22\n",
      "MSE: 504.5783,R2: 0.1928,RMSE: 22.4628, PCIP: 40.40,score_avg:29.8364\n",
      "r2: 0.1928\n",
      "pcip: 40.3974\n",
      "RMSE: 19.8295\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 48.1788\n",
      "Epoch [124/325] Loss: 281.26404\n",
      "Epoch [124/325] Val Loss: 304.21103\n",
      "increasing counter\n",
      "23\n",
      "MSE: 988.3630,R2: -0.5812,RMSE: 31.4382, PCIP: 50.50,score_avg:-3.8129\n",
      "r2: -0.5812\n",
      "pcip: 50.4967\n",
      "RMSE: 19.0385\n",
      "save pcip state\n",
      "Best RMSE so far: 18.7794\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 50.4967\n",
      "Epoch [125/325] Loss: 294.93097\n",
      "Epoch [125/325] Val Loss: 160.27153\n",
      "increasing counter\n",
      "24\n",
      "MSE: 766.8113,R2: -0.2268,RMSE: 27.6914, PCIP: 52.32,score_avg:14.8201\n",
      "r2: -0.2268\n",
      "pcip: 52.3179\n",
      "RMSE: 18.7497\n",
      "save pcip state\n",
      "Best RMSE so far: 18.7497\n",
      "Best r2 so far: 0.4216\n",
      "best_score_avg so far: 43.2671\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [126/325] Loss: 394.46765\n",
      "Epoch [126/325] Val Loss: 741.38208\n",
      "increasing counter\n",
      "25\n",
      "MSE: 326.0081,R2: 0.4784,RMSE: 18.0557, PCIP: 48.34,score_avg:48.0941\n",
      "r2: 0.4784\n",
      "pcip: 48.3444\n",
      "RMSE: 18.1084\n",
      "save rmse state\n",
      "save score avg state\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [127/325] Loss: 298.50964\n",
      "Epoch [127/325] Val Loss: 307.95142\n",
      "increasing counter\n",
      "26\n",
      "MSE: 350.9505,R2: 0.4385,RMSE: 18.7337, PCIP: 51.32,score_avg:47.5890\n",
      "r2: 0.4385\n",
      "pcip: 51.3245\n",
      "RMSE: 18.9102\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [128/325] Loss: 254.14833\n",
      "Epoch [128/325] Val Loss: 221.73474\n",
      "increasing counter\n",
      "27\n",
      "MSE: 459.6582,R2: 0.2646,RMSE: 21.4396, PCIP: 48.01,score_avg:37.2376\n",
      "r2: 0.2646\n",
      "pcip: 48.0132\n",
      "RMSE: 22.9937\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [129/325] Loss: 431.14819\n",
      "Epoch [129/325] Val Loss: 404.28973\n",
      "increasing counter\n",
      "28\n",
      "MSE: 387.1612,R2: 0.3806,RMSE: 19.6764, PCIP: 44.37,score_avg:41.2156\n",
      "r2: 0.3806\n",
      "pcip: 44.3709\n",
      "RMSE: 20.9197\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [130/325] Loss: 818.24396\n",
      "Epoch [130/325] Val Loss: 534.39240\n",
      "increasing counter\n",
      "29\n",
      "MSE: 436.5737,R2: 0.3016,RMSE: 20.8943, PCIP: 45.36,score_avg:37.7596\n",
      "r2: 0.3016\n",
      "pcip: 45.3642\n",
      "RMSE: 19.2694\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [131/325] Loss: 437.55966\n",
      "Epoch [131/325] Val Loss: 290.11716\n",
      "increasing counter\n",
      "30\n",
      "MSE: 430.3051,R2: 0.3116,RMSE: 20.7438, PCIP: 41.56,score_avg:36.3571\n",
      "r2: 0.3116\n",
      "pcip: 41.5563\n",
      "RMSE: 29.1449\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [132/325] Loss: 336.94751\n",
      "Epoch [132/325] Val Loss: 209.11710\n",
      "increasing counter\n",
      "31\n",
      "MSE: 504.8983,R2: 0.1922,RMSE: 22.4699, PCIP: 42.05,score_avg:30.6386\n",
      "r2: 0.1922\n",
      "pcip: 42.0530\n",
      "RMSE: 23.5452\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [133/325] Loss: 594.53998\n",
      "Epoch [133/325] Val Loss: 259.09802\n",
      "increasing counter\n",
      "32\n",
      "MSE: 472.2894,R2: 0.2444,RMSE: 21.7322, PCIP: 52.15,score_avg:38.2967\n",
      "r2: 0.2444\n",
      "pcip: 52.1523\n",
      "RMSE: 19.3328\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [134/325] Loss: 390.03244\n",
      "Epoch [134/325] Val Loss: 381.70053\n",
      "increasing counter\n",
      "33\n",
      "MSE: 357.0858,R2: 0.4287,RMSE: 18.8967, PCIP: 46.03,score_avg:44.4492\n",
      "r2: 0.4287\n",
      "pcip: 46.0265\n",
      "RMSE: 18.7486\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [135/325] Loss: 406.15488\n",
      "Epoch [135/325] Val Loss: 192.03198\n",
      "increasing counter\n",
      "34\n",
      "MSE: 486.9619,R2: 0.2209,RMSE: 22.0672, PCIP: 47.85,score_avg:34.9707\n",
      "r2: 0.2209\n",
      "pcip: 47.8477\n",
      "RMSE: 20.0581\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [136/325] Loss: 290.09488\n",
      "Epoch [136/325] Val Loss: 275.10281\n",
      "increasing counter\n",
      "35\n",
      "MSE: 511.2193,R2: 0.1821,RMSE: 22.6102, PCIP: 47.68,score_avg:32.9475\n",
      "r2: 0.1821\n",
      "pcip: 47.6821\n",
      "RMSE: 18.2194\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [137/325] Loss: 317.49457\n",
      "Epoch [137/325] Val Loss: 843.23712\n",
      "increasing counter\n",
      "36\n",
      "MSE: 483.7046,R2: 0.2261,RMSE: 21.9933, PCIP: 46.85,score_avg:34.7346\n",
      "r2: 0.2261\n",
      "pcip: 46.8543\n",
      "RMSE: 19.3181\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [138/325] Loss: 357.27310\n",
      "Epoch [138/325] Val Loss: 214.01987\n",
      "increasing counter\n",
      "37\n",
      "MSE: 456.2998,R2: 0.2700,RMSE: 21.3612, PCIP: 41.89,score_avg:34.4433\n",
      "r2: 0.2700\n",
      "pcip: 41.8874\n",
      "RMSE: 18.9789\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [139/325] Loss: 420.97903\n",
      "Epoch [139/325] Val Loss: 265.06729\n",
      "increasing counter\n",
      "38\n",
      "MSE: 361.9584,R2: 0.4209,RMSE: 19.0252, PCIP: 44.04,score_avg:43.0660\n",
      "r2: 0.4209\n",
      "pcip: 44.0397\n",
      "RMSE: 22.5296\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [140/325] Loss: 259.46814\n",
      "Epoch [140/325] Val Loss: 436.82111\n",
      "increasing counter\n",
      "39\n",
      "MSE: 392.9103,R2: 0.3714,RMSE: 19.8220, PCIP: 44.70,score_avg:40.9212\n",
      "r2: 0.3714\n",
      "pcip: 44.7020\n",
      "RMSE: 24.8030\n",
      "Best RMSE so far: 18.1084\n",
      "Best r2 so far: 0.4784\n",
      "best_score_avg so far: 48.0941\n",
      "Best pcip so far: 52.3179\n",
      "Epoch [141/325] Loss: 174.46051\n",
      "Epoch [141/325] Val Loss: 321.96664\n",
      "increasing counter\n",
      "40\n",
      "Early stopping!\n",
      "RMSE: 18.11\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Best trial config: {'lr': 0.00011600353938883104,\n",
    "# 'momentum': 0.009328857531182592,\n",
    "# 'opt_momentum': 0.00918667445141735, 'weight_std': 0.0033244058597709614,\n",
    "# 'weight_mean': 6.9949608597674535, 'bias_mean': 0.3226930394143123, 'bias_std': 0.012726759642886507,\n",
    "# 'batch_size': 14, 'epochs': 490, 'dims': [8, 128, 16, 32, 8, 256, 28], 'optimiser':\n",
    "# 'Adams', 'checkpoint_interval': 10}\n",
    "# otimiser': 'Adams', 'checkpoint_interval': 10,\n",
    "# 'opt_momentum': 0.009262583085697922}\n",
    "\n",
    "\n",
    "model_ = BayesianCNN(num_feature=X_train.shape[1],dims=[512,28,64,128,256],\n",
    "                       weight_std=4.7621 ,weight_mean=0.0002, bias_mean=0.0003,\n",
    "                      bias_std=16.738).to(\"cpu\")\n",
    "\n",
    "\n",
    "model,history,best_mse =train_func(model_,n_epochs=325,batch_size=66, lr=0.00012,optimizer_=\"Nadam\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "#save model\n",
    "best_model_name = f'mir_sand_model-1.0.1.pt'\n",
    "torch.save(best_trained_model.state_dict(),os.path.join(\"../model-store/\",best_model_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "best_model_name = f'mir_sand_model-1.0.1.pt'\n",
    "best_trained_model=torch.load(os.path.join(\"../model-store/\",best_model_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_feature: int, dims=[512, 28, 64], weight_std=0.001, bias_std=9.5):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "\n",
    "        # Define the fully connected layers based on the specified dimensions\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = num_feature\n",
    "        for out_dim in dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.fc_out = nn.Linear(in_dim, 1)  # Output layer carbon\n",
    "\n",
    "        # Update the dimensions of weight_mu and weight_rho to match the output dimension\n",
    "        self.weight_dim = out_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(1, self.weight_dim))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_std = weight_std\n",
    "        self.bias_std = bias_std\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weight means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.weight_mu, mean=0.0002, std=self.weight_std)\n",
    "        nn.init.normal_(self.weight_rho, mean=0.003, std=self.weight_std)\n",
    "\n",
    "        # Initialize bias means close to zero and standard deviations to be small\n",
    "        nn.init.normal_(self.bias_mu, mean=0.0003, std=self.bias_std)\n",
    "        nn.init.normal_(self.bias_rho, mean=0.004, std=self.bias_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "\n",
    "        # Pass through fully connected layers with specified dimensions\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = F.rrelu(fc_layer(x))\n",
    "\n",
    "        # Re-parameterization trick for sampling weights\n",
    "        weight_epsilon = Normal(0, 1).sample(self.weight_mu.size())\n",
    "        weight_sigma = torch.log(1 + torch.exp(self.weight_rho))\n",
    "        weight = self.weight_mu + weight_sigma * weight_epsilon\n",
    "\n",
    "        bias_epsilon = Normal(0, 1).sample(self.bias_mu.size())\n",
    "        bias_sigma = torch.log(1 + torch.exp(self.bias_rho))\n",
    "        bias = self.bias_mu + bias_sigma * bias_epsilon\n",
    "\n",
    "        # Enforce non-negativity on weights and biases\n",
    "        weight = torch.clamp(weight, min=0)\n",
    "        bias = torch.clamp(bias, min=0)\n",
    "\n",
    "        # Final linear layer operation\n",
    "        output = F.linear(x, weight, bias)\n",
    "        return output\n",
    "model = BayesianCNN(num_feature=X_train.shape[1],dims=[8,32,32,256,128,8],\n",
    "                     weight_std=0.0016,\n",
    "                      bias_std=0.0015744)\n",
    "model.load_state_dict(torch.load(os.path.join(\"../model-store/\",best_model_name)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54.779083]] (expected [53.870003])\n",
      "[[46.121532]] (expected [45.39])\n",
      "[[43.307953]] (expected [23.483])\n",
      "[[56.67434]] (expected [49.93])\n",
      "[[62.604366]] (expected [62.72])\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "\n",
    "\n",
    "        print(f\"{y_pred.numpy()} (expected {y_test[i].numpy()})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "          lower      upper       pred        obs        type variable\n0     19.448324  42.143402  11.347540  15.559998  Validation     sand\n1     17.634136  99.900000  63.645828  45.610001  Validation     sand\n2     31.624722  99.900000  72.977654  53.827000  Validation     sand\n3      3.390111   4.162225   0.386057   1.070000  Validation     sand\n4     10.597329  14.326819   1.864745   4.279999  Validation     sand\n...         ...        ...        ...        ...         ...      ...\n1000  44.544590  99.900000  82.117462  74.000000  Validation     sand\n1001  15.483009  33.049107   8.783049  36.910000  Validation     sand\n1002  18.657305  54.610344  17.976519  15.959999  Validation     sand\n1003  23.392887  99.900000  68.369049  40.959999  Validation     sand\n1004   1.546224   1.882745   0.168260   8.000000  Validation     sand\n\n[1005 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lower</th>\n      <th>upper</th>\n      <th>pred</th>\n      <th>obs</th>\n      <th>type</th>\n      <th>variable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19.448324</td>\n      <td>42.143402</td>\n      <td>11.347540</td>\n      <td>15.559998</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17.634136</td>\n      <td>99.900000</td>\n      <td>63.645828</td>\n      <td>45.610001</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31.624722</td>\n      <td>99.900000</td>\n      <td>72.977654</td>\n      <td>53.827000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.390111</td>\n      <td>4.162225</td>\n      <td>0.386057</td>\n      <td>1.070000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.597329</td>\n      <td>14.326819</td>\n      <td>1.864745</td>\n      <td>4.279999</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>44.544590</td>\n      <td>99.900000</td>\n      <td>82.117462</td>\n      <td>74.000000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>1001</th>\n      <td>15.483009</td>\n      <td>33.049107</td>\n      <td>8.783049</td>\n      <td>36.910000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>1002</th>\n      <td>18.657305</td>\n      <td>54.610344</td>\n      <td>17.976519</td>\n      <td>15.959999</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>1003</th>\n      <td>23.392887</td>\n      <td>99.900000</td>\n      <td>68.369049</td>\n      <td>40.959999</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>1004</th>\n      <td>1.546224</td>\n      <td>1.882745</td>\n      <td>0.168260</td>\n      <td>8.000000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n  </tbody>\n</table>\n<p>1005 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result= evaluate_soil_property(model,X_test_raw,sample_size=100)\n",
    "test_result[\"obs\"] = y_test\n",
    "test_result['type'] ='Validation'\n",
    "test_result['variable'] ='sand'\n",
    "test_result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "99.2968"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result['pred'].max()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "          lower      upper       pred        obs          type variable\n0      0.959274   5.650571   3.304922  22.489998   Calibration     sand\n1      1.112277   5.985457   3.548867   5.209999   Calibration     sand\n2      2.546515   9.101615   5.824065   7.209999   Calibration     sand\n3      2.923191   9.680145   6.301668   0.010000   Calibration     sand\n4     34.322906  40.789925  37.556416  30.489998   Calibration     sand\n...         ...        ...        ...        ...           ...      ...\n2407   6.211907  14.910117  10.561012  10.630000   Calibration     sand\n2408  71.234848  85.172424  78.203636  81.000000   Calibration     sand\n2409  26.194210  34.508106  30.351158  31.000000   Calibration     sand\n2410  67.621407  81.963005  74.792206  79.489998   Calibration     sand\n2411   7.676397  16.880323  12.278360  18.000000   Calibration     sand\n\n[2412 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lower</th>\n      <th>upper</th>\n      <th>pred</th>\n      <th>obs</th>\n      <th>type</th>\n      <th>variable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.959274</td>\n      <td>5.650571</td>\n      <td>3.304922</td>\n      <td>22.489998</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.112277</td>\n      <td>5.985457</td>\n      <td>3.548867</td>\n      <td>5.209999</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.546515</td>\n      <td>9.101615</td>\n      <td>5.824065</td>\n      <td>7.209999</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.923191</td>\n      <td>9.680145</td>\n      <td>6.301668</td>\n      <td>0.010000</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>34.322906</td>\n      <td>40.789925</td>\n      <td>37.556416</td>\n      <td>30.489998</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2407</th>\n      <td>6.211907</td>\n      <td>14.910117</td>\n      <td>10.561012</td>\n      <td>10.630000</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>71.234848</td>\n      <td>85.172424</td>\n      <td>78.203636</td>\n      <td>81.000000</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>26.194210</td>\n      <td>34.508106</td>\n      <td>30.351158</td>\n      <td>31.000000</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>67.621407</td>\n      <td>81.963005</td>\n      <td>74.792206</td>\n      <td>79.489998</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>7.676397</td>\n      <td>16.880323</td>\n      <td>12.278360</td>\n      <td>18.000000</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n  </tbody>\n</table>\n<p>2412 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result= evaluate_soil_property(model,X_train_raw,sample_size=100)\n",
    "train_result[\"obs\"] = y_train\n",
    "train_result['type'] =' Calibration'\n",
    "train_result['variable'] ='sand'\n",
    "train_result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "final_df=pd.merge(train_result,test_result,how=\"outer\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "          lower      upper       pred        obs          type variable\n0      0.959274   5.650571   3.304922  22.489998   Calibration     sand\n1      1.112277   5.985457   3.548867   5.209999   Calibration     sand\n2      2.546515   9.101615   5.824065   7.209999   Calibration     sand\n3      2.923191   9.680145   6.301668   0.010000   Calibration     sand\n4     34.322906  40.789925  37.556416  30.489998   Calibration     sand\n...         ...        ...        ...        ...           ...      ...\n3412  56.940807  69.150909  63.045860  74.000000    Validation     sand\n3413  45.413712  52.666031  49.039871  36.910000    Validation     sand\n3414  27.939157  36.571804  32.255482  15.959999    Validation     sand\n3415  48.254234  56.531227  52.392731  40.959999    Validation     sand\n3416   0.545586   5.235734   2.890660   8.000000    Validation     sand\n\n[3417 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lower</th>\n      <th>upper</th>\n      <th>pred</th>\n      <th>obs</th>\n      <th>type</th>\n      <th>variable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.959274</td>\n      <td>5.650571</td>\n      <td>3.304922</td>\n      <td>22.489998</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.112277</td>\n      <td>5.985457</td>\n      <td>3.548867</td>\n      <td>5.209999</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.546515</td>\n      <td>9.101615</td>\n      <td>5.824065</td>\n      <td>7.209999</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.923191</td>\n      <td>9.680145</td>\n      <td>6.301668</td>\n      <td>0.010000</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>34.322906</td>\n      <td>40.789925</td>\n      <td>37.556416</td>\n      <td>30.489998</td>\n      <td>Calibration</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3412</th>\n      <td>56.940807</td>\n      <td>69.150909</td>\n      <td>63.045860</td>\n      <td>74.000000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>3413</th>\n      <td>45.413712</td>\n      <td>52.666031</td>\n      <td>49.039871</td>\n      <td>36.910000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>3414</th>\n      <td>27.939157</td>\n      <td>36.571804</td>\n      <td>32.255482</td>\n      <td>15.959999</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>3415</th>\n      <td>48.254234</td>\n      <td>56.531227</td>\n      <td>52.392731</td>\n      <td>40.959999</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n    <tr>\n      <th>3416</th>\n      <td>0.545586</td>\n      <td>5.235734</td>\n      <td>2.890660</td>\n      <td>8.000000</td>\n      <td>Validation</td>\n      <td>sand</td>\n    </tr>\n  </tbody>\n</table>\n<p>3417 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.to_csv(\"C:/Projects/ResearchProjects/Research-SoilSpectroscopy/Smap/Models/CNN/Mir/Results/cnn_sand.csv\")\n",
    "final_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Interval Coverage Probability (PCIP): 51.24%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_pcip(y_true, lower_bounds, upper_bounds):\n",
    "    num_samples = len(y_true)\n",
    "    num_covering_intervals = np.sum((lower_bounds <= y_true) & (y_true <= upper_bounds))\n",
    "    pcip = (num_covering_intervals / num_samples) * 100\n",
    "    return pcip\n",
    "\n",
    "pcip = calculate_pcip(test_result['obs'], test_result['lower'], test_result['upper'])\n",
    "print(f\"Prediction Interval Coverage Probability (PCIP): {pcip:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Prediction Interval Width (mpiw): 9.15\n"
     ]
    }
   ],
   "source": [
    "#mean predition interval width\n",
    "def calculate_mpiw(lower_bounds, upper_bounds):\n",
    "    num_samples = len(lower_bounds)\n",
    "    num_covering_intervals = np.sum(upper_bounds -lower_bounds)\n",
    "    mpiw = (num_covering_intervals / num_samples)\n",
    "    return mpiw\n",
    "\n",
    "mpiw = calculate_mpiw(test_result['lower'], test_result['upper'])\n",
    "print(f\"Mean Prediction Interval Width (mpiw): {mpiw:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9418567689040978"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_train_raw,y_train,type=\"r2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8725837982987219"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_test_raw,y_test,type=\"r2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "9.062159"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_test_raw,y_test,type=\"rmse\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "7.890963"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,X_train_raw,y_train,type=\"rmse\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}